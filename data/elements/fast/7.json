[
    {
        "element_id": "ea6dd8f2-18b3-4ad2-8296-c8df762f18a9",
        "metadata": {},
        "text": "777",
        "type": "UncategorizedText"
    },
    {
        "element_id": "60cbafbe-a223-4512-9ec4-8bee92d9c7fa",
        "metadata": {},
        "text": "Large Language Model Augmented Narrative Driven Recommendations Andrew McCallum mccallum@cs.umass.edu University of Massachusetts Amherst USA",
        "type": "UncategorizedText"
    },
    {
        "element_id": "6fc1ce98-6be3-49b1-8cc9-875e757c0669",
        "metadata": {},
        "text": "Sheshera Mysore smysore@cs.umass.edu University of Massachusetts Amherst USA",
        "type": "Title"
    },
    {
        "element_id": "85576590-d4ef-4f9f-b341-88edd41ca89e",
        "metadata": {},
        "text": "Hamed Zamani hzamani@cs.umass.edu University of Massachusetts Amherst USA",
        "type": "Title"
    },
    {
        "element_id": "c13363d3-7bf3-4ea6-9887-07087cac9c1b",
        "metadata": {},
        "text": "ABSTRACT Narrative-driven recommendation (NDR) presents an information access problem where users solicit recommendations with verbose descriptions of their preferences and context, for example, travelers soliciting recommendations for points of interest while describ- ing their likes/dislikes and travel circumstances. These requests are increasingly important with the rise of natural language-based conversational interfaces for search and recommendation systems. However, NDR lacks abundant training data for models, and current platforms commonly do not support these requests. Fortunately, classical user-item interaction datasets contain rich textual data, e.g., reviews, which often describe user preferences and context \u2013 this may be used to bootstrap training for NDR models. In this work, we explore using large language models (LLMs) for data augmentation to train NDR models. We use LLMs for authoring synthetic narrative queries from user-item interactions with few- shot prompting and train retrieval models for NDR on synthetic queries and user-item interaction data. Our experiments demon- strate that this is an effective strategy for training small-parameter retrieval models that outperform other retrieval and LLM baselines for narrative-driven recommendation.",
        "type": "NarrativeText"
    },
    {
        "element_id": "903bc15c-4d8e-4468-a6e2-c0294ade3276",
        "metadata": {},
        "text": "interactions are effective, users soliciting recommendations often start with a vague idea about their desired target items or may desire recommendations depending on the context of use, often missing in historical interaction data (Figure 1). In these scenarios, it is common for users to solicit recommendations through long- form narrative queries describing their broad interests and context. Information access tasks like these have been studied as narrative- driven recommendations (NDR) for items ranging from books [5] and movies [18], to points of interest [1]. Bogers and Koolen [5] note these narrative requests to be common on discussion forums and several subreddits1, but, there is a lack of support for these complex natural language queries in current recommenders.",
        "type": "NarrativeText"
    },
    {
        "element_id": "a8001bd0-50f2-4e8f-bf94-d9d166af6a34",
        "metadata": {},
        "text": "However, with the emergence of conversational interfaces for information access tasks, support for complex NDR tasks is likely to become necessary. In this context, recent work has noted an increase in complex and subjective natural language requests com- pared to more conventional search interfaces [13, 34]. Furthermore, the emergence of large language models (LLM) with strong lan- guage understanding capabilities presents the potential for fulfilling such complex requests [9, 33]. This work explores the potential for re-purposing historical user-item recommendation datasets, tra- ditionally used for training collaborative filtering recommenders, with LLMs to support NDR.",
        "type": "NarrativeText"
    },
    {
        "element_id": "f2f28ce5-df45-4ab8-ae00-45aeeb2322a1",
        "metadata": {},
        "text": "CCS CONCEPTS \u2022 Information systems \u2192 Recommender systems; Users and inter- active retrieval; \u2022 Computing methodologies \u2192 Natural language generation. ACM Reference Format: Sheshera Mysore, Andrew McCallum, and Hamed Zamani. 2023. Large Language Model Augmented Narrative Driven Recommendations. In Sev- enteenth ACM Conference on Recommender Systems (RecSys \u201923), Septem- ber 18\u201322, 2023, Singapore, Singapore. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/3604915.3608829",
        "type": "NarrativeText"
    },
    {
        "element_id": "f9031d08-450c-4180-9699-a1215a284a44",
        "metadata": {},
        "text": "Specifically, given a user\u2019s interactions, \ud835\udc37\ud835\udc62 , with items and their accompanying text documents (e.g., reviews, descriptions) \ud835\udc41\ud835\udc62 \ud835\udc56=1, selected from a user-item interaction dataset I, we \ud835\udc37\ud835\udc62 = {\ud835\udc51\ud835\udc56 } prompt InstructGPT, a 175B parameter LLM, to author a synthetic narrative query \ud835\udc5e\ud835\udc62 based on \ud835\udc37\ud835\udc62 (Figure 2). Since we expect the query \ud835\udc5e\ud835\udc62 to be noisy and not fully representative of all the user reviews, \ud835\udc37\ud835\udc62 is filtered to retain only a fraction of the reviews based on a language-model assigned likelihood of \ud835\udc5e\ud835\udc62 given a user doc- ument, \ud835\udc51\ud835\udc56 . Then, a pre-trained LM based retrieval model (110M parameters) is fine-tuned for retrieval on the synthetic queries and filtered reviews.",
        "type": "NarrativeText"
    },
    {
        "element_id": "d17c9f54-617d-4c5e-a798-c99aeb0525e3",
        "metadata": {},
        "text": "1 INTRODUCTION Recommender systems personalized to users are an important com- ponent of several industry-scale platforms [16, 17, 46]. These sys- tems function by inferring users\u2019 interests from their prior inter- actions on the platform and making recommendations based on these inferred interests. While recommendations based on historical",
        "type": "NarrativeText"
    },
    {
        "element_id": "3dfedd4c-d519-4705-bd56-b0a36f4d0cc8",
        "metadata": {},
        "text": "Our approach, which we refer to as Mint2, follows from the observation that while narrative queries and suggestions are often made in online discussion forums, and could serve as training data, the number of these posts and the diversity of domains for which they are available is significantly smaller than the size and diversity of passively gathered user-item interaction datasets. E.g. while Bogers and Koolen [5] note nearly 25,000 narrative requests for books on the LibraryThing discussion forum, a publicly available user-item interaction dataset for Goodreads contains interactions with nearly 2.2M books by 460k users [43] .",
        "type": "NarrativeText"
    },
    {
        "element_id": "6c7b662c-dd9b-4528-9207-884bac6b358d",
        "metadata": {},
        "text": "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. RecSys \u201923, September 18\u201322, 2023, Singapore, Singapore \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0241-9/23/09. . . $15.00 https://doi.org/10.1145/3604915.3608829",
        "type": "NarrativeText"
    },
    {
        "element_id": "2d297418-cf3a-49d0-ac8e-45e3c70fe9b1",
        "metadata": {},
        "text": "We empirically evaluate Mint in a publicly available test collec- tion for point of interest recommendation: pointrec [1]. To train",
        "type": "NarrativeText"
    },
    {
        "element_id": "5312cb86-c866-402b-91cb-f89a87d4f5b8",
        "metadata": {},
        "text": "1r/MovieSuggestions, r/booksuggestions, r/Animesuggest 2Mint: Data augMentation with INteraction narraTives.",
        "type": "Title"
    },
    {
        "element_id": "ea0f978c-7635-4699-bdba-3c9ab52c0b13",
        "metadata": {},
        "text": "",
        "type": "PageBreak"
    },
    {
        "element_id": "956b4a68-5977-4853-a79f-b2015fa4096f",
        "metadata": {},
        "text": "778",
        "type": "UncategorizedText"
    },
    {
        "element_id": "39d3f92d-7cf8-441d-9bd5-c4c3f4e9cddd",
        "metadata": {},
        "text": "RecSys \u201923, September 18\u201322, 2023, Singapore, Singapore",
        "type": "Title"
    },
    {
        "element_id": "235a55e4-e178-4cdf-8e84-2975476cc8de",
        "metadata": {},
        "text": "Mysore, McCallum, Zamani",
        "type": "Title"
    },
    {
        "element_id": "1cd691b9-1646-40f9-a0fe-6d4bc65824db",
        "metadata": {},
        "text": "Figure 1: An example narrative query soliciting point of interest recommendations. The query describes the users preferences and the context of their request.",
        "type": "NarrativeText"
    },
    {
        "element_id": "e3c7567a-1b33-4015-b1b9-b4dc0dda6f2b",
        "metadata": {},
        "text": "Figure 2: The format of the prompt used in Mint for generating synthetic narrative queries from user-item interaction with a large language model.",
        "type": "NarrativeText"
    },
    {
        "element_id": "3594871e-491f-4d18-8fdd-f8df7fe38e5d",
        "metadata": {},
        "text": "our NDR models, we generate synthetic training data based on user-item interaction datasets from Yelp. Models (110M parameters) trained with Mint significantly outperform several baseline models and match the performance of significantly larger LLM baselines autoregressively generating recommendations. Code and synthetic datasets are available:3",
        "type": "NarrativeText"
    },
    {
        "element_id": "6ec1a41e-2cab-4197-9896-e54905c0d335",
        "metadata": {},
        "text": "Besides creating queries for ad-hoc retrieval tasks, concurrent work of Leszczynski et al. [25] has also explored the creation of syn- thetic conversational search datasets from music recommendation datasets with LLMs. The synthetic queries and user documents are then used to train bi-encoder retrieval models for conversational search. Our work resembles this in creating synthetic queries from sets of user items found in recommendation interaction datasets. However, it differs in the task of focus, creating long-form narra- tive queries for NDR. Finally, our work also builds on the recent perspective of Radlinski et al. [36] who make a case for natural language user profiles driving recommenders \u2013 narrative requests tie closely to natural language user profiles. Our work presents a step toward these systems.",
        "type": "NarrativeText"
    },
    {
        "element_id": "e35e240e-5623-4fe2-907d-f99147ffefb9",
        "metadata": {},
        "text": "2 RELATED WORK Data Augmentation for Information Access. A line of recent work has explored using language models to generate synthetic queries for data augmentation to train models for information re- trieval tasks [7, 8, 15, 23, 31]. Here, given a document collection of interest, a pre-trained language model is used to create synthetic queries for the document collection. An optional filtering step ex- cludes noisy queries, and finally, a bi-encoder or a cross-encoder is trained for the retrieval task. While earlier work of Ma et al. [31] train a custom query generation model on web-text datasets, more recent work has leveraged large language models for zero/few-shot question generation [7, 8, 15, 23]. In generating synthetic queries, this work indicates the effectiveness of smaller parameter LLMs (up to 6B parameters) for generating synthetic queries in simpler information-retrieval tasks [7, 8, 23], and finds larger models (100B parameters and above) to be necessary for harder tasks such as argument retrieval [15, 23]. Similar to this work, we explore the generation of synthetic queries with LLMs for a retrieval task. Un- like this work, we demonstrate a data augmentation method for creating effective training data from sets of user documents found in recommendation datasets rather than individual documents. Other work in this space has also explored training more efficient multi- vector models from synthetic queries instead of more expensive cross-encoder models [39] and generating queries with a diverse range of intents than the ones available in implicit feedback datasets to enhance item retrievability [35].",
        "type": "NarrativeText"
    },
    {
        "element_id": "df4edaf9-4995-4d75-a051-2f0d916c18d3",
        "metadata": {},
        "text": "Finally, while our work explores data augmentation from user- item interactions for a retrieval-oriented NDR task, prior work has also explored data augmentation of the user-item graph for training collaborative filtering models. This work has often explored aug- mentation to improve recommendation performance for minority [12, 47] or cold-start users [11, 28, 45]. And has leveraged genera- tive models [11, 45] and text similarity models [28] for augmenting the user-item graph.",
        "type": "NarrativeText"
    },
    {
        "element_id": "70ff336d-c123-4a80-b39d-af81e25faa6f",
        "metadata": {},
        "text": "Complex Queries in Information Access. With the advent of performant models for text understanding, focus on complex and interactive information access tasks has seen a resurgence [2, 29, 32, 48]. NDR presents an example of this \u2013 NDR was first formalized in Bogers and Koolen [5] for the case of book recommen- dation and subsequently studied in other domains [3, 4, 6]. Bogers and Koolen [5] systematically examined narrative requests posted by users on discussion forums. They defined NDR as a task requir- ing item recommendation based on a long-form narrative query and prior-user item interactions. While this formulation resembles personalized search [42] and query-driven recommendation [20], the length and complexity of requests differentiate these from NDR. Other work has also demonstrated the effectiveness of re-ranking initial recommendations from collaborative filtering approaches",
        "type": "NarrativeText"
    },
    {
        "element_id": "972e28cd-2ca5-4a38-aaae-57e92d5c0fe7",
        "metadata": {},
        "text": "3https://github.com/iesl/narrative-driven-rec-mint/",
        "type": "Title"
    },
    {
        "element_id": "16e7915c-1b5b-4fb1-871a-330e93f87599",
        "metadata": {},
        "text": "",
        "type": "PageBreak"
    },
    {
        "element_id": "f4ca655c-cc5b-4742-ac11-f19de0fbb72b",
        "metadata": {},
        "text": "779",
        "type": "UncategorizedText"
    },
    {
        "element_id": "14b142eb-63c1-4e92-b518-a27fec1e055d",
        "metadata": {},
        "text": "Large Language Model Augmented Narrative Driven Recommendations",
        "type": "Title"
    },
    {
        "element_id": "5c267c91-0568-4efe-bb94-922083c1357c",
        "metadata": {},
        "text": "RecSys \u201923, September 18\u201322, 2023, Singapore, Singapore",
        "type": "Title"
    },
    {
        "element_id": "6c1e82a6-5d4c-4bf6-b866-b8fc745ac122",
        "metadata": {},
        "text": "Figure 3: Mint re-purposes readily available user-item interaction datasets commonly used to train collaborative filtering models for narrative-driven recommendation. This is done by authoring narrative queries for sets of items liked by a user with a large language model. The data is filtered with a smaller language model and retrieval models are trained on the synthetic queries and user items.",
        "type": "NarrativeText"
    },
    {
        "element_id": "93d6285c-f275-488d-abbd-44ab8f67e8ed",
        "metadata": {},
        "text": "based on the narrative query [18]. More recent work of Afzali et al. [1] formulate the NDR task without access to the prior interactions of a user while also noting the value of contextual cues contained in the narrative request. In our work, we focus on this latter for- mulation of NDR, given the lack of focus on effectively using the rich narrative queries in most prior work. Further, we demonstrate the usefulness of data augmentation from LLMs and user-item interaction datasets lacking narrative queries.",
        "type": "NarrativeText"
    },
    {
        "element_id": "2017da54-c547-4fdd-98cf-e6057df020b3",
        "metadata": {},
        "text": "3.2.1 Narrative Queries from LLMs. To author a narrative query \ud835\udc5e\ud835\udc62 for a user in I, we make use of the 175B parameter InstructGPT4 model as our query generation model QGen. We include the text \ud835\udc41\ud835\udc62 of interacted items {\ud835\udc51\ud835\udc56 } \ud835\udc56=1 in the prompt for QGen, and instruct it to author a narrative query (Figure 2). To improve the coherence of generated queries and obtain correctly formatted outputs, we manually author narrative queries for 3 topically diverse users based on their interacted items and include it in the prompt for QGen. The same three few shot examples are used for the whole dataset I, and the three users were chosen from I. Generating narrative queries based on user interactions may also be considered a form of multi-document summarization for generating a natural language user profile [36].",
        "type": "NarrativeText"
    },
    {
        "element_id": "8709f04e-a5fd-4cb9-aaf5-77d05464d1de",
        "metadata": {},
        "text": "Besides this, a range of work has explored more complex, long- form, and interactive query formulations for information access; these resemble queries in NDR. Arguello et al. [2] define the tip of tongue retrieval task, a known-item search task where user queries describe the rich context of items while being unable to recall item metadata itself. Mysore et al. [32] formulate an aspect conditional query-by example task where results must match specific aspects of a long natural language query. And finally, a vibrant body of work has explored conversational critiquing of recommenders where nat- ural language feedback helps tune the recommendations received by users [30, 44, 49].",
        "type": "NarrativeText"
    },
    {
        "element_id": "97d371ba-5972-4ae9-b997-d79d45a0e701",
        "metadata": {},
        "text": "Filtering Items for Synthetic Queries. Since we expect user 3.2.2 items to capture multiple aspects of their interests and generated queries to only capture a subset of these interests, we only retain \ud835\udc41\ud835\udc62 some of the items present in {\ud835\udc51\ud835\udc56 } \ud835\udc56=1 before using it for training re- trieval models. For this, we use a pre-trained language model to com- pute the likelihood of the query given each user item, \ud835\udc43\ud835\udc3f\ud835\udc40 (\ud835\udc5e\ud835\udc62 |\ud835\udc51\ud835\udc56 ), and only retain the top \ud835\udc40 highly scoring item for \ud835\udc5e\ud835\udc62 , this re- sults in \ud835\udc40 training samples per user for our NDR retrieval models: {(\ud835\udc5e\ud835\udc62, \ud835\udc51\ud835\udc56 )\ud835\udc40 \ud835\udc56=1}. In our experiments, we use FlanT5 with 3B parame- ters [14] for computing and follow Sachan et al. [40] for computing \ud835\udc43\ud835\udc3f\ud835\udc40 (\ud835\udc5e\ud835\udc62 |\ud835\udc51\ud835\udc56 ). Note that our use of \ud835\udc43\ud835\udc3f\ud835\udc40 (\ud835\udc5e\ud835\udc62 |\ud835\udc51\ud835\udc56 ) represents a query- likelihood model classically used for ad-hoc search and recently shown to be an effective unsupervised re-ranking method when used with large pre-trained language models [40].",
        "type": "NarrativeText"
    },
    {
        "element_id": "92773b9b-3a4f-423d-ba88-ab2a4b4cdc77",
        "metadata": {},
        "text": "3 METHOD 3.1 Problem Setup In our work, we define narrative-driven recommendation (NDR) to be a ranking task, where given a narrative query \ud835\udc5e made by a user \ud835\udc62, a ranking system \ud835\udc53 must generate a ranking \ud835\udc45 over a collection of items C. Further, we assume access to a user-item interaction \ud835\udc41\ud835\udc62 dataset I consisting of user interactions with items (\ud835\udc62, {\ud835\udc51\ud835\udc56 } \ud835\udc56=1). We assume the items \ud835\udc51\ud835\udc56 to be textual documents like reviews or item descriptions. While we don\u2019t assume there to be any overlap in the users making narrative queries or the collection of items C and the user-items interaction dataset I, we assume them to be from the same broad domain, e.g., books, movies, points-of-interest.",
        "type": "NarrativeText"
    },
    {
        "element_id": "388ef693-632f-4cf1-a3ea-629455da6e88",
        "metadata": {},
        "text": "3.2.3 Training Retrieval Models. We train bi-encoder and cross- encoder models for NDR on the generated synthetic dataset \u2013 com- monly used models in search tasks. Bi-encoders are commonly used as scalable first-stage rankers from a large collection of items. On the other hand, cross-encoders allow a richer interaction between query and item and are used as second-stage re-ranking models. For both models, we use a pre-trained transformer language model architec- ture with 110M parameters, MPnet, a model similar to Bert [41]. Bi-encoder models embed the query and item independently into high dimensional vectors: q\ud835\udc62 = MPNet(\ud835\udc5e\ud835\udc62 ), d\ud835\udc56 = MPNet(\ud835\udc51\ud835\udc56 ) and rank items for the user based on the minimum L2 distance between",
        "type": "NarrativeText"
    },
    {
        "element_id": "3288581b-f724-4a28-a265-7deed9999740",
        "metadata": {},
        "text": "3.2 Proposed Method Our proposed method, Mint, for NDR, re-purposes a dataset of \ud835\udc41\ud835\udc62 abundantly available user-item interactions, I = {(\ud835\udc62, {\ud835\udc51\ud835\udc56 } \ud835\udc56=1)} into training data for retrieval models by using LLMs as query gener- \ud835\udc41\ud835\udc62 \ud835\udc56=1)}. ation models to author narrative queries \ud835\udc5e\ud835\udc62 : D = {(\ud835\udc5e\ud835\udc62, {\ud835\udc51\ud835\udc56 } Then, retrieval models are trained on the synthetic dataset D (Fig- ure 3).",
        "type": "NarrativeText"
    },
    {
        "element_id": "8774247c-634a-4d98-b28f-707d3325a6b4",
        "metadata": {},
        "text": "4https://platform.openai.com/docs/models/gpt-3, text-davinci-003",
        "type": "Title"
    },
    {
        "element_id": "aff99191-b1c2-414c-9e57-b82e906ac282",
        "metadata": {},
        "text": "",
        "type": "PageBreak"
    },
    {
        "element_id": "8d82fdbc-35d9-45b4-9ee0-d25c9e08333f",
        "metadata": {},
        "text": "780",
        "type": "UncategorizedText"
    },
    {
        "element_id": "71865f6c-d25e-4a3c-a3a0-a87ced5d39ee",
        "metadata": {},
        "text": "RecSys \u201923, September 18\u201322, 2023, Singapore, Singapore",
        "type": "Title"
    },
    {
        "element_id": "03c72f15-89df-437f-9b2f-90612f405271",
        "metadata": {},
        "text": "Mysore, McCallum, Zamani",
        "type": "Title"
    },
    {
        "element_id": "0f2a90a5-3b52-41a6-b0bf-61ba9012f7cb",
        "metadata": {},
        "text": "q\ud835\udc62 and d\ud835\udc56 . Embeddings are obtained by averaging token embeddings from the final layer of MPNet, and the same model is used for both queries and items. Cross-encoder models input both the query and item and output a score to be used for ranking \ud835\udc60 = \ud835\udc53Cr ([\ud835\udc5e\ud835\udc62 ; \ud835\udc51\ud835\udc56 ]), (cid:17). We where \ud835\udc53Cr is parameterized as w\ud835\udc47 dropout train our bi-encoder model with a margin ranking loss: L\ud835\udc35\ud835\udc56 = (cid:205)\ud835\udc62 (cid:205)\ud835\udc40 \ud835\udc56 ) + \ud835\udeff, 0] with randomly sam- pled negatives \ud835\udc51 \u2032 and \ud835\udeff = 1. Our cross-encoders are trained with a cross-entropy loss: L\ud835\udc36\ud835\udc5f = (cid:205)\ud835\udc62 (cid:205)\ud835\udc40 (cid:205)\ud835\udc51\u2032 \ud835\udc52\ud835\udc60\u2032 ). For training, 4 negative example items \ud835\udc51\u2032 are randomly sampled from ranks 100- 300 from our trained bi-encoder. At test time, we retrieve the top 200 items with our trained bi-encoder and re-rank them with the cross-encoder - we evaluate both these components in experiments and refer to them as BiEnc-Mint and CrEnc-Mint.",
        "type": "NarrativeText"
    },
    {
        "element_id": "1a5dfa97-74b8-4884-875a-16065f53b8ab",
        "metadata": {},
        "text": "dislikes). It also retains the users whose interests are summarizable by QGen. In the Yelp dataset, this results in 45,193 retained users. Now, 10,000 randomly selected users are chosen for generating syn- thetic narrative queries. For these users, a single randomly selected sentence from 10 of their reviews is included in the prompt (Figure 2) to QGen, i.e., \ud835\udc41\ud835\udc62 = 10. After generating synthetic queries, some items are filtered out (\u00a73.2.2). Here, we exclude 40% of the items for a user. This results in about 60,000 training samples for training BiEnc-Mint and CrEnc-Mint. These decisions were made manu- ally by examining the resulting datasets and the cost of authoring queries. The expense of generating \ud835\udc5e\ud835\udc62 was about USD 230.",
        "type": "NarrativeText"
    },
    {
        "element_id": "6245629d-4c33-4de5-88fe-94a56055a4eb",
        "metadata": {},
        "text": "(cid:16)",
        "type": "UncategorizedText"
    },
    {
        "element_id": "894ce206-9c21-4a0d-b7da-1d123dee5f12",
        "metadata": {},
        "text": "W\ud835\udc47 MPNet(\u00b7)",
        "type": "Title"
    },
    {
        "element_id": "9096a1d7-dd53-44ab-9afa-cb240b836ff0",
        "metadata": {},
        "text": "\ud835\udc56=1 max[\ud835\udc3f2(q\ud835\udc62, d\ud835\udc56 ) \u2212 \ud835\udc3f2(q\ud835\udc62, d\u2032",
        "type": "Title"
    },
    {
        "element_id": "69c7eefe-ab1a-4989-8581-269c5bec21f3",
        "metadata": {},
        "text": "\ud835\udc52\ud835\udc60",
        "type": "Title"
    },
    {
        "element_id": "637cd8ec-72b7-45bb-8827-f4883ec1187d",
        "metadata": {},
        "text": "\ud835\udc56=1 log(",
        "type": "Title"
    },
    {
        "element_id": "7b1b897a-f04b-4c0c-a169-99e2231dd5c1",
        "metadata": {},
        "text": "4.1.3 Baselines. We compare BiEnc-Mint and CrEnc-Mint mod- els against several standard and performant retrieval model base- lines. These span zero-shot/unsupervised rankers, supervised bi- encoders, unsupervised cross-encoders, and LLM baselines. BM25: A standard unsupervised sparse retrieval baseline based on term overlap between query and document, with strong generalization performance across tasks and domains [38]. Contriver: A BERT-base bi-encoder model pre-trained for zero-shot retrieval with weakly su- pervised query-document pairs [22]. MPNet-1B: A strong Sentence- Bert bi-encoder model initialized with MPNet-base and trained on 1 billion supervised query-document pairs aggregated from numer- ous domains [37]. BERT-MSM: A BERT-base bi-encoder fine-tuned on supervised question-passage pairs from MSMarco. UPR: A two- stage approach that retrieves items with a Contriver bi-encoder and re-ranks the top 200 items with a query-likelihood model using a FlanT5 model with 3B parameters [14, 40]. This may be seen as an unsupervised \u201ccross-encoder\u201d model. Grounded LLM: A re- cently proposed two-stage approach which autoregressively gener- ates ten pseudo-relevant items using an LLM (175B InstructGPT) prompted with the narrative query and generates recommenda- tions grounded in C by retrieving the nearest neighbors for each generated item using a bi-encoder [19]. We include one few-shot example of a narrative query and recommended items in the prompt to the LLM. We run this baseline three times and report average performance across runs. We report NDCG at 5 and 10, MAP, MRR, and Recall at 100 and 200. Finally, our reported results should be considered lower bounds on realistic performance due to the un- judged documents (about 70% at \ud835\udc58 = 10) in our test collections [10].",
        "type": "NarrativeText"
    },
    {
        "element_id": "f288b373-cdd9-4258-a320-c29bf5988246",
        "metadata": {},
        "text": "4 EXPERIMENTS AND RESULTS Next, we evaluate Mint on a publicly available test collection for NDR and present a series of ablations.",
        "type": "NarrativeText"
    },
    {
        "element_id": "17c2bab1-a559-47be-b52f-f004ea6f8746",
        "metadata": {},
        "text": "4.1 Experimental Setup 4.1.1 Datasets. We perform evaluations on an NDR dataset for point-of-interest (POI) recommendation Pointrec [1]. Pointrec contains 112 realistic narrative queries (130 words long) obtained from discussion forums on Reddit and items pooled from baseline rankers. The items are annotated on a graded relevance scale by crowd-workers and/or discussion forum members and further vali- dated by the dataset authors. The item collection C in Pointrec contains 700k POIs with metadata (category, city) and noisy text snippets describing the POI obtained from the Bing search engine. For test time ranking, we only rank the candidate items in the city and request category (e.g., \u201cRestaurants\u201d) of the query available in Pointrec - this follows prior practice to exclude clearly irrelevant items [1, 26]. We use user-item interaction datasets from Yelp to generate synthetic queries for training.5 Note also that we limit our evaluations to Pointrec since it presents the only publicly avail- able, manually annotated, and candidate pooled test collection for NDR, to our knowledge. Other datasets for NDR use document col- lections that are no longer publicly accessible [24], contain sparse and noisy relevance judgments due to them being determined with automatic rules applied to discussion threads [18, 24], lack pooling to gather candidates for judging relevance [18, 24], or lack realistic narrative queries [21]. We leave the development of more robust test collections and evaluation methods for NDR to future work.",
        "type": "NarrativeText"
    },
    {
        "element_id": "177010d1-a4f8-41a6-ad4a-16e141640fbc",
        "metadata": {},
        "text": "4.2 Results Table 1 presents the performance of the proposed method compared against baselines. Here, bold numbers indicate the best-performing model, and superscripts indicate statistical significance computed with two-sided t-tests at \ud835\udc5d < 0.05.",
        "type": "NarrativeText"
    },
    {
        "element_id": "a21e7b64-9a43-4de2-88d8-480debd3720c",
        "metadata": {},
        "text": "Implementation Details. Next, we describe important details 4.1.2 for Mint and leave finer details of the model and training to our code release. To sample user interactions for generating synthetic queries from the Yelp dataset, we exclude POIs and users with fewer than ten reviews to ensure that users were regular users of the site with well represented interests. This follows common prior practice in preparing user-item interaction datasets for use [27]. Then we retain users who deliver an average rating greater than 3/5 and with 10-30 above-average reviews. This desirably biases our data to users who commonly describe their likings (rather than",
        "type": "NarrativeText"
    },
    {
        "element_id": "7a9ed694-074a-45e6-8e54-ad61eee12746",
        "metadata": {},
        "text": "Here, we first note the performance of baseline approaches. We see BM25 outperformed by Contriver, a transformer bi-encoder model trained for zero-shot retrieval; this mirrors prior work [22]. Next, we see supervised bi-encoder models trained on similar pas- sage (MPNet-1B) and question-answer (BERT-MSM) pairs outper- form a weakly supervised model (Contriver) by smaller margins. Finally, the Grounded LLM outperforms all bi-encoder baselines, in- dicating strong few-shot generalization and mirroring prior results [19]. Examining the Mint models, we first note that the BiEnc- Mint sees statistically significant improvement compared to BM25",
        "type": "NarrativeText"
    },
    {
        "element_id": "8c56f67d-4654-4ff9-b2c3-fbc5907ff320",
        "metadata": {},
        "text": "5https://www.yelp.com/dataset",
        "type": "Title"
    },
    {
        "element_id": "1a5db6e9-c494-48b2-9c9d-209dd0e6fe39",
        "metadata": {},
        "text": "",
        "type": "PageBreak"
    },
    {
        "element_id": "1540024b-20de-4efb-bfa9-135ed2c03381",
        "metadata": {},
        "text": "781",
        "type": "UncategorizedText"
    },
    {
        "element_id": "adcc2fdf-2b9f-40c1-8433-d51492c048e2",
        "metadata": {},
        "text": "RecSys \u201923, September 18\u201322, 2023, Singapore, Singapore",
        "type": "Title"
    },
    {
        "element_id": "bdf82c26-3006-4681-b776-fc8974666b8d",
        "metadata": {},
        "text": "Large Language Model Augmented Narrative Driven Recommendations",
        "type": "Title"
    },
    {
        "element_id": "1d1e5fed-e448-4274-b295-b26fd3c9c568",
        "metadata": {},
        "text": "Table 1: Performance of the proposed method, Mint, for point-of-interest recommendation on Pointrec. The superscripts denote statistically significant improvements compared to specific baseline models.",
        "type": "NarrativeText"
    },
    {
        "element_id": "757d2dd4-5fdd-426c-a3cf-813c54530504",
        "metadata": {},
        "text": "Pointrec",
        "type": "Title"
    },
    {
        "element_id": "782aa1a9-41d4-4b12-9eb6-d0ca05394356",
        "metadata": {},
        "text": "Model 1BM25 2Contriver 3MPNet-1B 4BERT-MSM",
        "type": "Title"
    },
    {
        "element_id": "d36a5e61-a1fb-477c-a473-3be4170f62be",
        "metadata": {},
        "text": "Parameters NDCG@5 NDCG@10 MAP",
        "type": "Title"
    },
    {
        "element_id": "ef9e91e1-7421-4633-9e70-0cca711c8101",
        "metadata": {},
        "text": "Recall@100",
        "type": "Title"
    },
    {
        "element_id": "35f07371-ef5e-4b5e-b1ad-3f63b3a1b83c",
        "metadata": {},
        "text": "MRR",
        "type": "Title"
    },
    {
        "element_id": "06e16dcb-d8ad-4f76-bd91-5a38fa06ecff",
        "metadata": {},
        "text": "Recall@200",
        "type": "Title"
    },
    {
        "element_id": "d3779599-90d4-4074-9efe-82bf27752166",
        "metadata": {},
        "text": "0.4194 0.4455 0.4439 0.4679",
        "type": "UncategorizedText"
    },
    {
        "element_id": "ac1f15cc-c2d4-4e5c-8d8f-fa167ae8c5a9",
        "metadata": {},
        "text": "0.2685 0.3355 0.3566 0.3320",
        "type": "UncategorizedText"
    },
    {
        "element_id": "fa5b8122-8468-42a3-8253-cd93b0aff1cf",
        "metadata": {},
        "text": "0.1182 0.1660 0.1621 0.1528",
        "type": "UncategorizedText"
    },
    {
        "element_id": "b69e42bb-89b9-43e6-a5af-09a3c6b40db8",
        "metadata": {},
        "text": "110M 110M 110M",
        "type": "ListItem"
    },
    {
        "element_id": "97e466eb-cfe2-45bb-a9f0-6c7d9b4523fa",
        "metadata": {},
        "text": "0.2464 0.2776 0.2842 0.2886",
        "type": "UncategorizedText"
    },
    {
        "element_id": "9ba8667b-878e-4dd9-9cc4-915f3b769949",
        "metadata": {},
        "text": "0.5429 0.5552 0.5657 0.5816",
        "type": "UncategorizedText"
    },
    {
        "element_id": "0bd305d6-04ac-4a37-8a0d-3d281c1cc00f",
        "metadata": {},
        "text": "0.2682 0.2924 0.3038 0.3117 5Grounded LLM 175B+110M 0.3558 0.3586 0.34891 0.372512",
        "type": "UncategorizedText"
    },
    {
        "element_id": "1b075ae0-17ae-4533-898e-edc1cc6facf1",
        "metadata": {},
        "text": "0.5797 0.5552",
        "type": "UncategorizedText"
    },
    {
        "element_id": "280fa725-59ee-4735-ab09-0d0400a7f0c8",
        "metadata": {},
        "text": "0.4797 0.4489 0.49141 0.5448123",
        "type": "UncategorizedText"
    },
    {
        "element_id": "83e7aab5-86ed-44cf-94e9-bb2406b92b17",
        "metadata": {},
        "text": "0.3861 0.4013 0.39821 0.43171",
        "type": "UncategorizedText"
    },
    {
        "element_id": "0ae4e67f-a0f4-4360-8e45-8090c556eca7",
        "metadata": {},
        "text": "0.3251 0.3242 0.32631 0.348912",
        "type": "UncategorizedText"
    },
    {
        "element_id": "8ace1964-6ab3-4d12-8304-db4301fa721c",
        "metadata": {},
        "text": "0.1808 0.1712 0.18901 0.219214",
        "type": "UncategorizedText"
    },
    {
        "element_id": "34a50a5f-6174-4762-abcc-60073a95778f",
        "metadata": {},
        "text": "6UPR",
        "type": "Title"
    },
    {
        "element_id": "9bd2a9b9-7e71-4970-9352-266c8bc18c6f",
        "metadata": {},
        "text": "110M+3B",
        "type": "UncategorizedText"
    },
    {
        "element_id": "5998f38f-8bdc-4872-bfc9-9b017d76a7f4",
        "metadata": {},
        "text": "BiEnc-Mint CrEnc-Mint",
        "type": "Title"
    },
    {
        "element_id": "9c4dc71c-0195-437e-89dd-dd17b3c7070f",
        "metadata": {},
        "text": "110M 2\u00d7110M",
        "type": "UncategorizedText"
    },
    {
        "element_id": "e124bb9c-b766-4821-9114-5b51ebfcaa26",
        "metadata": {},
        "text": "0.6221 0.6221",
        "type": "UncategorizedText"
    },
    {
        "element_id": "9204620f-35f5-47d8-ae3a-a6b67e008af7",
        "metadata": {},
        "text": "6B LLM for Item Queries. We find a smaller 6B LLM to result in poor quality data when used to generate narrative queries con- \ud835\udc41\ud835\udc62 ditioned on {\ud835\udc51\ud835\udc56 } \ud835\udc56=1. Here we simplify the text generation task \u2013 using a 6B LLM to generate queries for individual items \ud835\udc51\ud835\udc56 . This experiment also mirrors the setup for generating synthetic queries for search tasks [7, 15]. Here, we use 3-few shot examples and sam- ple one item per user for generating \ud835\udc5e\ud835\udc62 . Given the lower cost of using a smaller LLM, we use all 45,193 users in our Yelp dataset rather than a smaller random sample. From Table 2, we see that this results in higher quality queries than using smaller LLMs for gen- \ud835\udc41\ud835\udc62 \ud835\udc56=1. The resulting BiEnc model erating narrative queries from {\ud835\udc51\ud835\udc56 } underperforms the BiEnc-Mint, indicating the value of generating complex queries conditioned on multiple items as in Mint for NDR. We see that CrEnc approaches the performance of CrEnc-Mint\u2013 note, however, that this approach uses the performant BiEnc-Mint for sampling negatives and first stage ranking. We leave further exploration of using small parameter LLMs for data augmentation for NDR models to future work.",
        "type": "NarrativeText"
    },
    {
        "element_id": "4e4ad4ae-7867-45eb-b407-3d2665e396f1",
        "metadata": {},
        "text": "and outperforms the best bi-encoder baselines by 11-13% on preci- sion measures and 5-7% on recall measures. Specifically, we see a model trained for question-answering (BERT-MSM) underperform BiEnc-Mint, indicating the challenge of the NDR task. Further, BiEnc-Mint, trained on 5 orders of magnitude lesser data than MPNet-1B, sees improved performance \u2013 indicating the quality of data obtained from Mint. Furthermore, BiEnc-Mint also performs at par with a 175B LLM while offering the inference efficiency of a small-parameter bi-encoder. Next, we see CrEnc-Mint outperform the baseline bi-encoders, BiEnc-Mint, UPR, and Grounded LLM by 4-21% on precision measures and 7-13% on recall measures \u2013 demonstrating the value of Mint for training NDR models.",
        "type": "NarrativeText"
    },
    {
        "element_id": "b78d9fec-56ec-448b-ae76-7a7cec1468f6",
        "metadata": {},
        "text": "4.3 Ablations In Table 2, we ablate various design choices in Mint. Different choices result in different training sets for the BiEnc and CrEnc models. Also, note that in reporting ablation performance for CrEnc, we still use the performant BiEnc-Mint model for obtaining nega- tive examples for training and first-stage ranking. Without high- quality negative examples, we found CrEnc to result in much poorer performance.",
        "type": "NarrativeText"
    },
    {
        "element_id": "064783bf-420e-4d07-9cd9-07a5f4d8986f",
        "metadata": {},
        "text": "5 CONCLUSIONS In this paper, we present Mint, a data augmentation method for the narrative-driven recommendation (NDR) task. Mint re-purposes historical user-item interaction datasets for NDR by using a 175B pa- rameter large language model to author long-form narrative queries while conditioning on the text of items liked by users. We evaluate bi-encoder and cross-encoder models trained on data from Mint on the publicly available Pointrec test collection for narrative-driven point of interest recommendation. We demonstrate that the result- ing models outperform several strong baselines and ablated models and match or outperform a 175B LLM directly used for NDR in a 1-shot setup.",
        "type": "NarrativeText"
    },
    {
        "element_id": "5bac1444-adbd-4198-bda5-832719ef13b6",
        "metadata": {},
        "text": "No item filtering. Since synthetic queries are unlikely to rep- \ud835\udc41\ud835\udc62 resent all the items of a user, Mint excludes user items {\ud835\udc51\ud835\udc56 } \ud835\udc56=1 which have a low likelihood of being generated from the document (\u00a73.2.2). Without this step, we expect the training set for training retrieval models to be larger and noisier. In Table 2, we see that excluding this step leads to a lower performance for BiEnc and CrEnc, indicating that the quality of data obtained is important for performance.",
        "type": "NarrativeText"
    },
    {
        "element_id": "e12b0ad1-420d-4d77-ae31-265e1ebf4881",
        "metadata": {},
        "text": "6B LLM for QGen. Mint relies on using an expensive 175B pa- rameter InstructGPT model for QGen. Here, we investigate the \ud835\udc41\ud835\udc62 efficacy for generating \ud835\udc5e\ud835\udc62 for {\ud835\udc51\ud835\udc56 } \ud835\udc56=1 with a 6B parameter Instruct- GPT model (text-curie-001). We use an identical setup to the 175B LLM for this. In Table 2, we see that training on the synthetic narrative queries of the smaller LLM results in worse models \u2013 of- ten underperforming the baselines in Table 1. This indicates the inability of a smaller model to generate complex narrative queries while conditioning on a set of user items. This necessity of a larger LLM for generating queries in complex retrieval tasks has been observed in prior work [15, 23].",
        "type": "NarrativeText"
    },
    {
        "element_id": "8193b9c8-e92a-40fb-ad89-6d2d2a1a9f4f",
        "metadata": {},
        "text": "However, Mint also presents some limitations. Given our use of historical interaction datasets for generating synthetic training data and the prevalence of popular interests in these datasets longer, tailed interests are unlikely to be present in the generated syn- thetic datasets. In turn, causing retrieval models to likely see poorer performance on these requests. Our use of LLMs to generate syn- thetic queries also causes the queries to be repetitive in structure, likely causing novel longer-tail queries to be poorly served. These limitations may be addressed in future work.",
        "type": "NarrativeText"
    },
    {
        "element_id": "bd460c7e-09f9-4478-8ca4-ef2b6cacfa82",
        "metadata": {},
        "text": "",
        "type": "PageBreak"
    },
    {
        "element_id": "47965a1d-0492-4e49-a471-3bfc201f3002",
        "metadata": {},
        "text": "782",
        "type": "UncategorizedText"
    },
    {
        "element_id": "2b08a50c-9623-47d9-b84f-37c14e9f3ad8",
        "metadata": {},
        "text": "Mysore, McCallum, Zamani",
        "type": "Title"
    },
    {
        "element_id": "48633560-d397-4f90-ad99-3cb9264799c1",
        "metadata": {},
        "text": "RecSys \u201923, September 18\u201322, 2023, Singapore, Singapore",
        "type": "Title"
    },
    {
        "element_id": "1b7d48d3-6e78-4c5c-94ea-104506dd7ad3",
        "metadata": {},
        "text": "Table 2: Mint ablated for different design choices on Pointrec.",
        "type": "NarrativeText"
    },
    {
        "element_id": "62fe3831-fbd8-4936-8f12-42f8fa9b35c1",
        "metadata": {},
        "text": "Pointrec",
        "type": "Title"
    },
    {
        "element_id": "c85b4072-4ffb-4aa5-a35c-b63f265442d5",
        "metadata": {},
        "text": "Ablation",
        "type": "Title"
    },
    {
        "element_id": "fe9ea93e-8120-4d51-8194-6b48d1e4a802",
        "metadata": {},
        "text": "MRR",
        "type": "Title"
    },
    {
        "element_id": "7c22574a-58f5-4eae-9a8b-d218916da7f2",
        "metadata": {},
        "text": "Recall@200",
        "type": "Title"
    },
    {
        "element_id": "db317d22-bb5d-4689-bcef-73356e744cdf",
        "metadata": {},
        "text": "Recall@100",
        "type": "Title"
    },
    {
        "element_id": "b794699f-f57c-4567-9dcd-20406036763d",
        "metadata": {},
        "text": "NDCG@5 NDCG@10 MAP",
        "type": "Title"
    },
    {
        "element_id": "26c7d40c-fea1-442c-a9a8-896797178541",
        "metadata": {},
        "text": "0.1890 0.1634 0.1125 0.1721",
        "type": "UncategorizedText"
    },
    {
        "element_id": "fe53de77-9499-404d-8fcf-0bd8e72d495b",
        "metadata": {},
        "text": "0.3982 0.3505 0.2287 0.3384",
        "type": "UncategorizedText"
    },
    {
        "element_id": "4a3eb0c6-e634-4f97-bee0-ee2d8537af19",
        "metadata": {},
        "text": "0.3263 0.2766 0.2293 0.2875",
        "type": "UncategorizedText"
    },
    {
        "element_id": "df7f2df1-aa17-4be6-b62c-7b4755f5af06",
        "metadata": {},
        "text": "0.5263 0.4979 0.426 0.4800",
        "type": "UncategorizedText"
    },
    {
        "element_id": "ad430324-3852-4974-9f5d-949ad0c075af",
        "metadata": {},
        "text": "0.3489 0.2949 0.2336 0.3012",
        "type": "UncategorizedText"
    },
    {
        "element_id": "9bc19a40-61ee-4ac0-872e-1c26afcf085a",
        "metadata": {},
        "text": "BiEnc-Mint \u2212 No item filtering \u2212 6B LLM for QGen \u2212 6B LLM for Item Queries CrEnc-Mint \u2212 No item filtering \u2212 6B LLM for QGen \u2212 6B LLM for Item Queries",
        "type": "NarrativeText"
    },
    {
        "element_id": "35c45565-37c7-46b5-b76a-d0b3df002f22",
        "metadata": {},
        "text": "0.6221 0.5951 0.5435 0.5909",
        "type": "UncategorizedText"
    },
    {
        "element_id": "7723810d-66d8-4c11-bd3b-b924e5c382ad",
        "metadata": {},
        "text": "0.2192 0.2071 0.1341 0.2128",
        "type": "UncategorizedText"
    },
    {
        "element_id": "908fafa4-bd93-4f0b-a84b-355d5b80ab4e",
        "metadata": {},
        "text": "0.3725 0.3570 0.2618 0.3792",
        "type": "UncategorizedText"
    },
    {
        "element_id": "a3581ab6-156a-408f-baae-f02a3895f19c",
        "metadata": {},
        "text": "0.5448 0.5366 0.4841 0.5546",
        "type": "UncategorizedText"
    },
    {
        "element_id": "94104cbc-5555-49f4-878f-07ee191841ae",
        "metadata": {},
        "text": "0.6221 0.6221 0.6221 0.6221",
        "type": "UncategorizedText"
    },
    {
        "element_id": "91116b19-f0b9-4e39-923a-eecee6d38d9c",
        "metadata": {},
        "text": "0.3489 0.3379 0.2421 0.3451",
        "type": "UncategorizedText"
    },
    {
        "element_id": "08198c7d-af88-47f5-a650-3cb20e52fb03",
        "metadata": {},
        "text": "0.4317 0.4063 0.3118 0.4098",
        "type": "UncategorizedText"
    },
    {
        "element_id": "26b1f300-7284-495e-b053-87795f969f2e",
        "metadata": {},
        "text": "Besides this, other avenues also present rich future work. While Mint leverages a 175B LLM for generating synthetic queries, smaller parameter LLMs may be explored for this purpose - perhaps by training dedicated QGen models. Mint may also be expanded to explore more active strategies for sampling items and users for whom narrative queries are authored - this may allow more effi- cient use of large parameter LLMs while ensuring higher quality training datasets. Next, the generation of synthetic queries from sets of documents may be explored for a broader range of retrieval tasks beyond NDR given its promise to generate larger training sets \u2013 a currently underexplored direction. Finally, given the lack of larger-scale test collections for NDR and the effectiveness of LLMs for authoring narrative queries from user-item interaction, fruitful future work may also explore the creation of larger-scale datasets in a mixed-initiative setup to robustly evaluate models for NDR.",
        "type": "NarrativeText"
    },
    {
        "element_id": "4c5cca5e-36b2-4a93-b7f8-2b1e8733bb15",
        "metadata": {},
        "text": "Society: 14th International Conference, iConference 2019, Washington, DC, USA, March 31\u2013April 3, 2019, Proceedings 14. Springer, 503\u2013515.",
        "type": "UncategorizedText"
    },
    {
        "element_id": "bcde7661-648e-4ece-8ebf-2b1b991c71f6",
        "metadata": {},
        "text": "[5] Toine Bogers and Marijn Koolen. 2017. Defining and Supporting Narrative-Driven Recommendation. In Proceedings of the Eleventh ACM Conference on Recommender Systems (Como, Italy) (RecSys \u201917). Association for Computing Machinery, New York, NY, USA, 238\u2013242. https://doi.org/10.1145/3109859.3109893",
        "type": "NarrativeText"
    },
    {
        "element_id": "2f97e55e-ac8c-4f12-b416-050794e98c66",
        "metadata": {},
        "text": "[6] Toine Bogers and Marijn Koolen. 2018. \u201cI\u2019m looking for something like. . . \u201d: Combining Narratives and Example Items for Narrative-driven Book Recommen- dation. In Knowledge-aware and Conversational Recommender Systems Workshop. CEUR Workshop Proceedings.",
        "type": "NarrativeText"
    },
    {
        "element_id": "5532f21c-28d0-495a-8d65-b3d8d8d47433",
        "metadata": {},
        "text": "[7] Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022. InPars: Unsupervised Dataset Generation for Information Retrieval. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (Madrid, Spain) (SIGIR \u201922). Association for Computing Machinery, New York, NY, USA, 2387\u20132392. https://doi.org/10.1145/3477495. 3531863",
        "type": "NarrativeText"
    },
    {
        "element_id": "ebd0ded5-72b9-4fde-b234-737b4a9d8f06",
        "metadata": {},
        "text": "[8] Leonid Boytsov, Preksha Patel, Vivek Sourabh, Riddhi Nisar, Sayani Kundu, Ramya Ramanathan, and Eric Nyberg. 2023. InPars-Light: Cost-Effective Unsu- pervised Training of Efficient Rankers. arXiv:2301.02998",
        "type": "NarrativeText"
    },
    {
        "element_id": "7233f433-ddd2-49ed-babb-5c16c2ef52dd",
        "metadata": {},
        "text": "[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ran- zato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 1877\u20131901. https://proceedings.neurips.cc/paper_files/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf",
        "type": "NarrativeText"
    },
    {
        "element_id": "736356dd-a0b6-4ae6-9e7e-d88e4b58b3cb",
        "metadata": {},
        "text": "ACKNOWLEDGMENTS We thank anonymous reviewers for their invaluable feedback. This work was partly supported by the Center for Intelligent Informa- tion Retrieval, NSF grants IIS-1922090 and 2143434, the Office of Naval Research contract number N000142212688, an Amazon Alexa Prize grant, and the Chan Zuckerberg Initiative under the project Scientific Knowledge Base Construction. Any opinions, findings and conclusions or recommendations expressed here are those of the authors and do not necessarily reflect those of the sponsors.",
        "type": "NarrativeText"
    },
    {
        "element_id": "5e368240-63d9-483b-8f6b-22187a096bc7",
        "metadata": {},
        "text": "[10] Chris Buckley and Ellen M. Voorhees. 2004. Retrieval Evaluation with Incomplete Information. In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (Sheffield, United Kingdom) (SIGIR \u201904). Association for Computing Machinery, New York, NY, USA, 25\u201332. https://doi.org/10.1145/1008992.1009000",
        "type": "NarrativeText"
    },
    {
        "element_id": "b8134f02-4793-4204-8d56-a192a3e69daa",
        "metadata": {},
        "text": "[11] Dong-Kyu Chae, Jihoo Kim, Duen Horng Chau, and Sang-Wook Kim. 2020. AR- CF: Augmenting Virtual Users and Items in Collaborative Filtering for Addressing Cold-Start Problems. In Proceedings of the 43rd International ACM SIGIR Con- ference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR \u201920). Association for Computing Machinery, New York, NY, USA, 1251\u20131260. https://doi.org/10.1145/3397271.3401038",
        "type": "NarrativeText"
    },
    {
        "element_id": "66c1a8ba-01be-4fbe-b3df-ff9e97d2e9b9",
        "metadata": {},
        "text": "REFERENCES [1] Jafar Afzali, Aleksander Mark Drzewiecki, and Krisztian Balog. 2021. POINTREC: A Test Collection for Narrative-Driven Point of Interest Recommendation. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, Canada) (SIGIR \u201921). As- sociation for Computing Machinery, New York, NY, USA, 2478\u20132484. https: //doi.org/10.1145/3404835.3463243",
        "type": "NarrativeText"
    },
    {
        "element_id": "e97c53f2-03dc-465f-9e23-d066f9ea5b77",
        "metadata": {},
        "text": "[12] Lei Chen, Le Wu, Kun Zhang, Richang Hong, Defu Lian, Zhiqiang Zhang, Jun Zhou, and Meng Wang. 2023. Improving Recommendation Fairness via Data Augmentation. In Proceedings of the ACM Web Conference 2023 (Austin, TX, USA) (WWW \u201923). Association for Computing Machinery, New York, NY, USA, 1012\u20131020. https://doi.org/10.1145/3543507.3583341",
        "type": "NarrativeText"
    },
    {
        "element_id": "2855d172-bdf9-4caa-b639-a98396d1baa3",
        "metadata": {},
        "text": "[2] Jaime Arguello, Adam Ferguson, Emery Fine, Bhaskar Mitra, Hamed Zamani, and Fernando Diaz. 2021. Tip of the Tongue Known-Item Retrieval: A Case Study in Movie Identification. In Proceedings of the 6th international ACM SIGIR Conference on Human Information Interaction and Retrieval. ACM. https://dlnext.acm.org/ doi/10.1145/3406522.3446021",
        "type": "NarrativeText"
    },
    {
        "element_id": "1f63aba3-0b16-4a17-a354-d13f58fa1c80",
        "metadata": {},
        "text": "[13] Li Chen, Zhirun Zhang, Xinzhi Zhang, and Lehong Zhao. 2022. A Pilot Study for Understanding Users\u2019 Attitudes Towards a Conversational Agent for News Recommendation. In Proceedings of the 4th Conference on Conversational User Interfaces (Glasgow, United Kingdom) (CUI \u201922). Association for Computing Machinery, New York, NY, USA, Article 36, 6 pages. https://doi.org/10.1145/ 3543829.3544530",
        "type": "NarrativeText"
    },
    {
        "element_id": "fd4c470a-48e8-401e-8ef9-11df5776f436",
        "metadata": {},
        "text": "[3] Toine Bogers, Maria G\u00e4de, Marijn Koolen, Vivien Petras, and Mette Skov. 2018. \u201cWhat was this Movie About this Chick?\u201d A Comparative Study of Relevance Aspects in Book and Movie Discovery. In Transforming Digital Worlds: 13th Inter- national Conference, iConference 2018, Sheffield, UK, March 25-28, 2018, Proceedings 13. Springer, 323\u2013334.",
        "type": "NarrativeText"
    },
    {
        "element_id": "e0a299c9-6eb2-4854-86fb-a7d8d044c793",
        "metadata": {},
        "text": "[14] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416 (2022). [15] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith Hall, and Ming-Wei Chang. 2023. Promptagator: Few-shot",
        "type": "NarrativeText"
    },
    {
        "element_id": "b345e091-9d25-4026-a89a-e41ea01f36fe",
        "metadata": {},
        "text": "[4] Toine Bogers, Maria G\u00e4de, Marijn Koolen, Vivien Petras, and Mette Skov. 2019. \u201cLooking for an amazing game I can relax and sink hours into...\u201d: A Study of Relevance Aspects in Video Game Discovery. In Information in Contemporary",
        "type": "NarrativeText"
    },
    {
        "element_id": "2d074877-a453-4831-ac45-e1f3c0f9377c",
        "metadata": {},
        "text": "",
        "type": "PageBreak"
    },
    {
        "element_id": "4b28d237-b6e6-47b0-8f81-edfca819533e",
        "metadata": {},
        "text": "783",
        "type": "UncategorizedText"
    },
    {
        "element_id": "24d2c9e7-1925-4be8-a78d-45d14f9351e3",
        "metadata": {},
        "text": "Large Language Model Augmented Narrative Driven Recommendations",
        "type": "Title"
    },
    {
        "element_id": "b87a664d-fe0c-4cc6-89d6-27fe743c38b7",
        "metadata": {},
        "text": "RecSys \u201923, September 18\u201322, 2023, Singapore, Singapore",
        "type": "Title"
    },
    {
        "element_id": "749cea30-887e-4465-a606-2976df48bb97",
        "metadata": {},
        "text": "Advances in Neural Information Processing Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc., 27730\u201327744. https://proceedings.neurips.cc/paper_files/paper/2022/file/ b1efde53be364a73914f58805a001731-Paper-Conference.pdf",
        "type": "NarrativeText"
    },
    {
        "element_id": "c0d556f1-d6f7-45cd-8a46-395ead42d020",
        "metadata": {},
        "text": "Dense Retrieval From 8 Examples. In The Eleventh International Conference on Learning Representations. https://openreview.net/forum?id=gmL46YMpu2J [16] Abhinandan S. Das, Mayur Datar, Ashutosh Garg, and Shyam Rajaram. 2007. Google News Personalization: Scalable Online Collaborative Filtering. In Pro- ceedings of the 16th International Conference on World Wide Web (Banff, Alberta, Canada) (WWW \u201907). Association for Computing Machinery, New York, NY, USA, 271\u2013280. https://doi.org/10.1145/1242572.1242610",
        "type": "NarrativeText"
    },
    {
        "element_id": "d2f2bef3-9ee2-4be2-8b10-cf5a0a96dac5",
        "metadata": {},
        "text": "[34] Andrea Papenmeier, Dagmar Kern, Daniel Hienert, Alfred Sliwa, Ahmet Aker, and Norbert Fuhr. 2021. Starting Conversations with Search Engines - Interfaces That Elicit Natural Language Queries. In Proceedings of the 2021 Conference on Human Information Interaction and Retrieval (Canberra ACT, Australia) (CHIIR \u201921). Association for Computing Machinery, New York, NY, USA, 261\u2013265. https: //doi.org/10.1145/3406522.3446035",
        "type": "NarrativeText"
    },
    {
        "element_id": "cc4ba58c-a222-440c-b3bb-e8c1ed866fe4",
        "metadata": {},
        "text": "[17] James Davidson, Benjamin Liebald, Junning Liu, Palash Nandy, Taylor Van Vleet, Ullas Gargi, Sujoy Gupta, Yu He, Mike Lambert, Blake Livingston, and Dasarathi Sampath. 2010. The YouTube Video Recommendation System. In Proceedings of the Fourth ACM Conference on Recommender Systems (Barcelona, Spain) (RecSys \u201910). Association for Computing Machinery, New York, NY, USA, 293\u2013296. https: //doi.org/10.1145/1864708.1864770",
        "type": "NarrativeText"
    },
    {
        "element_id": "a54277af-6bed-4ec0-b621-a570e0c771a7",
        "metadata": {},
        "text": "[35] Gustavo Penha, Enrico Palumbo, Maryam Aziz, Alice Wang, and Hugues Bouchard. 2023. Improving Content Retrievability in Search with Controllable Query Generation. In Proceedings of the ACM Web Conference 2023 (Austin, TX, USA) (WWW \u201923). Association for Computing Machinery, New York, NY, USA, 3182\u20133192. https://doi.org/10.1145/3543507.3583261",
        "type": "NarrativeText"
    },
    {
        "element_id": "084b1828-63b1-4a71-ac7d-b58560f9aab0",
        "metadata": {},
        "text": "[18] Lukas Eberhard, Simon Walk, Lisa Posch, and Denis Helic. 2019. Evaluating Narrative-Driven Movie Recommendations on Reddit. In Proceedings of the 24th International Conference on Intelligent User Interfaces (Marina del Ray, California) (IUI \u201919). Association for Computing Machinery, New York, NY, USA, 1\u201311. https: //doi.org/10.1145/3301275.3302287",
        "type": "NarrativeText"
    },
    {
        "element_id": "4a59afd7-3d3f-47be-8f70-43f6c37c635e",
        "metadata": {},
        "text": "[36] Filip Radlinski, Krisztian Balog, Fernando Diaz, Lucas Dixon, and Ben Wedin. 2022. On Natural Language User Profiles for Transparent and Scrutable Rec- ommendation. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (Madrid, Spain) (SIGIR \u201922). Association for Computing Machinery, New York, NY, USA, 2863\u20132874. https://doi.org/10.1145/3477495.3531873",
        "type": "NarrativeText"
    },
    {
        "element_id": "88dbd31e-e788-4add-a74a-4c5efff344f6",
        "metadata": {},
        "text": "[19] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022. Precise Zero-Shot Dense Retrieval without Relevance Labels. arXiv preprint arXiv:2212.10496 (2022). [20] Negar Hariri, Bamshad Mobasher, and Robin Burke. 2013. Query-Driven Context Aware Recommendation. In Proceedings of the 7th ACM Conference on Recom- mender Systems (Hong Kong, China) (RecSys \u201913). Association for Computing Machinery, New York, NY, USA, 9\u201316. https://doi.org/10.1145/2507157.2507187 [21] Seyyed Hadi Hashemi, Jaap Kamps, Julia Kiseleva, Charles LA Clarke, and Ellen M Voorhees. 2016. Overview of the TREC 2016 Contextual Suggestion Track.. In TREC.",
        "type": "NarrativeText"
    },
    {
        "element_id": "764c3b98-f54b-405a-a984-5aab3789c5a4",
        "metadata": {},
        "text": "[37] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. In Proceedings of the 2019 Conference on Em- pirical Methods in Natural Language Processing. Association for Computational Linguistics. https://arxiv.org/abs/1908.10084",
        "type": "NarrativeText"
    },
    {
        "element_id": "bad152cb-6ba5-4803-bacb-9696f5cdae41",
        "metadata": {},
        "text": "[38] Stephen Robertson and Hugo Zaragoza. 2009. The Probabilistic Relevance Framework: BM25 and Beyond. Found. Trends Inf. Retr. 3, 4 (apr 2009), 333\u2013389. https://doi.org/10.1561/1500000019",
        "type": "NarrativeText"
    },
    {
        "element_id": "b509f713-e024-4b04-a99e-630b5ce6b880",
        "metadata": {},
        "text": "[22] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo- janowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised Dense Infor- mation Retrieval with Contrastive Learning. Transactions on Machine Learning Research (2022). https://openreview.net/forum?id=jKN1pXi7b0",
        "type": "NarrativeText"
    },
    {
        "element_id": "8b3a9907-5138-4c80-9336-32110e58fa07",
        "metadata": {},
        "text": "[39] Jon Saad-Falcon, Omar Khattab, Keshav Santhanam, Radu Florian, Martin Franz, Salim Roukos, Avirup Sil, Md Arafat Sultan, and Christopher Potts. 2023. UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers. arXiv:2303.00807 [cs.IR]",
        "type": "NarrativeText"
    },
    {
        "element_id": "009878bc-dc02-4656-8387-65a55ed1c931",
        "metadata": {},
        "text": "[23] Vitor Jeronymo, Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, Roberto Lotufo, Jakub Zavrel, and Rodrigo Nogueira. 2023. InPars-v2: Large Language Models as Efficient Dataset Generators for Information Retrieval. arXiv:2301.01820 [24] Marijn Koolen, Toine Bogers, Maria G\u00e4de, Mark Hall, Iris Hendrickx, Hugo Huurdeman, Jaap Kamps, Mette Skov, Suzan Verberne, and David Walsh. 2016. Overview of the CLEF 2016 Social Book Search Lab. In Experimental IR Meets Mul- tilinguality, Multimodality, and Interaction, Norbert Fuhr, Paulo Quaresma, Teresa Gon\u00e7alves, Birger Larsen, Krisztian Balog, Craig Macdonald, Linda Cappellato, and Nicola Ferro (Eds.). Springer International Publishing, Cham, 351\u2013370. [25] Megan Leszczynski, Ravi Ganti, Shu Zhang, Krisztian Balog, Filip Radlinski, Fernando Pereira, and Arun Tejasvi Chaganty. 2023. Generating Synthetic Data for Conversational Music Recommendation Using Random Walks and Language Models. arXiv:2301.11489",
        "type": "NarrativeText"
    },
    {
        "element_id": "f89c173d-0d8e-486b-8018-1e6cbc662606",
        "metadata": {},
        "text": "[40] Devendra Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau Yih, Joelle Pineau, and Luke Zettlemoyer. 2022. Improving Passage Retrieval with Zero-Shot Question Generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 3781\u20133797. https://aclanthology. org/2022.emnlp-main.249",
        "type": "NarrativeText"
    },
    {
        "element_id": "6f148f98-8159-49fe-8750-636aebcdd89d",
        "metadata": {},
        "text": "[41] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. 2020. MPNet: Masked and Permuted Pre-training for Language Understanding. In Advances in Neural Information Processing Systems, Vol. 33. https://proceedings.neurips.cc/paper_ files/paper/2020/file/c3a690be93aa602ee2dc0ccab5b7b67e-Paper.pdf",
        "type": "NarrativeText"
    },
    {
        "element_id": "84a786f9-8c23-4afe-a432-cbaf97ff9824",
        "metadata": {},
        "text": "[42] Jaime Teevan, Susan T. Dumais, and Eric Horvitz. 2005. Personalizing Search via Automated Analysis of Interests and Activities. In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (Salvador, Brazil) (SIGIR \u201905). Association for Computing Machinery, New York, NY, USA, 449\u2013456. https://doi.org/10.1145/1076034.1076111 [43] Mengting Wan and Julian McAuley. 2018. Item Recommendation on Monotonic Behavior Chains. In Proceedings of the 12th ACM Conference on Recommender Systems (Vancouver, British Columbia, Canada) (RecSys \u201918). Association for Computing Machinery, New York, NY, USA, 86\u201394. https://doi.org/10.1145/ 3240323.3240369",
        "type": "NarrativeText"
    },
    {
        "element_id": "ca7d91ce-7d95-4948-96e9-df90f615628c",
        "metadata": {},
        "text": "[26] Xin Liu, Yong Liu, Karl Aberer, and Chunyan Miao. 2013. Personalized Point-of- Interest Recommendation by Mining Users\u2019 Preference Transition. In Proceedings of the 22nd ACM International Conference on Information & Knowledge Manage- ment (San Francisco, California, USA) (CIKM \u201913). Association for Computing Ma- chinery, New York, NY, USA, 733\u2013738. https://doi.org/10.1145/2505515.2505639 [27] Yiding Liu, Tuan-Anh Nguyen Pham, Gao Cong, and Quan Yuan. 2017. An Experimental Evaluation of Point-of-Interest Recommendation in Location-Based Social Networks. Proc. VLDB Endow. 10, 10 (jun 2017), 1010\u20131021. https://doi. org/10.14778/3115404.3115407",
        "type": "NarrativeText"
    },
    {
        "element_id": "77cd5868-fb10-431a-8454-80e9e82e40bb",
        "metadata": {},
        "text": "[44] Haonan Wang, Chang Zhou, Carl Yang, Hongxia Yang, and Jingrui He. 2021.",
        "type": "UncategorizedText"
    },
    {
        "element_id": "ee2317df-d1cc-4269-86c3-bb1b02c4b6eb",
        "metadata": {},
        "text": "[28] Federico L\u00f3pez, Martin Scholz, Jessica Yung, Marie Pellat, Michael Strube, and Lucas Dixon. 2021. Augmenting the user-item graph with textual similarity models. arXiv preprint arXiv:2109.09358 (2021).",
        "type": "NarrativeText"
    },
    {
        "element_id": "2b0b0688-63a1-49c4-a6e6-7a059f23f302",
        "metadata": {},
        "text": "Controllable Gradient Item Retrieval. In Web Conference.",
        "type": "NarrativeText"
    },
    {
        "element_id": "74b49b70-5990-4602-b7de-72b03d4e6ff3",
        "metadata": {},
        "text": "[45] Qinyong Wang, Hongzhi Yin, Hao Wang, Quoc Viet Hung Nguyen, Zi Huang, and Lizhen Cui. 2019. Enhancing Collaborative Filtering with Generative Aug- mentation. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (Anchorage, AK, USA) (KDD \u201919). As- sociation for Computing Machinery, New York, NY, USA, 548\u2013556. https: //doi.org/10.1145/3292500.3330873",
        "type": "NarrativeText"
    },
    {
        "element_id": "6d614971-26c6-40a4-8fbe-0623b87179b6",
        "metadata": {},
        "text": "[29] Xing Han Lu, Siva Reddy, and Harm de Vries. 2023. The StatCan Dialogue Dataset: Retrieving Data Tables through Conversations with Genuine Intents. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, Dubrovnik, Croatia, 2799\u20132829. https://aclanthology.org/2023.eacl-main.206",
        "type": "NarrativeText"
    },
    {
        "element_id": "07cd5ec4-f9a2-4a1c-b02f-5e0cd002bf29",
        "metadata": {},
        "text": "[46] Jiajing Xu, Andrew Zhai, and Charles Rosenberg. 2022. Rethinking Personalized Ranking at Pinterest: An End-to-End Approach. In Proceedings of the 16th ACM Conference on Recommender Systems (Seattle, WA, USA) (RecSys \u201922). Association for Computing Machinery, New York, NY, USA, 502\u2013505. https://doi.org/10. 1145/3523227.3547394",
        "type": "NarrativeText"
    },
    {
        "element_id": "8331ae8a-8a25-4942-a2a3-21072ba5e9c6",
        "metadata": {},
        "text": "[30] Kai Luo, Scott Sanner, Ga Wu, Hanze Li, and Hojin Yang. 2020. Latent Linear",
        "type": "UncategorizedText"
    },
    {
        "element_id": "d7a41671-2275-4764-9fff-7789bfa842a6",
        "metadata": {},
        "text": "Critiquing for Conversational Recommender Systems. In The Web Conference. [31] Ji Ma, Ivan Korotkov, Yinfei Yang, Keith Hall, and Ryan McDonald. 2021. Zero-shot Neural Passage Retrieval via Domain-targeted Synthetic Question Generation. In Proceedings of the 16th Conference of the European Chapter of the Associa- tion for Computational Linguistics: Main Volume. Association for Computational Linguistics, Online, 1075\u20131088. https://doi.org/10.18653/v1/2021.eacl-main.92",
        "type": "NarrativeText"
    },
    {
        "element_id": "4299f8db-8a0b-4b89-a226-dfde0012dc22",
        "metadata": {},
        "text": "[47] Yuxin Ying, Fuzhen Zhuang, Yongchun Zhu, Deqing Wang, and Hongwei Zheng. 2023. CAMUS: Attribute-Aware Counterfactual Augmentation for Minority Users in Recommendation. In Proceedings of the ACM Web Conference 2023 (Austin, TX, USA) (WWW \u201923). Association for Computing Machinery, New York, NY, USA, 1396\u20131404. https://doi.org/10.1145/3543507.3583538",
        "type": "NarrativeText"
    },
    {
        "element_id": "670607b8-abd4-4c91-9414-bc49fac08efc",
        "metadata": {},
        "text": "[32] Sheshera Mysore, Tim O\u2019Gorman, Andrew McCallum, and Hamed Zamani. 2021. CSFCube - A Test Collection of Computer Science Research Articles for Faceted Query by Example. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). https://doi.org/10.48550/arXiv. 2103.12906",
        "type": "NarrativeText"
    },
    {
        "element_id": "9633b78a-dee8-4f81-be14-da77f8f34cc3",
        "metadata": {},
        "text": "[48] Hamed Zamani, Johanne R Trippas, Jeff Dalton, and Filip Radlinski. 2022. Con- versational information seeking. arXiv preprint arXiv:2201.08808 (2022). [49] Jie Zou, Yifan Chen, and Evangelos Kanoulas. 2020. Towards Question-Based Recommender Systems. In Proceedings of the 43rd International ACM SIGIR Confer- ence on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR \u201920). Association for Computing Machinery, New York, NY, USA, 881\u2013890. https://doi.org/10.1145/3397271.3401180",
        "type": "NarrativeText"
    },
    {
        "element_id": "92a9ac28-7dbb-45d0-8bd8-12459124d804",
        "metadata": {},
        "text": "[33] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In",
        "type": "NarrativeText"
    },
    {
        "element_id": "c8e04ed0-c9c3-48fb-83dc-ec5cc21fa474",
        "metadata": {},
        "text": "",
        "type": "PageBreak"
    }
]