[
    "37",
    "Generating Diverse Code Explanations using the GPT-3 Large Language Model Andrew Tran andrew.tran10@temple.edu Temple University Philadelphia, PA, USA",
    "Stephen MacNeil stephen.macneil@temple.edu Temple University Philadelphia, PA, USA",
    "Dan Mogil daniel.mogil@temple.edu Temple University Philadelphia, PA, USA",
    "Seth Bernstein seth.bernstein@temple.edu Temple University Philadelphia, PA, USA",
    "Ziheng Huang z8huang@ucsd.edu University of California\u2014San Diego La Jolla, CA, USA",
    "Erin Ross erinross@temple.edu Temple University Philadelphia, PA, USA",
    "explanation types to illustrate the explanatory power of GPT-3. The additional types include: 1) tracing the execution of code, 2) fixing bugs and explaining how they were fixed, 3) generating analogies to real world settings, 4) listing relevant programming concepts, and 5) predicting the console output.",
    "KEYWORDS large language models, natural language processing, code explana- tions, computer science education",
    "ACM Reference Format: Stephen MacNeil, Andrew Tran, Dan Mogil, Seth Bernstein, Erin Ross, and Ziheng Huang. 2022. Generating Diverse Code Explanations using the GPT-3 Large Language Model. In Proceedings of the 2022 ACM Conference on International Computing Education Research V.2 (ICER 2022), August 7\u201311, 2022, Lugano and Virtual Event, Switzerland. ACM, New York, NY, USA, 3 pages. https://doi.org/10.1145/3501709.3544280",
    "1 ABSTRACT Good explanations are essential to efficiently learning introductory programming concepts [10]. To provide high-quality explanations at scale, numerous systems automate the process by tracing the execution of code [8, 12], defining terms [9], giving hints [16], and providing error-specific feedback [10, 16]. However, these ap- proaches often require manual effort to configure and only explain a single aspect of a given code segment. Large language models (LLMs) are also changing how students interact with code [7]. For example, Github\u2019s Copilot can generate code for programmers [4], leading researchers to raise concerns about cheating [7]. Instead, our work focuses on LLMs\u2019 potential to support learning by explain- ing numerous aspects of a given code snippet. This poster features a systematic analysis of the diverse natural language explanations that GPT-3 can generate automatically for a given code snippet. We present a subset of three use cases from our evolving design space of AI Explanations of Code.",
    "Figure 1: A prompt and explanation based on analogy.",
    "2.1 Analyzing and explaining time complexity Instructors rate time complexity as the most difficult programming topic [17]. However, understanding time complexity is important [6, 13] because it facilitates decision-making so students choose an appropriate algorithm for a given problem. This use case shows GPT-3 can identify and explain time complexity.",
    "2 USE CASES To understand the types of explanations GPT-3 [2] can generate, we issued over 700 prompts across numerous code snippets. An example prompt and resulting explanation is shown in Figure 1. We discovered eight explanation types and Figure 2 includes three",
    "2.2 Identifying common mistakes made by",
    "beginner programmers",
    "Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). ICER 2022, August 7\u201311, 2022, Lugano and Virtual Event, Switzerland \u00a9 2022 Copyright held by the owner/author(s). ACM ISBN 978-1-4503-9195-5/22/08. https://doi.org/10.1145/3501709.3544280",
    "Commonality exists in how students solve programming prob- lems [15] and the mistakes they make [1, 11]. Pedagogical tech- niques, such as the \u2018muddiest point\u2019 highlight these common and most confusing concepts [3, 14]. GPT-3 can automatically create a checklist of common mistakes students might make regarding a given code snippet.",
    "38",
    "ICER 2022, August 7\u201311, 2022, Lugano and Virtual Event, Switzerland",
    "MacNeil et al.",
    "Figure 2: Three example explanations automatically generated by GPT-3 for an \u2018anonymized\u2019 Binary Search code snippet.",
    "2.3 Summarizing code at multiple levels of",
    "[5] Kathryn Cunningham, Yike Qiao, Alex Feng, and Eleanor O\u2019Rourke. 2022. Bring- ing \"High-Level\" Down to Earth: Gaining Clarity in Conversational Program- mer Learning Goals. In Proceedings of the 53rd ACM Technical Symposium on Computer Science Education V. 1 (Providence, RI, USA) (SIGCSE 2022). As- https: sociation for Computing Machinery, New York, NY, USA, 551\u2013557. //doi.org/10.1145/3478431.3499370",
    "abstraction",
    "Before understanding how a code snippet executes, it is often useful to understand the purpose of the code [5]. The summary gener- ated by GPT-3 and shown in Figure 2 defines the goal, traces the execution, and highlights relevant CS concepts such as arrays.",
    "[6] Elvina Elvina and Oscar Karnalim. 2017. Complexitor: An educational tool for learning algorithm time complexity in practical manner. ComTech: Computer, Mathematics and Engineering Applications 8, 1 (2017), 21\u201327.",
    "[7] James Finnie-Ansley, Paul Denny, Brett A. Becker, Andrew Luxton-Reilly, and James Prather. 2022. The Robots Are Coming: Exploring the Implications of Ope- nAI Codex on Introductory Programming. In Australasian Computing Education Conference (Virtual Event, Australia) (ACE \u201922). ACM, New York, NY, USA, 10\u201319. https://doi.org/10.1145/3511861.3511863",
    "3 DISCUSSION Our three use cases demonstrate the potential for GPT-3 to explain code for intro CS students. Our poster presentation will feature all eight explanation types as a design space of explanations to convey the diversity of explanations that can be generated by LLMs. We will highlight best practices for generating effective explanations and pitfalls that lead to less effective explanations. We are evaluating the usefulness of these explanations in a series of summer classes.",
    "[8] Philip J Guo. 2013. Online python tutor: embeddable web-based program visual- ization for cs education. In Proceeding of the 44th ACM technical symposium on Computer science education. 579\u2013584.",
    "[9] Andrew Head, Codanda Appachu, Marti A Hearst, and Bj\u00f6rn Hartmann. 2015. Tutorons: Generating context-relevant, on-demand explanations and demonstra- tions of online code. In 2015 IEEE Symposium on Visual Languages and Human- Centric Computing (VL/HCC). IEEE, 3\u201312.",
    "[10] Samiha Marwan, Ge Gao, Susan Fisk, Thomas W. Price, and Tiffany Barnes. 2020. Adaptive Immediate Feedback Can Improve Novice Programming Engagement and Intention to Persist in Computer Science. In Proceedings of the 2020 ACM Conference on International Computing Education Research (Virtual Event, New Zealand) (ICER \u201920). Association for Computing Machinery, New York, NY, USA, 194\u2013203. https://doi.org/10.1145/3372782.3406264",
    "REFERENCES [1] Amjad Altadmri and Neil CC Brown. 2015. 37 million compilations: Investigating novice programming mistakes in large-scale student data. In Proceedings of the 46th ACM Technical Symposium on Computer Science Education. 522\u2013527. [2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in Neural Information Processing Systems 33 (2020), 1877\u20131901.",
    "[11] Davin McCall and Michael K\u00f6lling. 2014. Meaningful categorisation of novice pro- grammer errors. In 2014 IEEE Frontiers in Education Conference (FIE) Proceedings. IEEE, 1\u20138.",
    "[12] Greg L Nelson, Benjamin Xie, and Amy J Ko. 2017. Comprehension first: eval- uating a novel pedagogy and tutoring system for program tracing in CS1. In Proceedings of the 2017 ACM conference on international computing education research. 2\u201311.",
    "[3] Adam Carberry, Stephen Krause, Casey Ankeny, and Cynthia Waters. 2013. \u201cUnmuddying\u201d course content using muddiest point reflections. In 2013 IEEE Frontiers in Education Conference (FIE). IEEE, 937\u2013942.",
    "[13] Miranda Parker and Colleen Lewis. 2014. What makes big-O analysis difficult: understanding how students understand runtime analysis. Journal of Computing Sciences in Colleges 29, 4 (2014), 164\u2013174.",
    "[4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021).",
    "[14] Daniel Perez, Leila Zahedi, Monique Ross, Jia Zhu, Tiffany Vinci-Cannava, Laird Kramer, and Maria Charters. 2020. WIP: An exploration into the muddiest points",
    "39",
    "Generating Diverse Explanations with Large Language Models",
    "ICER 2022, August 7\u201311, 2022, Lugano and Virtual Event, Switzerland",
    "and self-efficacy of students in introductory computer science courses. In 2020 IEEE Frontiers in Education Conference (FIE). IEEE, 1\u20135.",
    "iSnap: towards intelligent tutoring in novice programming environments. In Proceedings of the 2017 ACM SIGCSE Technical Symposium on computer science education. 483\u2013488. [17] Carsten Schulte and Jens Bennedsen. 2006. What do teachers teach in introductory programming?. In Proceedings of the second international workshop on Computing education research. 17\u201328.",
    "[16] Thomas W Price, Yihuan Dong, and Dragan Lipovac. 2017.",
    "[15] Chris Piech, Mehran Sahami, Jonathan Huang, and Leonidas Guibas. 2015. Au- tonomously generating hints by inferring problem solving policies. In Proceedings of the second (2015) acm conference on learning@ scale. 195\u2013204."
]