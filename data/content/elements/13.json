[
    "AdvancesinEngineeringSoftware188(2024)103571",
    "Availableonline7December20230965-9978/\u00a92023ElsevierLtd.Allrightsreserved.",
    "Contents lists available at ScienceDirect",
    "Advances in Engineering Software",
    "journal homepage: www.elsevier.com/locate/advengsoft",
    "Research paper SMT 2.0: A Surrogate Modeling Toolbox with a focus on hierarchical and mixed variables Gaussian processes Paul Saves a,b,\u2217,1, R\u00e9mi Lafage a,1, Nathalie Bartoli a,1, Youssef Diouane c,1, Jasper Bussemaker d,1, Thierry Lefebvre a,1, John T. Hwang e,1, Joseph Morlier f,1, Joaquim R.R.A. Martins g,1 a ONERA/DTIS, Universit\u00e9 de Toulouse, Toulouse, France b ISAE-SUPAERO, Universit\u00e9 de Toulouse, Toulouse, France c Polytechnique Montr\u00e9al, Montreal, QC, Canada d German Aerospace Center (DLR), Institute of System Architectures in Aeronautics, Hamburg, Germany e University of California San Diego, Department of Mechanical and Aerospace Engineering, La Jolla, CA, USA f ICA, Universit\u00e9 de Toulouse, ISAE\u2013SUPAERO, INSA, CNRS, MINES ALBI, UPS, Toulouse, France g University of Michigan, Department of Aerospace Engineering, Ann Arbor, MI, USA",
    "A B S T R A C T",
    "A R T I C L E I N F O",
    "The Surrogate Modeling Toolbox (SMT) is an open-source Python package that offers a collection of surrogate modeling methods, sampling techniques, and a set of sample problems. This paper presents SMT 2.0, a major new release of SMT that introduces significant upgrades and new features to the toolbox. This release adds the capability to handle mixed-variable surrogate models and hierarchical variables. These types of variables are becoming increasingly important in several surrogate modeling applications. SMT 2.0 also improves SMT by extending sampling methods, adding new surrogate models, and computing variance and kernel derivatives for Kriging. This release also includes new functions to handle noisy and use multi-fidelity data. To the best of our knowledge, SMT 2.0 is the first open-source surrogate library to propose surrogate models for hierarchical and mixed inputs. This open-source software is distributed under the New BSD license.2",
    "Dataset link: https://colab.research.google.com /github/SMTorg/smt/blob/master/tutorial/No tebookRunTestCases_Paper_SMT_v2.ipynb",
    "Keywords: Surrogate modeling Gaussian process Kriging Hierarchical problems Hierarchical and mixed-categorical inputs Meta variables",
    "Kriging models (also known as Gaussian processes) that take advantage of derivative information are one of SMT\u2019s key features [6]. Numerical experiments have shown that SMT achieved lower prediction error and computational cost than Scikit-learn [7] and UQLab [8] for a fixed number of points [9]. SMT has been applied to rocket engine coaxial-injector optimization [10], aircraft engine consumption mod- eling [11], numerical integration [12], multi-fidelity sensitivity analy- sis [13], high-order robust finite elements methods [14,15], planning for photovoltaic solar energy [16], wind turbines design optimiza- tion [17], porous material optimization for a high pressure turbine vane [18], chemical process design [19] and many other applications. In systems engineering, architecture-level choices significantly in- fluence the final system performance, and therefore, it is desirable to consider such choices in the early design phases [20]. Architectural choices are parameterized with discrete design variables; examples in- clude the selection of technologies, materials, component connections,",
    "1. Motivation and significance",
    "With the increasing complexity and accuracy of numerical models, it has become more challenging to run complex simulations and computer codes [1,2]. As a consequence, surrogate models have been recognized as a key tool for engineering tasks such as design space exploration, uncertainty quantification, and optimization [3]. In practice, surrogate models are used to reduce the computational effort of these tasks by replacing expensive numerical simulations with closed-form approxi- mations [4, Ch. 10]. To build such a model, we start by evaluating the original expensive simulation at a set of points through a Design of Experiments (DoE). Then, the corresponding evaluations are used to build the surrogate model according to the chosen approximation, such as Kriging, quadratic interpolation, or least squares regression.",
    "The Surrogate Modeling Toolbox (SMT) is an open-source frame- work that provides functions to efficiently build surrogate models [5].",
    "\u2217 Corresponding author at: ONERA/DTIS, Universit\u00e9 de Toulouse, Toulouse, France.",
    "E-mail addresses: paul.saves@onera.fr (P. Saves), remi.lafage@onera.fr (R. Lafage), nathalie.bartoli@onera.fr (N. Bartoli), youssef.diouane@polymtl.ca",
    "(Y. Diouane), jasper.bussemaker@dlr.de (J. Bussemaker), thierry.lefebvre@onera.fr (T. Lefebvre), jhwang@eng.ucsd.edu (J.T. Hwang), joseph.morlier@isae-supaero.fr (J. Morlier), jrram@umich.edu (J.R.R.A. Martins).",
    "1 All authors contributed to this work, research and manuscript. 2 https://github.com/SMTorg/SMT",
    "https://doi.org/10.1016/j.advengsoft.2023.103571 Received 22 August 2023; Received in revised form 23 October 2023; Accepted 26 November 2023",
    "AdvancesinEngineeringSoftware188(2024)1035712",
    "P. Saves et al.",
    "Table 1 Comparison of software packages for hierarchical and mixed Kriging models. \u2713= implemented. * = user-defined.",
    "BOTorch",
    "Dakota",
    "DiceKriging",
    "KerGP",
    "LVGP",
    "Parmoo",
    "Spearmint",
    "SMT 2.0",
    "Package",
    "Reference License Language Mixed var. GD kernel CR kernel HH kernel EHH kernel Hierarchical var.",
    "[25] MIT Python \u2713 \u2713",
    "[26] EPL C \u2713 \u2713",
    "[27] GPL R \u2713 \u2713",
    "[32] GPL R \u2713 *",
    "[28] GPL R \u2713",
    "[29] BSD Python \u2713",
    "[30] GNU Python \u2713",
    "This paper BSD Python \u2713 \u2713 \u2713 \u2713 \u2713 \u2713",
    "\u2713",
    "\u2713",
    "\u2713",
    "\u2713 *",
    "and number of instantiated elements. When design problems include both discrete variables and continuous variables, they are said to have mixed variables.",
    "optimization (BO) with hierarchical and mixed variables or noisy co- Kriging that have been successfully applied to aircraft design [39], data fusion [40], and structural design [41]. The SMT 2.0 interface is more user-friendly and offers an improved and more detailed documentation for users and developers.3 SMT 2.0 is hosted publicly4 and can be directly imported within Python scripts. It is released under the New BSD License and runs on Linux, MacOS, and Windows operating sys- tems. Regression tests are run automatically for each operating system whenever a change is committed to the repository. In short, SMT 2.0 builds on the strengths of the original SMT package while adding new features. On one hand, the emphasis on derivatives (including predic- tion, training and output derivatives) is maintained and improved in SMT 2.0. On the other hand, this new release includes support for hierarchical and mixed variables Kriging based models. For the sake of reproducibility, an open-source notebook is available that gathers all the methods and results presented on this paper.5",
    "When architectural choices lead to different sets of design variables, we have hierarchical variables [21,22]. For example, consider differ- ent aircraft propulsion architectures [23]. A conventional gas turbine would not require a variable to represent a choice in the electrical power source, while hybrid or pure electric propulsion would require such a variable. The relationship between the choices and the sets of variables can be represented by a hierarchy.",
    "Handling hierarchical and mixed variables requires specialized sur- rogate modeling techniques [24]. To address these needs, SMT 2.0 is offering researchers and practitioners a collection of cutting-edge tools to build surrogate models with continuous, mixed and hierarchical variables. The main objective of this paper is to detail the new enhance- ments that have been added in this release compared to the original SMT 0.2 release [5].",
    "The remainder of the paper is organized as follows. First, we in- troduce the organization and the main implemented features of the release in Section 2. Then, we describe the mixed-variable Kriging model with an example in Section 3. Similarly, we describe and provide an example for a hierarchical-variable Kriging model in Section 4. The Bayesian optimization models and applications are described in Section 5. Finally, we describe the other relevant contributions in Section 6 and conclude in Section 7.",
    "There are two new major capabilities in SMT 2.0: the ability to build surrogate models involving mixed variables and the support for hierarchical variables within Kriging models. To handle mixed variables in Kriging models, existing libraries such as BoTorch [25], Dakota [26], DiceKriging [27], LVGP [28], Parmoo [29], and Spearmint [30] implement simple mixed models by using either continuous relax- ation (CR), also known as one-hot encoding [30], or a Gower distance (GD) based correlation kernel [31]. KerGP [32] (developed in R) imple- ments more general kernels but there is no Python open-source toolbox that implements more general kernels to deal with mixed variables, such as the homoscedastic hypersphere (HH) [33] and exponential homoscedastic hypersphere (EHH) [34] kernels. Such kernels require the tuning of a large number of hyperparameters but lead to more accurate Kriging surrogates than simpler mixed kernels [34,35]. SMT 2.0 implements all these kernels (CR, GD, HH, and EHH) through a unified framework and implementation. To handle hierarchical vari- ables, no library in the literature can build peculiar surrogate models except SMT 2.0, which implements two Kriging methods for these variables. Notwithstanding, most softwares are compatible with a na\u00efve strategy called the imputation method [24] but this method lacks depth and depends on arbitrary choices. This is why Hutter and Osborne [21] proposed a first kernel, called Arc-Kernel which in turn was generalized by Horn et al. [36] with a new kernel called the Wedge- Kernel [37]. None of these kernels are available in any open-source modeling software. Furthermore, thanks to the framework introduced in Audet et al. [38], our proposed kernels are sufficiently general so that all existing hierarchical kernels are included within it. Section 4 describes the two kernels implemented in SMT 2.0 that are referred as SMT Arc-Kernel and SMT Alg-Kernel. In particular, Alg- Kernel is a novel hierarchical kernel introduced in this paper. Table 1 outlines the main features of the state-of-the-art modeling software that can handle hierarchical and mixed variables.",
    "2. SMT 2.0 : an improved surrogate modeling toolbox",
    "From a software point of view, SMT 2.0 maintains and improves the modularity and generality of the original SMT version [5]. In this section, we describe the software as follows. Section 2.1 describes the legacy of SMT 0.2. Then, Section 2.2 describes the organization of the repository. Finally, Section 2.3 shows the new capabilities implemented in the SMT 2.0 update.",
    "2.1. Background on SMT former version: SMT 0.2",
    "SMT [5] is an open-source collaborative work originally developed by ONERA, NASA Glenn, ISAE-SUPAERO/ICA and the University of Michigan. Now, both Polytechnique Montr\u00e9al and the University of California San Diego are also contributors. SMT 2.0 updates and ex- tends the original SMT repository capabilities among which the original publication [5] focuses on different types of derivatives for surrogate models detailed hereafter.",
    "SMT 2.0 introduces other enhancements, such as additional sam- pling procedures, new surrogate models, new Kriging kernels (and their derivatives), Kriging variance derivatives, and an adaptive criterion for high-dimensional problems. SMT 2.0 adds applications of Bayesian",
    "3 http://smt.readthedocs.io/en/latest 4 https://github.com/SMTorg/smt 5 https://github.com/SMTorg/smt/tree/master/tutorial/",
    "NotebookRunTestCases_Paper_SMT_v2.ipynb",
    "AdvancesinEngineeringSoftware188(2024)1035713",
    "P. Saves et al.",
    "Table 2 Impact of using Numba on training time of the hierarchical Goldstein problem. Speedup is calculated excluding the JIT compilation table, as this step is only needed once after SMT installation.",
    "highlighted in blue and detailed on Fig. 1. The new major features implemented in SMT 2.0 are highlighted in lavender whereas the legacy features that were already in present in the original publication for SMT 0.2 [5] are in black.",
    "Training set",
    "Without numba",
    "Numba",
    "Speedup",
    "JIT overhead",
    "15 points 150 points",
    "1.3 s 38 s",
    "1.1 s 7.4 s",
    "15% 80%",
    "24 s 23 s",
    "2.3. New features within SMT 2.0",
    "The main objective of this new release is to enable Kriging surrogate models for use with both hierarchical and mixed variables. Moreover, for each of these five sub-modules described in Section 2.2, several improvements have been made between the original version and the SMT 2.0 release.",
    "A Python surrogate modeling framework with derivatives. One of the original main motivations for SMT was derivative support. In fact, none of the existing packages for surrogate modeling such as Scikit-learn in Python [7], SUMO in Matlab [42] or GPML in Matlab and Octave [43] focuses on derivatives. Three types of derivatives are distinguished: prediction derivatives, training derivatives, and output derivatives. SMT also includes new models with derivatives such as Kriging with Partial Least Squares (KPLS) [44] and Regularized Minimal-energy Tensor-product Spline (RMTS) [3]. These developed derivatives were even used in a novel algorithm called Gradient-Enhanced Kriging with Partial Least Squares (GEKPLS) [6] to use with adjoint methods, for example [45].",
    "Hierarchical and mixed design space. A new design space definition class DesignSpace has been added that implements hierarchical and mixed functionalities. Design variables can either be continu- ous (FloatVariable), ordered (OrdinalVariable) or categorical (CategoricalVariable). The integer type (IntegerVariable) rep- resents a special case of the ordered variable, specified by bounds (inclusive) rather than a list of possible values. The hierarchical struc- ture of the design space can be defined using declare_decreed_var: this function declares that a variable is a decreed variable that is activated when the associated meta variable takes one of a set of specified values, see Section 4 for background. The DesignSpace class also implements mechanisms for sampling valid design vectors (i.e. design vectors that adhere to the hierarchical structure of the design space) using any of the below-mentioned samplers, for cor- recting and imputing design vectors, and for requesting which design variables are acting in a given design vector. Correction ensures that variables have valid values (e.g. integers for discrete variables) [24], and imputation replaces non-acting variables by some default value (0 for discrete variables, mid-way between the bounds for continuous variables in SMT 2.0) [47].",
    "Software architecture, documentation, and automatic testing. SMT is orga- nized along three main sub-modules that implement a set of sampling techniques (sampling_methods), benchmarking functions (problems), and surrogate modeling techniques (surrogate_models). The toolbox documentation6 is created using reStructuredText and Sphinx, a doc- umentation generation package for Python, with custom extensions. Code snippets in the documentation pages are taken directly from actual tests in the source code and are automatically updated. The output from these code snippets and tables of options are generated dynamically by custom Sphinx extensions. This leads to high-quality documentation with minimal effort. Along with user documentation, developer documentation is also provided to explain how to contribute to SMT. This includes a list of API methods for the SurrogateModel, SamplingMethod, and Problem classes, that must be implemented to create a new surrogate modeling method, sampling technique, or benchmarking problem. When a developer submits a pull request, it is merged only after passing the automated tests and receiving approval from at least one reviewer. The repository on GitHub7 is linked to continuous integration tests (GitHub Actions) for Windows, Linux and MacOS, to a coverage test on coveralls.io and to a dependency version check for Python with DependaBot. Various parts of the source code have been accelerated using Numba [46], an LLVM-based just-in-time (JIT) compiler for numpy-heavy Python code. Numba is applied to con- ventional Python code using function decorators, thereby minimizing its impact on the development process and not requiring an additional build step. For a mixed Kriging surrogate with 150 training points, a speedup of up to 80% is observed, see Table 2. The JIT compilation step only needs to be done once when installing or upgrading SMT and adds an overhead of approximately 24 s on a typical workstation In this paper, all results are obtained using an Intel\u00ae Xeon\u00ae CPU E5-2650 v4 @ 2.20 GHz core and 128 GB of memory with a Broadwell- generation processor front-end and a compute node of a peak power of 844 GFlops.",
    "Sampling. SMT implements three methods for sampling. The first one is a na\u00efve approach, called Random that draws uniformly points along every dimension. The second sampling method is called Full Fac- torial and draws a point for every cross combination of variables, to have an \u2018\u2018exhaustive\u2019\u2019 design of experiments. The last one is the Latin Hypercube Sampling (LHS) [48] that draws a point in every Latin square parameterized by a certain criterion. For LHS, a new criterion to manage the randomness has been implemented and the sampling method was adapted for multi-fidelity and mixed or hierarchical variables. More details about the new sampling techniques are given in Section 6.1.",
    "Problems. SMT implements two new engineering problems: a mixed variant of a cantilever beam described in Section 3 and a hierarchical neural network described in Section 4.",
    "Surrogate models. In order to keep up with state-of-art, several re- leases done from the original version developed new options for the already existing surrogates. In particular, compared to the original publication [5], SMT 2.0 adds gradient-enhanced neural networks [45] and marginal Gaussian process [49] models to the list of available surrogates. More details about the new models are given in Section 6.2.",
    "2.2. Organization of SMT 2.0",
    "The main features of the open-source repository SMT 2.0 are described in Fig. 1. More precisely, Sampling Methods, Problems and Surrogate models are kept from SMT 0.2 and two new sections Models applications and Interactive notebooks have been added to the architecture of the code. These sections are",
    "Applications. Several applications have been added to the toolbox to demonstrate the surrogate models capabilities. The most relevant ap- plication is efficient global optimization (EGO), a Bayesian optimiza- tion algorithm [50,51]. EGO optimizes expensive-to-evaluate black-box problems with a chosen surrogate model and a chosen optimization criterion [52]. The usage of EGO with hierarchical and mixed variables is described in Section 5.",
    "6 https://smt.readthedocs.org 7 https://github.com/SMTorg/smt",
    "AdvancesinEngineeringSoftware188(2024)1035714",
    "P. Saves et al.",
    "Fig. 1. Functionalities of SMT 2.0. The new major features implemented in SMT 2.0 compared to SMT 0.2 are highlighted with the lavender color.",
    "3. Surrogate models with mixed variables in SMT 2.0",
    "Interactive notebooks. These tutorials introduce and explain how to use the toolbox for different surrogate models and applications.8 Every tutorial is available both as a .ipynb file and directly on Google colab.9 In particular, a hierarchical and mixed variables dedicated notebook is available to reproduce the results presented on this paper.10 In the following, Section 3 details the Kriging based surrogate models for mixed variables, and Section 4 presents our new Kriging surrogate for hierarchical variables. Section 5 details the EGO applica- tion and the other new relevant features aforementioned are described succinctly in Section 6.",
    "As mentioned in Section 1, design variables can be either of continu- ous or discrete type, and a problem with both types is a mixed-variable problem. Discrete variables can be ordinal or categorical. A discrete variable is ordinal if there is an order relation within the set of possible values. An example of an ordinal design variable is the number of engines in an aircraft. A possible set of values in this case could be 2, 4, 8. A discrete variable is categorical if no order relation is known between the possible choices the variable can take. One example of a categorical variable is the color of a surface. A possible example of a set of choices could be blue, red, green. The possible choices are called the levels of the variable.",
    "8 https://github.com/SMTorg/smt/tree/master/tutorial 9 https://colab.research.google.com/github/SMTorg/smt/ 10 https://github.com/SMTorg/smt/tree/master/tutorial/",
    "Several methods have been proposed to address the recent increase interest in mixed Kriging based models [30\u201333,35,39,53,54]. The main difference from a continuous Kriging model is in the estimation of",
    "NotebookRunTestCases_Paper_SMT_v2.ipynb",
    "AdvancesinEngineeringSoftware188(2024)1035715",
    "P. Saves et al.",
    "Table 3 Categorical kernels implemented in SMT 2.0.",
    "\ud835\udf05(\ud835\udf19)",
    "\ud835\udef7(\ud835\udee9\ud835\udc56) [\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udc57,\ud835\udc57 \u2236= 1 [\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udc57,\ud835\udc57 \u2236= [\ud835\udee9\ud835\udc56]\ud835\udc57,\ud835\udc57 ; [\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udc57\u2260\ud835\udc57\u2032 \u2236= 0 [\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udc57,\ud835\udc57 \u2236= 0 ; [\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udc57\u2260\ud835\udc57\u2032 \u2236= log \ud835\udf16 [\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udc57,\ud835\udc57 \u2236= 1 ; [\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udc57\u2260\ud835\udc57\u2032 \u2236= [\ud835\udc36(\ud835\udee9\ud835\udc56)\ud835\udc36(\ud835\udee9\ud835\udc56)\u22a4]\ud835\udc57,\ud835\udc57\u2032",
    "Name",
    "# of hyperparam.",
    "SMT GD SMT CR SMT EHH SMT HH",
    "exp(\u2212\ud835\udf19) exp(\u2212\ud835\udf19) exp(\u2212\ud835\udf19) \ud835\udf19",
    "\ud835\udf03\ud835\udc56 ; [\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udc57\u2260\ud835\udc57\u2032 \u2236= 0",
    "1 \ud835\udc3f\ud835\udc56 1 2 1 2",
    "2",
    "([\ud835\udc36(\ud835\udee9\ud835\udc56)\ud835\udc36(\ud835\udee9\ud835\udc56)\u22a4]\ud835\udc57,\ud835\udc57\u2032 \u2212 1)",
    "(\ud835\udc3f\ud835\udc56)(\ud835\udc3f\ud835\udc56 \u2212 1) (\ud835\udc3f\ud835\udc56)(\ud835\udc3f\ud835\udc56 \u2212 1)",
    "2",
    "Table 4 Results of the cantilever beam models [34, Table 4].",
    "the categorical correlation matrix, which is critical to determine the mean and variance predictions. As mentioned in Section 1, approaches such as CR [30,39], continuous latent variables [54], and GD [31] use a kernel-based method to estimate the correlation matrix. Other methods estimate the correlation matrix by modeling the correlation entries directly [32,35,53], such as HH [33] and EHH [34]. The HH correlation kernel is of particular interest because it generalizes simpler kernels such as CR and GD [34]. In SMT 2.0, the correlation kernel is an option that can be set to either CR (CONT_RELAX_KERNEL), GD (GOWER_KERNEL), HH (HOMO_HSPHERE_KERNEL) EHH (EXP_HOMO_HSPHERE_KERNEL).",
    "Categorical kernel",
    "Displacement error (cm)",
    "Likelihood",
    "# of hyperparam.",
    "SMT GD SMT CR SMT EHH SMT HH",
    "1.3861 1.1671 0.1613 0.2033",
    "111.13 155.32 236.25 235.66",
    "3 14 68 68",
    "or",
    "3.2. An engineering design test-case",
    "3.1. Mixed Gaussian processes",
    "A classic engineering problem commonly used for model validation is the beam bending problem [32,58]. This problem is illustrated on Fig. 2(a) and consists of a cantilever beam in its linear range loaded at its free end with a force \ud835\udc39 . As in Cheng et al. [58], the Young modulus is \ud835\udc38 = 200 GPa and the chosen load is \ud835\udc39 = 50 kN. Also, as in Roustant et al. [32], 12 possible cross-sections can be used. These 12 sections consist of 4 possible shapes that can be either hollow, thick or full as illustrated in Fig. 2(b).",
    "The continuous and ordinal variables are both treated similarly in SMT 2.0 with a continuous kernel, where the ordinal values are converted to continuous through relaxation. For categorical variables, four models (GD, CR, EHH and HH) can be used in SMT 2.0 if specified by the API. This is why we developed a unified mathematical formulation that allows a unique implementation for any model.",
    "Denote \ud835\udc59 the number of categorical variables. For a given \ud835\udc56 \u2208 {1, \u2026 , \ud835\udc59}, the \ud835\udc56th categorical variable is denoted \ud835\udc50\ud835\udc56 and its number of levels is denoted \ud835\udc3f\ud835\udc56. The hyperparameter matrix peculiar to this variable \ud835\udc50\ud835\udc56 is",
    "To compare the mixed Kriging models of SMT 2.0, we draw a 98 point LHS as training set and the validation set is a grid of 12 \u00d7 30 \u00d7 30 = 10800 points. For the four implemented methods, displacement error (computed with a root-mean-square error criterion), likelihood, number of hyperparameters and computational time for every model are shown in Table 4. For the continuous variables, we use the square exponential kernel. More details are found in [34]. As expected, the complex EHH and HH models lead to a lower displacement error and a higher likelihood value, but use more hyperparameters and increase the computational cost compared to GD and CR. On this test case, the kernel EHH is easier to optimize than HH but in general, they are similar in terms of performance. Also, by default SMT 2.0 uses CR as it is known to be a good trade-off between complexity and performance [59].",
    "[\ud835\udee9\ud835\udc56]1,1 \u23a1 \u23a2 [\ud835\udee9\ud835\udc56]1,2 \u23a2 \u22ee \u23a2 \u23a2 [\ud835\udee9\ud835\udc56]1,\ud835\udc3f\ud835\udc56 \u23a3",
    "\ud835\udc7a\ud835\udc9a\ud835\udc8e.",
    "\u23a4 \u23a5 \u23a5 \u23a5 \u23a5 \u23a6",
    "[\ud835\udee9\ud835\udc56]2,2 \u22f1 \u2026",
    "\ud835\udee9\ud835\udc56 =",
    ",",
    "\u22f1 [\ud835\udee9\ud835\udc56]\ud835\udc3f\ud835\udc56\u22121,\ud835\udc3f\ud835\udc56",
    "[\ud835\udee9\ud835\udc56]\ud835\udc3f\ud835\udc56,\ud835\udc3f\ud835\udc56",
    "and the categorical parameters are defined as \ud835\udf03\ud835\udc50\ud835\udc4e\ud835\udc61 = {\ud835\udee91, \u2026 , \ud835\udee9\ud835\udc59}. For two given inputs in the DoE, for example, the \ud835\udc5fth and \ud835\udc60th points, let \ud835\udc56 and \ud835\udc50\ud835\udc60 \ud835\udc50\ud835\udc5f \ud835\udc56 be the associated categorical variables taking respectively the \ud835\udcc1\ud835\udc56 \ud835\udc60 level on the categorical variable \ud835\udc50\ud835\udc56. The categorical correlation kernel is defined by \ud835\udc58\ud835\udc50\ud835\udc4e\ud835\udc61(\ud835\udc50\ud835\udc5f, \ud835\udc50\ud835\udc60, \ud835\udf03\ud835\udc50\ud835\udc4e\ud835\udc61) =",
    "\ud835\udc5f and the \ud835\udcc1\ud835\udc56",
    "\ud835\udc59 \u220f",
    "(1)",
    "4. Surrogate models with hierarchical variables in SMT 2.0",
    "\ud835\udf05([\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udcc1\ud835\udc5f",
    ") \ud835\udf05([\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udcc1\ud835\udc60",
    ")\ud835\udf05([\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udcc1\ud835\udc5f",
    ")\ud835\udf05([\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udcc1\ud835\udc60",
    ")",
    "\ud835\udc56 ,\ud835\udcc1\ud835\udc60 \ud835\udc56",
    "\ud835\udc56 ,\ud835\udcc1\ud835\udc60 \ud835\udc56",
    "\ud835\udc56 ,\ud835\udcc1\ud835\udc5f \ud835\udc56",
    "\ud835\udc56 ,\ud835\udcc1\ud835\udc5f \ud835\udc56",
    "\ud835\udc56=1",
    "To introduce the newly developed Kriging model for hierarchical variables implemented in SMT 2.0, we present the general mathe- matical framework for hierarchical and mixed variables established by Audet et al. [38]. In SMT 2.0, two variants of our new method are implemented, namely SMT Alg-Kernel and SMT Arc-Kernel. In particular, the SMT Alg-Kernel is a novel correlation kernel introduced in this paper.",
    "where \ud835\udf05 is either a positive definite kernel or identity and \ud835\udef7(.) is a symmetric positive definite (SPD) function such that the matrix \ud835\udef7(\ud835\udee9\ud835\udc56) is SPD if \ud835\udee9\ud835\udc56 is SPD. For an exponential kernel, Table 3 gives the parameterizations of \ud835\udef7 and \ud835\udf05 that correspond to GD, CR, HH, and EHH kernels. The complexity of these different kernels depends on the number of hyperparameters that characterizes them. As defined by Saves et al. [34], for every categorical variable \ud835\udc56 \u2208 {1, \u2026 , \ud835\udc59}, the matrix \ud835\udc36(\ud835\udee9\ud835\udc56) \u2208 R\ud835\udc3f\ud835\udc56\u00d7\ud835\udc3f\ud835\udc56 is lower triangular and built using a hypersphere decomposition [55,56] from the symmetric matrix \ud835\udee9\ud835\udc56 \u2208 R\ud835\udc3f\ud835\udc56\u00d7\ud835\udc3f\ud835\udc56 of hyperparameters. The variable \ud835\udf16 is a small positive constant and the variable \ud835\udf03\ud835\udc56 denotes the only positive hyperparameter that is used for the Gower distance kernel.",
    "4.1. The hierarchical variables framework",
    "A problem structure is classified as hierarchical when the sets of active variables depend on architectural choices. This occurs frequently in industrial design problems. In hierarchical problems, we can classify variables as neutral, meta (also known as dimensional) or decreed (also known as conditionally active) as detailed in Audet et al. [38]. Neutral variables are the variables that are not affected by the hierarchy whereas the value assigned to meta variables determines which decreed variables are activated. For example, a meta variable could be the number of engines. If the number of engines changes, the number of decreed bypass ratios that every engine should specify also changes.",
    "Another Kriging based model that can use mixed variables is Kriging with partial least squares (KPLS) [57]. KPLS adapts Kriging to high dimensional problems by using a reduced number of hyperparameters thanks to a projection into a smaller space. Also, for a general surrogate, not necessarily Kriging, SMT 2.0 uses continuous relaxation to allow whatever model to handle mixed variables. For example, we can use mixed variables with least squares (LS) or quadratic polynomial (QP) models. We now illustrate the abilities of the toolbox in terms of mixed modeling over an engineering test case.",
    "AdvancesinEngineeringSoftware188(2024)1035716",
    "P. Saves et al.",
    "Fig. 2. Cantilever beam problem [34, Figure 6].",
    "Fig. 3. Variables classification as used in SMT 2.0.",
    "2. The number of neurons in the hidden layer number \ud835\udc58 is either included or excluded. For example, the \u2018\u2018# of neurons in the 3rd layer\u2019\u2019 would be excluded for an input that only has 2 hidden layers. Therefore, \u2018\u2018# of neurons hidden layer \ud835\udc58\u2019\u2019 are decreed variables.",
    "However, the wing aspect ratio being neutral, it is not affected by this hierarchy.",
    "Problems involving hierarchical variables are generally dependant on discrete architectures and as such involve mixed variables. Hence, in addition to their role (neutral, meta or decreed), each variable also has a variable type amongst categorical, ordinal or continuous. For the sake of simplicity and because both continuous and ordinal variables are treated similarly [34], we chose to regroup them as quantitative variables. For instance, the neutral variables \ud835\udc65neu may be partitioned into different variable types, such that \ud835\udc65neu = (\ud835\udc65cat neu) where \ud835\udc65cat neu represents the categorical variables and \ud835\udc65qnt neu are the quantitative ones. The variable classification scheme in SMT 2.0 is detailed in Fig. 3.",
    "3. The \u2018\u2018Learning rate\u2019\u2019, \u2018\u2018Momentum\u2019\u2019, \u2018\u2018Activation function\u2019\u2019 and \u2018\u2018Batch size\u2019\u2019 are not affected by the hierarchy choice. Therefore, they are neutral variables.",
    "neu, \ud835\udc65qnt",
    "According to their types, the MLP input variables can be classified as follows:",
    "4. The meta variable \u2018\u2018# of hidden layers\u2019\u2019 is an integer and, as such, is represented by the component \ud835\udc65qnt met .",
    "To explain the framework and the new Kriging model, we illustrate the inputs variables of the model using a classical machine learn- ing problem related to the hyperparameters optimization of a fully- connected Multi-Layer Perceptron (MLP) [38] on Fig. 4. In Table 5, we detail the input variables of the model related to the MLP problem (i.e., the hyperparameters of the neural network, together with their types and roles). To keep things clear and concise, the chosen problem is a simplification of the original problem developed by Audet et al. [38]. Regarding the MLP problem of Fig. 4 and following the classi- fication scheme of Fig. 3, we start by separating the input variables according to their role. In fact,",
    "5. The decreed variables \u2018\u2018# of neurons hidden layer \ud835\udc58\u2019\u2019 are integers and, as such, are represented by the component \ud835\udc65qnt dec.",
    "6. The \u2018\u2018Learning rate\u2019\u2019, \u2018\u2018Momentum\u2019\u2019, \u2018\u2018Activation function\u2019\u2019 and \u2018\u2018Batch size\u2019\u2019 are, respectively, continuous, for the first two (ev- ery value between two bounds), categorical (qualitative between three choices) and integer (quantitative between 6 choices). Therefore, the \u2018\u2018Activation function\u2019\u2019 and the \u2018\u2018Momentum\u2019\u2019 are represented by the component \ud835\udc65cat neu. The \u2018\u2018Learning rate\u2019\u2019 and the \u2018\u2018Batch size\u2019\u2019 are represented by the component \ud835\udc65qnt neu. \ue244",
    "6. The \u2018\u2018Learning rate\u2019\u2019, \u2018\u2018Momentum\u2019\u2019, \u2018\u2018Activation function\u2019\u2019 and \u2018\u2018Batch size\u2019\u2019 are, respectively, continuous, for the first two (ev- ery value between two bounds), categorical (qualitative between three choices) and integer (quantitative between 6 choices). Therefore, the \u2018\u2018Activation function\u2019\u2019 and the \u2018\u2018Momentum\u2019\u2019 are represented by the component \ud835\udc65cat neu. The \u2018\u2018Learning rate\u2019\u2019 and the \u2018\u2018Batch size\u2019\u2019 are represented by the component \ud835\udc65qnt neu. \ue244",
    "1. changing the number of hidden layers modifies the number of inputs variables. Therefore, \u2018\u2018# of hidden layers\u2019\u2019 is a meta variable.",
    "To model hierarchical variables, as proposed in [38], we separate inc(\ud835\udc65met ).",
    "\ud835\udc65met \u2208\ue244",
    "met",
    "AdvancesinEngineeringSoftware188(2024)1035717",
    "P. Saves et al.",
    "Fig. 4. The Multi-Layer Perceptron (MLP) problem. Source: Figure adapted from [38, Figure 1].",
    "Table 5 A detailed description of the variables in the MLP problem.",
    "4.2. A Kriging model for hierarchical variables",
    "MLP Hyperparameters",
    "Variable",
    "Domain",
    "Type",
    "Role",
    "In this section, a new method to build a Kriging model with hierar- chical variables is introduced based on the framework aforementioned. The proposed methods are included in SMT 2.0.",
    "[10\u22125, 10\u22122] [0, 1] {\ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc48 , \ud835\udc46\ud835\udc56\ud835\udc54\ud835\udc5a\ud835\udc5c\ud835\udc56\ud835\udc51, \ud835\udc47 \ud835\udc4e\ud835\udc5b\u210e}",
    "\ud835\udc5f \ud835\udefc \ud835\udc4e",
    "Learning rate Momentum Activation function Batch size # of hidden layers # of neurons hidden layer \ud835\udc58",
    "FLOAT FLOAT ENUM",
    "NEUTRAL NEUTRAL NEUTRAL",
    "4.2.1. Motivation and state-of-the-art",
    "\ud835\udc4f \ud835\udc59",
    "{8, 16, \u2026 , 128, 256} {1, 2, 3}",
    "ORD ORD",
    "NEUTRAL META",
    "Assuming that the decreed variables are quantitative, Hutter and Osborne [21] proposed several kernels for the hierarchical context. A classic approach, called the imputation method (Imp-Kernel) leads to an efficient paradigm in practice that can be easily integrated into a more general framework as proposed by Bussemaker et al. [24]. However, this kernel lacks depth and depends on arbitrary choices. Therefore, Hutter and Osborne [21] also proposed a more general kernel, called Arc-Kernel and Horn et al. [36] generalized this kernel even more and proposed a new formulation called the Wedge- Kernel [37]. The drawbacks of these two methods are that they add some extra hyperparameters for every decreed dimension (respectively one extra hyperparameter for the Arc-Kernel and two hyperparam- eters for the Wedge-Kernel) and that they need a normalization according to the bounds. More recently, Pelamatti et al. [60] developed a hierarchical kernel for Bayesian optimization. However, our work is also more general thanks to the framework introduced earlier [38] that considers variable-wise formulation and is more flexible as we are allowing sub-problems to be intersecting.",
    "\ud835\udc5b\ud835\udc58",
    "{50, 51, \u2026 , 55}",
    "ORD",
    "DECREED",
    "Hence, for a given point \ud835\udc65 \u2208 \ue244, one has \ud835\udc65 = (\ud835\udc65neu, \ud835\udc65met , \ud835\udc65inc(\ud835\udc65met )), where \ud835\udc65neu \u2208 \ue244 inc(\ud835\udc62met ) are defined as follows:",
    "neu, \ud835\udc65met \u2208 \ue244",
    "met and \ud835\udc65inc(\ud835\udc62met ) \u2208 \ue244",
    "The components \ud835\udc65neu \u2208 \ue244",
    "neu gather all neutral variables that are not impacted by the meta variables but needed. For ex- ample, in the MLP problem, \ue244 neu gathers the possible learning rates, momentum, activation functions and batch sizes. Namely, from Table 5, \ue244 neu = [10\u22125, 10\u22122] \u00d7 [0, 1] \u00d7 {ReLu, Sigmoid, Tanh} \u00d7 {8, 16, \u2026 , 256}.",
    "The components \ud835\udc65met gather the meta (also known as dimen- sional) variables that determine the inclusion or exclusion of other variables. For example, in the MLP problem, \ue244 met represents the possible numbers of layers in the MLP. Namely, from Table 5, \ue244 met = {1, 2, 3}.",
    "In the following, we describe our new method to build a correlation kernel for hierarchical variables. In particular, we introduce a new alge- braic kernel called Alg-Kernel that behaves like the Arc-Kernel whilst correcting most of its drawbacks. In particular, our kernel does not add any hyperparameters, and the normalization is handled in a natural way.",
    "The components \ud835\udc65inc(\ud835\udc65met ), contain the decreed variables whose inclusion (decreed-included) or exclusion (decreed-excluded) is determined by the values of the meta components \ud835\udc65met . For exam- ple, in the MLP problem, \ue244 dec represents the number of neurons in the decreed layers. Namely, from Table 5, \ue244 inc(\ud835\udc65met = 3) = inc(\ud835\udc65met = 2) = [50, 55]2 and \ue244 [50, 55]3, \ue244 inc(\ud835\udc65met = 1) = [50, 55].",
    "4.2.2. A new hierarchical correlation kernel",
    "For modeling purposes, we assume that the decreed space is quan- dec . Let \ud835\udc62 \u2208 \ue244 be an input point partitioned as",
    "dec = \ue244 qnt",
    "titative, i.e., \ue244",
    "AdvancesinEngineeringSoftware188(2024)1035718",
    "P. Saves et al.",
    "\ud835\udc58alg met,dec. For every \ud835\udc56 \u2208 \ud835\udc3cdec, if \ud835\udc56 \u2208 \ud835\udc3c \ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f given by",
    "\ud835\udc62 = (\ud835\udc62neu, \ud835\udc62met , \ud835\udc62inc(\ud835\udc62met )) and, similarly, \ud835\udc63 \u2208 \ue244 is another input such that \ud835\udc63 = (\ud835\udc63neu, \ud835\udc63met , \ud835\udc63inc(\ud835\udc63met )). The new kernel \ud835\udc58 that we propose for hierarchical variables is given by",
    "\ud835\udc62,\ud835\udc63 , the new algebraic distance is",
    "\u239b \u239c \u239c \u239c \u239d",
    "\u239e \u239f \u239f \u239f \u23a0",
    "2|[\ud835\udc62inc(\ud835\udc62met )]\ud835\udc56 \u2212 [\ud835\udc63inc(\ud835\udc63met )]\ud835\udc56| \u221a",
    "\ud835\udc58(\ud835\udc62, \ud835\udc63) = \ud835\udc58neu(\ud835\udc62neu, \ud835\udc63neu) \u00d7 \ud835\udc58met (\ud835\udc62met , \ud835\udc63met )",
    "\ud835\udc51alg([\ud835\udc62inc(\ud835\udc62met )]\ud835\udc56, [\ud835\udc63inc(\ud835\udc63met )]\ud835\udc56) =",
    "\ud835\udf03\ud835\udc56,",
    "\u221a",
    "\u00d7 \ud835\udc58met,dec([\ud835\udc62met , \ud835\udc62inc(\ud835\udc62met )], [\ud835\udc63met , \ud835\udc63inc(\ud835\udc63met )]),",
    "(2)",
    "2 + 1",
    "2 + 1",
    "[\ud835\udc62inc(\ud835\udc62met )]\ud835\udc56",
    "[\ud835\udc63inc(\ud835\udc63met )]\ud835\udc56",
    "where \ud835\udc58neu, \ud835\udc58met and \ud835\udc58met,dec are as follows:",
    "(6)",
    "\ud835\udc58neu represents the neutral kernel that encompasses both categor- ical and quantitative neutral variables, i.e., \ud835\udc58neu can be decom- posed into two parts \ud835\udc58neu(\ud835\udc62neu, \ud835\udc63neu) = \ud835\udc58cat (\ud835\udc62cat neu). The categorical kernel, denoted \ud835\udc58cat , could be any Symmetric Positive Definite (SPD) [34] mixed kernel (see Section 3). For the quantitative (integer or continuous) variables, a distance- based kernel is used. The chosen quantitative kernel (Exponential, Mat\u00e9rn, . . . ), always depends on a given distance \ud835\udc51. For example, the \ud835\udc5b-dimensional exponential kernel gives )).",
    "where \ud835\udf03\ud835\udc56 \u2208 R+ is a continuous hyperparameter. Otherwise, if \ud835\udc56 \u2208 \ud835\udc3cdec but \ud835\udc56 \u2209 \ud835\udc3c \ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f \ud835\udc62,\ud835\udc63 , there should be a non-zero residual distance between the two different subspaces \ue244 inc(\ud835\udc62met ) and \ue244 inc(\ud835\udc63met ) to ensure the kernel SPD property. To have a residual not depending on the decreed values, our model considers that there is a unit distance",
    "\ud835\udc51alg([\ud835\udc62inc(\ud835\udc62met )]\ud835\udc56, [\ud835\udc63inc(\ud835\udc63met )]\ud835\udc56) = 1.0 \ud835\udf03\ud835\udc56, \u2200\ud835\udc56 \u2208 \ud835\udc3cdec \u29f5 \ud835\udc3c \ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f \ud835\udc62,\ud835\udc63 .",
    "The induced meta kernel \ud835\udc58alg of \ud835\udc58alg is defined as:",
    "met (\ud835\udc62met , \ud835\udc63met ) to preserve the SPD property",
    "\u220f",
    "(3)",
    "\ud835\udc58alg met (\ud835\udc62met , \ud835\udc63met ) =",
    "\ud835\udc58qnt (1.0 \ud835\udf03\ud835\udc56).",
    "(7)",
    "\ud835\udc56\u2208\ud835\udc3cmet",
    "\ud835\udc58met is the meta variables related kernel. It is also separated into two parts: \ud835\udc58met (\ud835\udc62met , \ud835\udc63met ) = \ud835\udc58cat (\ud835\udc62cat met ) where the quantitative kernel is ordered and not continuous because meta variables take value in a finite set. met , \ud835\udc63cat",
    "Not only our kernel of Eq. (2) uses less hyperparameters than the Arc- Kernel (as we cut off its extra parameters) but it is also a more flexible kernel as it allows different kernels for meta and decreed variables. Moreover, another advantage of our kernel is that it is numerically more stable thanks to the new non-stationary [61] algebraic distance defined in Eq. (7) [62]. Our proposed distance also does not need any rescaling by the bounds to have values between 0 and 1.",
    "\ud835\udc58met,dec is an SPD kernel that models the correlations between the meta levels (all the possible subspaces) and the decreed variables. In what comes next, we detailed this kernel.",
    "In what comes next, we will refer to the implementation of the kernels Arc-Kernel and Alg-Kernel by SMT Arc-Kernel and SMT Alg-Kernel. We note also that the implementation of SMT Arc-Kernel differs slightly from the original Arc-Kernel as we fixed some hyperparameters to 1 in order to avoid adding extra hy- perparameters and use the formulation of Eq. (2) and rescaling of the data.",
    "4.2.3. Towards an algebraic meta-decreed kernel",
    "the the Arc-Kernel were first proposed in [21,47] and the distance-based kernels such as Arc-Kernel or Wedge-Kernel [37] were proven to be SPD. Nevertheless, to guarantee this SPD property, the same hyperparameters are used to model the correlations between the meta levels and between the decreed variables [47]. For this reason, the Arc-Kernel includes additional continuous hyperparameters which makes the training of the GP models more expensive and introduces more numerical stability issues. In this context, we are proposing a new algebraic meta-decreed kernel (denoted as Alg-Kernel) that enjoys similar properties as Arc-Kernel but without using additional continuous hyperparameters nor rescaling. In the SMT 2.0 release, we included Alg-Kernel and a simpler version of Arc-Kernel that do not relies on additional hyperparameters.",
    "Meta-decreed kernels",
    "like",
    "imputation kernel",
    "or",
    "4.2.4. Illustration on the MLP problem",
    "In this section, we illustrate the hierarchical Arc-Kernel on the MLP example. For that sake, we consider two design variables \ud835\udc62 and \ud835\udc63 such that \ud835\udc62 = (2.10\u22124, 0.9, ReLU, 16, 2, 55, 51) and \ud835\udc63 = (5.10\u22123, 0.8, Sigmoid, 64, 3, 50, 54, 53). Since the value of \ud835\udc62met (i.e., the number of hidden layers) differs from one point to another (namely, 2 for \ud835\udc62 and 3 for \ud835\udc63), the associated variables \ud835\udc62inc(\ud835\udc62met ) have either 2 or 3 variables for the number of neurons in each layer (namely 55 and 51 for \ud835\udc62, and 50, 54 and 53 for the point \ud835\udc63). In our case, 8 hyperparame- ters ([\ud835\udc451]1,2, \ud835\udf031, \u2026 , \ud835\udf037) will have to be optimized where \ud835\udc58 is given by Eq. (2). These 7 hyperparameters can be described using our proposed framework as follows:",
    "Our proposed Alg-Kernel kernel is given by",
    "\ud835\udc58alg met,dec([\ud835\udc62met , \ud835\udc62inc(\ud835\udc62met )], [\ud835\udc63met , \ud835\udc63inc(\ud835\udc63met )])",
    "(4)",
    "= \ud835\udc58alg",
    "met (\ud835\udc62met , \ud835\udc63met ) \u00d7 \ud835\udc58alg",
    "dec(\ud835\udc62inc(\ud835\udc62met ), \ud835\udc63inc(\ud835\udc63met )). Mathematically, we could consider that there is only one meta variable whose levels correspond to every possible included subspace. Let \ud835\udc3csub denotes the components indices of possible subspaces, the subspaces parameterized by the meta component \ud835\udc62met are defined as \ue244 inc(\ud835\udc62met = \ud835\udc59 \u2208 \ud835\udc3csub. It follows that the fully extended continuous decreed \ud835\udc59), space writes as \ue244 inc(\ud835\udc62met = \ud835\udc59) and \ud835\udc3cdec is the set of the \ud835\udc59\u2208\ud835\udc3csub associated indices. Let \ud835\udc3c \ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f \ud835\udc62,\ud835\udc63 denotes the set of components related to the space \ue244 inc(\ud835\udc62met , \ud835\udc63met ) containing the variables decreed-included in both \ue244",
    "For the neutral components, we have \ud835\udc62neu = (2.10\u22124, 0.9, ReLU, 16) and \ud835\udc63neu = (5.10\u22123, 0.8, Sigmoid, 64). Therefore, for a categorical matrix kernel \ud835\udc451 and a square exponential quantitative kernel, neu)\ud835\udc58qnt (\ud835\udc62qnt = [\ud835\udc451]1,2 exp [\u2212\ud835\udf031(2.10\u22124 \u2212 5.10\u22123)2] exp [\u2212\ud835\udf032(0.9 \u2212 0.8)2] exp [\u2212\ud835\udf033(16 \u2212 64)2].",
    "dec = \u22c3",
    "\ue244",
    "inc(\ud835\udc62met ) and \ue244",
    "inc(\ud835\udc63met ).",
    "The values [\ud835\udc451]1,2, \ud835\udf031, \ud835\udf032 and \ud835\udf033 need to be optimized. Here, [\ud835\udc451]1,2 is the correlation between \"ReLU\" and \"Sigmoid\".",
    "Since the decreed variables are quantitative, one has",
    "\ud835\udc58alg dec(\ud835\udc62inc(\ud835\udc62met ), \ud835\udc63inc(\ud835\udc63met )) = \ud835\udc58qnt (\ud835\udc62inc(\ud835\udc62met ), \ud835\udc63inc(\ud835\udc63met )) =",
    "For the meta components, we have \ud835\udc62met = 2 and \ud835\udc63met = 3. Therefore, for a square exponential quantitative kernel,",
    "\u220f",
    "(5)",
    "\ud835\udc58qnt ([\ud835\udc62inc(\ud835\udc62met )]\ud835\udc56, [\ud835\udc63inc(\ud835\udc63met )]\ud835\udc56)",
    "\ud835\udc56\u2208\ud835\udc3c \ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f \ud835\udc62,\ud835\udc63",
    "met )\ud835\udc58qnt (\ud835\udc62qnt",
    "met , \ud835\udc63qnt met )",
    "\ud835\udc58met (\ud835\udc62met , \ud835\udc63met ) = \ud835\udc58cat (\ud835\udc62cat",
    "met , \ud835\udc63cat = exp [\u2212\ud835\udf034(3 \u2212 2)2].",
    "The construction of the quantitative kernel \ud835\udc58qnt depends on a given distance denoted \ud835\udc51alg. The kernel \ud835\udc58alg met is an induced meta kernel that depends on the same distance \ud835\udc51alg to preserve the SPD property of",
    "The value \ud835\udf034 needs to be optimized.",
    "AdvancesinEngineeringSoftware188(2024)1035719",
    "P. Saves et al.",
    "For the meta-decreed kernel, we have [\ud835\udc62met , \ud835\udc62inc(\ud835\udc62met )] = [2, (55, 51)] and [\ud835\udc63met , \ud835\udc63inc(\ud835\udc63met )] = [3, (50, 54, 53)] which gives",
    "Table 6 Results on the neural network model.",
    "Hierarchical method",
    "Prediction error (RMSE)",
    "Likelihood",
    "# of hyperparam.",
    "\ud835\udc58alg met,dec([\ud835\udc62met , \ud835\udc62inc(\ud835\udc62met )], [\ud835\udc63met , \ud835\udc63inc(\ud835\udc63met )]) dec((55, 51), (50, 54, 53)). (",
    "SMT Alg-kernel SMT Arc-kernel Imp-Kernel",
    "3.7610 4.9208 4.5455",
    "176.11 162.01 170.64",
    "10 10 10",
    "= \ud835\udc58alg",
    "met (2, 3) \ud835\udc58alg",
    ")",
    "2\u00d7|51\u221254| \ud835\udf036 = 2.178.10\u22123 \ud835\udf036. In The distance \ud835\udc51alg(51, 54) = \u221a 512+1 general, for surrogate models, and in particular in SMT 2.0, the input data are normalized. With a unit normalization from [50, 55] to [0, 1], we would have \ud835\udc51alg(0.2, 0.8) =",
    "\u221a",
    "542+1",
    "our modeling method is Bayesian optimization to perform quickly the hyperparameter optimization of a neural network [63].",
    "(",
    ")",
    "2\u00d70.6 \u221a",
    "\ud835\udf036 = 0.919 \ud835\udf036. Similarly, we have, between 55 and 50, \ud835\udc51alg(0, 1) = 1.414 \ud835\udf035. Hence, for a square exponential quantitative kernel, one gets",
    "\u221a",
    "0.22+1",
    "0.62+1",
    "5. Bayesian optimization within SMT 2.0",
    "\ud835\udc58alg met,dec([\ud835\udc62met , \ud835\udc62inc(\ud835\udc62met )], [\ud835\udc63met , \ud835\udc63inc(\ud835\udc63met )])",
    "Efficient global optimization (EGO) is a sequential Bayesian op- timization algorithm designed to find the optimum of a black-box function that may be expensive to evaluate [52]. EGO starts by fitting a Kriging model to an initial DoE, and then uses an acquisition function to select the next point to evaluate. The most used acquisition function is the expected improvement. Once a new point has been evaluated, the Kriging model is updated. Successive updates increase the model accu- racy over iterations. This enrichment process repeats until a stopping criterion is met.",
    "= exp [\u2212\ud835\udf037] \u00d7 exp [\u22121.414 \ud835\udf035] \u00d7 exp [\u22120.919 \ud835\udf036],",
    "where the meta induced component is \ud835\udc58alg met (\ud835\udc62met , \ud835\udc63met ) = exp [\u2212\ud835\udf037] because the decreed value 53 in \ud835\udc63 has nothing to be compared with in \ud835\udc62 as in Eq. (7). The values \ud835\udf035, \ud835\udf036 and \ud835\udf037 need to be opti- mized which complete the description of the hyperparameters. We note that for the MLP problem, Alg-Kernel models use 10 hyperparameters whereas the Arc-Kernel would require 12 hyperparameters without the meta kernel (\ud835\udf034) but with 3 extra decreed hyperparameters and the Wedge-Kernel would require 15 hyperparameters. For deep learning applications, a more complex perceptron with up to 10 hidden layers would require 17 hyperparameters with SMT 2.0 models against 26 for Arc-Kernel and 36 for Wedge-Kernel. The next section illustrates the interest of our method to build a surrogate model for this neural network engineering problem.",
    "Because SMT 2.0 implements Kriging models that handle mixed and hierarchical variables, we can use EGO to solve problems in- volving such design variables. Other Bayesian optimization algorithms often adopt approaches based on solving subproblems with contin- uous or non-hierarchical Kriging. This subproblem approach is less efficient and scales poorly, but it can only solve simple problems. Several Bayesian optimization software packages can handle mixed or hierarchical variables with such a subproblem approach. The pack- ages include BoTorch [25], SMAC [65], Trieste [66], HEBO [67], OpenBox [68], and Dragonfly [69].",
    "4.3. A neural network test-case using SMT 2.0",
    "In this section, we apply our models to the hyperparameters opti- mization of a MLP problem aforementioned of Fig. 4. Within SMT 2.0 an example illustrates this MLP problem. For the sake of showing the Kriging surrogate abilities, we implemented a dummy function with no significance to replace the real black-box that would require training a whole Neural Network (NN) with big data. This function requires a number of variables that depends on the value of the meta variable, i.e the number of hidden layers. To simplify, we have chosen only 1, 2 or 3 hidden layers and therefore, we have 3 decreed variables but deep neural networks could also be investigated as our model can tackle a few dozen variables. A test case (test_hierarchical_variables_NN ) shows that our model SMT Alg-Kernel interpolates the data prop- erly, checks that the data dimension is correct and also asserts that the inactive decreed variables have no influence over the prediction. In Fig. 5 we illustrate the usage of Kriging surrogates with hierarchical and mixed variables based on the implementation of SMT 2.0 for test_hierarchical_variables_NN.",
    "5.1. A mixed optimization problem",
    "Fig. 6 compares the four EGO methods implemented in SMT 2.0: SMT GD, SMT CR, SMT EHH and SMT HH. The mixed test case that illustrates Bayesian optimization is a toy test case [64] detailed in Ap- pendix A. This test case has two variables, one continuous and one categorical with 10 levels. To assess the performance of our algorithm, we performed 20 runs with different initial DoE sampled by LHS. Every DoE consists of 5 points and we chose a budget of 55 infill points. Fig. 6(a) plots the convergence curves for the four methods. In particular, the median is the solid line, and the first and third quantiles are plotted in dotted lines. To visualize better the data dispersion, the boxplots of the 20 best solutions after 20 evaluations are plotted in Fig. 6(b). As expected, the more a method is complex, the better the optimization. Both SMT HH and SMT EHH converged in around 18 evaluations whereas SMT CR and SMT GD take around 26 iterations as shown on Fig. 6(a). Also, the more complex the model, the closer the optimum is to the real value as shown on Fig. 6(b).",
    "To compare the hierarchical models of SMT 2.0 (SMT Alg-Kernel and SMT Arc-Kernel) with the state-of-the-art imputation method previously used on industrial application (Imp-Kernel) [24], we draw a 99 point LHS (33 points by meta level) as a training set and the validation set is a LHS of 3 \u00d7 1000 = 3000 points. For the Imp-Kernel, the decreed-excluded values are replaced by 52 because the mean value 52.5 is not an integer (by default, SMT rounds to the floor value). For the three methods, the precision (computed with a root-mean- square error RMSE criterion), the likelihood and the computational time are shown in Table 6. For this problem, we can see that SMT Alg- kernel gives better performance than the imputation method or SMT Arc-kernel. Also, as all methods use the same number of hyperpa- rameters, they have similar time performances. A direct application of",
    "In Fig. 7 we illustrate how to use EGO with mixed variables based on the implementation of SMT 2.0. The illustrated problem is a mixed variant of the Branin function [70].",
    "Note that a dedicated notebook is available to reproduce the results presented in this paper and the mixed integer notebook also includes an extra mechanical application with composite materials [41].11",
    "11 https://colab.research.google.com/github/SMTorg/smt/blob/master/",
    "tutorial/SMT_MixedInteger_application.ipynb",
    "AdvancesinEngineeringSoftware188(2024)10357110",
    "P. Saves et al.",
    "Fig. 5. Example of usage of Hierarchical and Mixed Kriging surrogate.",
    "AdvancesinEngineeringSoftware188(2024)10357111",
    "P. Saves et al.",
    "Fig. 6. Optimization results for the Toy function [64].",
    "Fig. 7. Example of usage of mixed surrogates for Bayesian optimization.",
    "AdvancesinEngineeringSoftware188(2024)10357112",
    "P. Saves et al.",
    "Fig. 8. Optimization results for the hierarchical Goldstein function.",
    "6. Other relevant contributions in SMT 2.0",
    "5.2. A hierarchical optimization problem",
    "The new release SMT 2.0 introduces several improvements be- sides Kriging for hierarchical and mixed variables. This section details the most important new contributions. Recall from Section 2.2 that five sub-modules are present in the code: Sampling, Problems, Surrogate Models, Applications and Notebooks.",
    "The hierarchical test case considered in this paper to illustrate Bayesian optimization is a modified Goldstein function [60] detailed in Appendix B. The resulting optimization problem involves 11 vari- ables: 5 are continuous, 4 are integer (ordinal) and 2 are categorical. These variables consist in 6 neutral variables, 1 dimensional (or meta) variable and 4 decreed variables. Depending on the meta variable values, 4 different sub-problems can be identified. The optimization problem is given by: neu, \ud835\udc65qnt \ud835\udc5a , \ud835\udc65qnt neu, \ud835\udc65cat min \ud835\udc53 (\ud835\udc65cat dec) w.r.t. \ud835\udc65cat neu = \ud835\udc642 \u2208 {0, 1} \ud835\udc65qnt neu = (\ud835\udc651, \ud835\udc652, \ud835\udc655, \ud835\udc673, \ud835\udc674) \u2208 {0, 100}3 \u00d7 {0, 1, 2}2 \ud835\udc65cat \ud835\udc5a = \ud835\udc641 \u2208 {0, 1, 2, 3} \ud835\udc65qnt dec = (\ud835\udc653, \ud835\udc654, \ud835\udc671, \ud835\udc672) \u2208 {0, 100}2 \u00d7 {0, 1, 2}2",
    "6.1. Contributions to Sampling",
    "Pseudo-random sampling. The Latin Hypercube Sampling (LHS) is a stochastic sampling technique to generate quasi-random sampling dis- tributions. It is among the most popular sampling method in computer experiments thanks to its simplicity and projection properties with high-dimensional problems. The LHS method uses the pyDOE package (Design Of Experiments for Python). Five criteria for the construction of LHS are implemented in SMT. The first four criteria (center, maximin, centermaximin, correlation) are the same as in pyDOE.12 The last criterion ese, is implemented by the authors of SMT [48]. In SMT 2.0 a new LHS method was developed for the Nested design of experiments (NestedLHS) [73] to use with multi- fidelity surrogates. A new mathematical method (expand_lhs) [40] was developed in SMT 2.0 to increase the size of a design of exper- iments while maintaining the ese property. Moreover, we proposed a sampling method for mixed variables, and the aforementioned LHS method was applied to hierarchical variables in Fig. 8.",
    "(8)",
    "Compared to the model choice of Pelamatti et al. [60], we chose to model \ud835\udc655 and \ud835\udc642 as neutral variables even if \ud835\udc53 does not depend on \ud835\udc655 when \ud835\udc642 = 0. Other modeling choices are kept; for example, \ud835\udc642 is a so-called \u2018\u2018binary variable\u2019\u2019 and not a categorical one [71]. Similarly, we also keep the formulation of \ud835\udc641 as a categorical variable but a better model would be to model it as a \u2018\u2018cyclic variable\u2019\u2019 [72]. The resulting problem is described in Appendix B. To assess the performance of our algorithm, we performed 20 runs with different initial DoE sampled by LHS. Every DoE consists of \ud835\udc5b + 1 = 12 points and we chose a budget of 5\ud835\udc5b = 55 infill points. To compare our method with a baseline, we also tested the random search method thanks to the expand_lhs new method [40] described in Section 6.1 and we also optimized the Goldstein function using EGO with a classic Kriging model based on imputation method (Imp-Kernel). This method replaces the decreed- excluded variables by their mean values: 50 or 1 respectively for (\ud835\udc653, \ud835\udc654) and (\ud835\udc671, \ud835\udc672). Fig. 8(a) plots the convergence curves for the four methods. In particular, the median is the solid line and the first and third quantiles are plotted in dotted lines. To visualize better the correspond- ing data dispersion, the boxplots of the 20 best solutions are plotted in Fig. 8(b). The results in Fig. 8 show that the hierarchical Kriging models of SMT 2.0 lead to better results than the imputation method or the random search both in terms of final objective value and variance over the 20 runs and in term of convergence rate. More precisely, SMT Arc-Kernel and SMT Alg-Kernel Kriging model gave the best EGO results and managed to converge correctly as shown in Fig. 8(b). More precisely, the 20 sampled DoEs led to similar performance and from one DoE, the method SMT Alg-Kernel managed to find the true minimum. However, this result has not been reproduced in other runs and is therefore not statistically significant. The variance between the runs is of similar magnitude regardless of the considered methods.",
    "6.2. Contributions to Surrogate models",
    "New kernels and their derivatives for Kriging. Kriging surrogates are based on hyperparameters and on a correlation kernel. Four correla- tion kernels are now implemented in SMT 2.0 [74]. In SMT, these correlation functions are absolute exponential (abs_exp), Gaussian (squar_exp), Matern 5/2 (matern52) and Matern 3/2 (matern32). In addition, the implementation of gradient and Hessian for each kernel makes it possible to calculate both the first and second derivatives of the GP likelihood with respect to the hyperparameters [5].",
    "Variance derivatives for Kriging. To perform uncertainty quantification for system analysis purposes, it could be interesting to know more about the variance derivatives of a model [75\u201377]. For that purpose and also to pursue the original publication about derivatives [5], SMT 2.0 extends the derivative support to Kriging variances and kernels.",
    "12 https://pythonhosted.org/pyDOE/index.html",
    "AdvancesinEngineeringSoftware188(2024)10357113",
    "P. Saves et al.",
    "Noisy Kriging. In engineering and in big data contexts with real exper- iments, surrogate models for noisy data are of significant interest. In particular, there is a growing need for techniques like noisy Kriging and noisy Multi-Fidelity Kriging (MFK) for data fusion [78]. For that purpose, SMT 2.0 has been designed to accommodate Kriging and MFK to noisy data including the option to incorporate heteroscedastic noise (using the use_het_noise option) and to account for different noise levels for each data source [40].",
    "where the paths can be sampled by generating independent standard Normal distributed samples. The different methods are documented in the tutorial Gaussian Process Trajectory Sampling [84].",
    "Parallel Bayesian optimization. Due to the recent progress made in hardware configurations, it has been of high interest to perform parallel optimizations. A parallel criterion called qEI [85] was developed to perform Efficient Global Optimization (EGO): the goal is to be able to run batch optimization. At each iteration of the algorithm, multiple new sampling points are extracted from the known ones. These new sampling points are then evaluated using a parallel computing environ- ment. Five criteria are implemented in SMT 2.0: Kriging Believer (KB), Kriging Believer Upper Bound (KBUB), Kriging Believer Lower Bound (KBLB), Kriging Believer Random Bound (KBRand) and Constant Liar (CLmin) [86].",
    "Kriging with partial least squares. Beside MGP, for high-dimensional problems, the toolbox implements Kriging with partial least squares (KPLS) [57] and its extension KPLSK [44]. The PLS information is computed by projecting the data into a smaller space spanned by the principal components. By integrating this PLS information into the Kriging correlation matrix, the number of inputs can be scaled down, thereby reducing the number of hyperparameters required. The result- ing number of hyperparameters \ud835\udc51\ud835\udc52 is indeed much smaller than the original problem dimension \ud835\udc51. Recently, in SMT 2.0, we extended the KPLS method for multi-fidelity Kriging (MFKPLS and MFKPLSK) [73,79, 80]. We also proposed an automatic criterion to choose automatically the reduced dimension \ud835\udc51\ud835\udc52 based on Wold\u2019s R criterion [81]. This criterion has been applied to aircraft optimization with EGO where the number \ud835\udc51\ud835\udc52 (\ud835\ude97_\ud835\ude8c\ud835\ude98\ud835\ude96\ud835\ude99 in the code) for the model is automatically selected at every iteration [39]. Special efforts have been made to accommodate KPLS for multi-fidelity and mixed integer data [79,80].",
    "7. Conclusion",
    "SMT 2.0 introduces significant upgrades to the Surrogate Modeling Toolbox. This new release adds support for hierarchical and mixed variables and improves the surrogate models with a particular focus on Kriging (Gaussian process) models. SMT 2.0 is distributed through an open-source license and is freely available online.14 We provide documentation that caters to both users and potential developers.15 SMT 2.0 enables users and developers collaborating on the same project to have a common surrogate modeling tool that facilitates the exchange of methods and reproducibility of results.",
    "Marginal Gaussian process. SMT 2.0 implements Marginal Gaussian Process (MGP) surrogate models for high dimensional problems [82]. MGP are Gaussian processes taking into account hyperparameters un- certainty defined as a density probability function. Especially we sup- pose that the function to model \ud835\udc53 \u2236 \ud835\udefa \u21a6 R, where \ud835\udefa \u2282 R\ud835\udc51 and \ud835\udc51 is the number of design variables, lies in a linear embedding \ue22d such as \ue22d = {\ud835\udc62 = \ud835\udc34\ud835\udc65, \ud835\udc65 \u2208 \ud835\udefa}, \ud835\udc34 \u2208 R\ud835\udc51\u00d7\ud835\udc51\ud835\udc52 and \ud835\udc53 (\ud835\udc65) = \ud835\udc53\ue22d(\ud835\udc34\ud835\udc65) with \ud835\udc53 (\ud835\udc65) = \ud835\udc53\ue22d \u2236 \ue22d \u21a6 R and \ud835\udc51\ud835\udc52 \u226a \ud835\udc51. Then, we must use a kernel \ud835\udc58(\ud835\udc65, \ud835\udc65\u2032) = \ud835\udc58\ue22d(\ud835\udc34\ud835\udc65, \ud835\udc34\ud835\udc65\u2032) whose each component of the transfer matrix \ud835\udc34 is an hyperparameter. Thus we have \ud835\udc51\ud835\udc52 \u00d7 \ud835\udc51 hyperparameters to find. Note that \ud835\udc51\ud835\udc52 is defined as \ud835\ude97_\ud835\ude8c\ud835\ude98\ud835\ude96\ud835\ude99 in the code [49].",
    "SMT has been widely used in aerospace and mechanical modeling applications. Moreover, the toolbox is general and can be useful for anyone who needs to use or develop surrogate modeling techniques, regardless of the targeted applications. SMT is currently the only open- source toolbox that can build hierarchical and mixed surrogate models.",
    "Declaration of competing interest",
    "The authors declare that they have no known competing finan- cial interests or personal relationships that could have appeared to influence the work reported in this paper.",
    "Gradient-enhanced neural network. The new release SMT 2.0 imple- ments Gradient-Enhanced Neural Network (GENN) models [45]. Gradient-Enhanced Neural Networks (GENN) are fully connected multi- layer perceptrons whose training process was modified to account for gradient information. Specifically, the model is trained to minimize not only the prediction error of the response but also the prediction error of the partial derivatives: the chief benefit of gradient enhancement is better accuracy with fewer training points. Note that GENN applies to regression (single-output or multi-output), but not classification since there is no gradient in that case. The implementation is fully vectorized and uses ADAM optimization, mini-batch, and L2-norm regularization. For example, GENN can be used to learn airfoil geometries from a database. This usage is documented in SMT 2.0.13",
    "Data availability",
    "Data will be made available on request. Results can be reproduced freely online at https://colab.research.google.com/github/SMTorg/smt/ blob/master/tutorial/NotebookRunTestCases_Paper_SMT_v2.ipynb.",
    "Acknowledgments",
    "We want to thank all those who contribute to this release. Namely, M. A. Bouhlel, I. Cardoso, R. Carreira Rufato, R. Charayron, R. Conde Arenzana, S. Dubreuil, A. F. L\u00f3pez-Lopera, M. Meliani, M. Menz, N. Mo\u00ebllo, A. Thouvenot, R. Priem, E. Roux and F. Vergnes. This work is part of the activities of ONERA - ISAE - ENAC joint research group. We also acknowledge the partners institutions: ONERA, NASA Glenn, ISAE- SUPAERO, Institut Cl\u00e9ment Ader (ICA), the University of Michigan, Polytechnique Montr\u00e9al and the University of California San Diego.",
    "6.3. Contributions to Applications",
    "Kriging trajectory and sampling. Sampling a GP with high resolution is usually expensive due to the large dimension of the associated covariance matrix. Several methods are proposed to draw samples of a GP on a given set of points. To sample a conditioned GP, the classic method consists in using a Cholesky decomposition (or eigende- composition) of the conditioned covariance matrix of the process but some numerical computational errors can lead to non SPD matrix. A more recent approach based on Karhunen\u2013Lo\u00e8ve decomposition of the covariance kernel with the Nystr\u00f6m method has been proposed in [83]",
    "The research presented in this paper has been performed in the framework of the AGILE 4.0 project (Towards cyber-physical collabo- rative aircraft development), funded by the European Union Horizon 2020 research and innovation framework programme under grant agreement n\u25e6 815122 and in the COLOSSUS project (Collaborative System of Systems Exploration of Aviation Products, Services and",
    "13 https://smt.readthedocs.io/en/latest/_src_docs/examples/airfoil_",
    "14 https://github.com/SMTorg/SMT 15 https://smt.readthedocs.io/en/latest/",
    "parameters/learning_airfoil_parameters.html",
    "AdvancesinEngineeringSoftware188(2024)10357114",
    "P. Saves et al.",
    "Then, the functions \ud835\udc530, \ud835\udc531 and \ud835\udc532 are defined as mixed variants of \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont as such",
    "Business Models) funded by the European Union Horizon Europe re- search and innovation framework programme under grant agreement n\u25e6 101097120.",
    "\ud835\udc530(\ud835\udc651, \ud835\udc652, \ud835\udc671, \ud835\udc672, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) =",
    "We also are grateful to E. Hall\u00e9-Hannan from Polytechnique Mon-",
    "( 1",
    "1",
    "\ud835\udc671=0\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 20, 20, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)",
    "tr\u00e9al for the hierarchical variables framework.",
    "\ud835\udc672=0 + 1",
    "\ud835\udc671=1\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 50, 20, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)",
    "Appendix A. Toy test function",
    ")",
    "+ 1",
    "\ud835\udc671=2\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 80, 20, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) ( 1",
    "1",
    "\ud835\udc671=0\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 20, 50, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)",
    "This Appendix gives the detail of the toy function of Section 5.1.16",
    "\ud835\udc672=1 + 1",
    "(B.3)",
    "First, we recall the optimization problem:",
    "\ud835\udc671=1\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 50, 50, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)",
    "min \ud835\udc53 (\ud835\udc65cat , \ud835\udc65qnt ) w.r.t. \ud835\udc65cat = \ud835\udc501 \u2208 {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}",
    ")",
    "+ 1",
    "\ud835\udc671=2\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 80, 50, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) ( 1",
    "(A.1)",
    "1",
    "\ud835\udc671=0\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 20, 80, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)",
    "\ud835\udc672=2 + 1",
    "\ud835\udc65qnt = \ud835\udc651 \u2208 [0, 1]",
    "\ud835\udc671=1\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 50, 80, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)",
    "The toy function \ud835\udc53 is defined as",
    ")",
    "+ 1",
    "\ud835\udc671=2\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 80, 80, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)",
    "\ud835\udc53 (\ud835\udc65, \ud835\udc501) =1",
    "\ud835\udc501=0 cos(3.6\ud835\udf0b(\ud835\udc65 \u2212 2)) + \ud835\udc65 \u2212 1",
    "\ud835\udc65 2",
    "+1",
    "\ud835\udc531(\ud835\udc651, \ud835\udc652, \ud835\udc653, \ud835\udc672, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) =",
    "\ud835\udc501=1 2 cos(1.1\ud835\udf0b exp(\ud835\udc65)) \u2212",
    "+ 2",
    "1",
    "\ud835\udc672=0\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, \ud835\udc653, 20, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) + 1",
    "1 2",
    "+1",
    "\ud835\udc501=2 cos(2\ud835\udf0b\ud835\udc65) +",
    "\ud835\udc65",
    "\ud835\udc672=1\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, \ud835\udc653, 50, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)",
    "\ud835\udc65 \u2212 1 2",
    "+1",
    "\ud835\udc501=3 \ud835\udc65(cos(3.4\ud835\udf0b(\ud835\udc65 \u2212 1)) \u2212",
    ")",
    "+ 1",
    "\ud835\udc672=2\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, \ud835\udc653, 80, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)",
    "\ud835\udc652 2",
    "+1",
    "\ud835\udc532(\ud835\udc651, \ud835\udc652, \ud835\udc654, \ud835\udc671, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) =",
    "\ud835\udc501=4 \u2212",
    "(A.2)",
    "1",
    "\ud835\udc65 2",
    "\ud835\udc671=0\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 20, \ud835\udc654, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) + 1",
    "+1",
    "\ud835\udc501=5 2 cos(0.25\ud835\udf0b exp(\u2212\ud835\udc654))2 \u2212",
    "+ 1",
    "\ud835\udc671=1\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, 50, \ud835\udc652, \ud835\udc654, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)",
    "\ud835\udc65 2",
    "+1",
    "\ud835\udc501=6 \ud835\udc65 cos(3.4\ud835\udf0b\ud835\udc65) \u2212",
    "+ 1",
    "+ 1",
    "\ud835\udc671=2\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 80, \ud835\udc654, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) To finish with, the function \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont is given by",
    "\ud835\udc65 2",
    "+1",
    "\ud835\udc501=7 \u2212 \ud835\udc65(cos(3.5\ud835\udf0b\ud835\udc65) +",
    ") + 2",
    "\ud835\udc655 \ud835\udc501=8 \u2212 + 1 2 \ud835\udc501=9 \u2212 cos(2.5\ud835\udf0b\ud835\udc65)2\u221a",
    "+1",
    "\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, \ud835\udc653, \ud835\udc654, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) = 53.3108 + 0.184901\ud835\udc651 \u2212 5.02914\ud835\udc651 3.10\u22126 \u2212 0.00146393\ud835\udc651\ud835\udc652 + 7.98772\ud835\udc653 \u2212 0.00301588\ud835\udc651\ud835\udc653 \u2212 0.00272291\ud835\udc651\ud835\udc654 + 0.0017004\ud835\udc652\ud835\udc653 + 0.0038428\ud835\udc652\ud835\udc654 \u2212 0.000198969\ud835\udc653\ud835\udc654 + 1.86025\ud835\udc651\ud835\udc652\ud835\udc653.10\u22125 \u2212 1.88719\ud835\udc651\ud835\udc652\ud835\udc654.10\u22126 + 2.50923\ud835\udc651\ud835\udc653\ud835\udc654.10\u22125 \u2212 5.62199\ud835\udc652\ud835\udc653\ud835\udc654.10\u22125 + \ud835\udc642",
    "\ud835\udc673 .10\u22128 \u2212 0.0870775\ud835\udc652 \u2212 0.106959\ud835\udc653",
    "3.10\u22126 + 7.72522\ud835\udc651 \ud835\udc674 .10\u22126 + 0.00242482\ud835\udc654 + 1.32851\ud835\udc654",
    "+1",
    "\ud835\udc65 \u2212 0.5 ln(\ud835\udc65 + 0.5) \u2212 1.3",
    "Appendix B. Hierarchical Goldstein test function",
    ")",
    ")",
    "(",
    "( 2\ud835\udf0b 100",
    "\ud835\udc655",
    ".",
    "5 cos",
    "\u2212 2",
    "This Appendix gives the detail of the hierarchical Goldstein problem",
    "of Section 5.2.17 First, we recall the optimization problem:",
    "(B.4)",
    "\ud835\udc5a , \ud835\udc65qnt neu, \ud835\udc65qnt dec) neu = \ud835\udc642 \u2208 {0, 1} \ud835\udc65qnt neu = (\ud835\udc651, \ud835\udc652, \ud835\udc655, \ud835\udc673, \ud835\udc674) \u2208 [0, 100]3 \u00d7 {0, 1, 2}2 \ud835\udc65cat \ud835\udc5a = \ud835\udc641 \u2208 {0, 1, 2, 3} \ud835\udc65qnt dec = (\ud835\udc653, \ud835\udc654, \ud835\udc671, \ud835\udc672) \u2208 [0, 100]2 \u00d7 {0, 1, 2}2",
    "min \ud835\udc53 (\ud835\udc65cat",
    "neu, \ud835\udc65cat",
    "Appendix C. Supplementary data",
    "w.r.t. \ud835\udc65cat",
    "(B.1)",
    "More at https://colab.research.google.com/github/SMTorg/smt/blob/",
    "master/tutorial/NotebookRunTestCases_Paper_SMT_v2.ipynb.",
    "Supplementary material related to this article can be found online",
    "at https://doi.org/10.1016/j.advengsoft.2023.103571.",
    "The hierarchical and mixed function \ud835\udc53 is defined as a hierarchical function that depends on \ud835\udc530, \ud835\udc531, \ud835\udc532 and \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont as describes in the following.",
    "References",
    "[1] Mader CA, Martins JRRA, Alonso JJ, van der Weide E. ADjoint: An approach for the rapid development of discrete adjoint solvers. AIAA J 2008;46:863\u201373. [2] Kennedy M, O\u2019Hagan A. Bayesian calibration of computer models. J R Stat Soc",
    "\ud835\udc53 (\ud835\udc651, \ud835\udc652, \ud835\udc653, \ud835\udc654, \ud835\udc671, \ud835\udc672, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc641, \ud835\udc642) =",
    "1",
    "\ud835\udc641=0\ud835\udc530(\ud835\udc651, \ud835\udc652, \ud835\udc671, \ud835\udc672, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)",
    "Ser B Stat Methodol 2001;63:425\u201364.",
    "[3] Hwang JT, Martins JRRA. A fast-prediction surrogate model for large datasets.",
    "+ 1",
    "\ud835\udc641=1\ud835\udc531(\ud835\udc651, \ud835\udc652, \ud835\udc653, \ud835\udc672, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)",
    "(B.2)",
    "Aerosp Sci Technol 2018;75:74\u201387.",
    "[4] Martins JRRA, Ning A. Engineering design optimization. Cambridge University",
    "+ 1",
    "\ud835\udc641=2\ud835\udc532(\ud835\udc651, \ud835\udc652, \ud835\udc654, \ud835\udc671, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642)",
    "Press; 2021.",
    "[5] Bouhlel MA, Hwang JT, Bartoli N, Lafage R, Morlier J, Martins JRA. A Python surrogate modeling framework with derivatives. Adv Eng Softw 2019;135:102662.",
    "+ 1",
    "\ud835\udc641=3\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, \ud835\udc653, \ud835\udc654, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642).",
    "[6] Bouhlel MA, Martins J. Gradient-enhanced kriging for high-dimensional",
    "problems. Eng Comput 2019;35:157\u201373.",
    "16 https://github.com/jbussemaker/SBArchOpt 17 https://github.com/jbussemaker/SBArchOpt",
    "[7] Pedregosa F, Varoquaux G, Gramfort A, Thirion VMB, Grisel O, et al. Scikit-learn:",
    "Machine learning in Python. J Mach Learn Res 2011;12:2825\u201330.",
    "AdvancesinEngineeringSoftware188(2024)10357115",
    "P. Saves et al.",
    "[38] Audet C, Hall\u00e9-Hannan E, Le Digabel S. A general mathematical framework for constrained mixed-variable blackbox optimization problems with meta and categorical variables. Oper Res Forum 2023;4:1\u201337.",
    "[8] Lataniotis C, Marelli S, Sudret B. Uqlab 2.0 and uqcloud: open-source vs. In: SIAM conference on uncertainty",
    "cloud-based uncertainty quantification. quantification. 2022.",
    "[39] Saves P, Nguyen Van E, Bartoli N, Diouane Y, Lefebvre T, David C, Defoort S, Morlier J. Bayesian optimization for mixed variables using an adaptive dimension reduction process: applications to aircraft design. In: AIAA scitech 2022. 2022. [40] Conde Arenzana R, L\u00f3pez-Lopera A, Mouton S, Bartoli N, Lefebvre T. Multi- In:",
    "[9] Faraci A, Beaurepaire P, Gayton N. Review on Python toolboxes for Kriging",
    "surrogate modelling. In: ESREL. 2022.",
    "[10] Kr\u00fcgener M, Zapata Usandivaras J, Bauerheim M, Urbano A. Coaxial-injector surrogate modeling based on Reynolds-averaged Navier\u2013Stokes simulations using deep learning. J Propuls Power 2022;38:783\u201398.",
    "fidelity Gaussian process model ECCOMAS aerobest. 2021.",
    "for CFD and wind tunnel data fusion.",
    "[11] Ming D, Williamson D, Guillas S. Deep Gaussian process emulation using",
    "[41] Rufato RC, Diouane Y, Henry J, Ahlfeld R, Morlier J. A mixed-categorical data-driven approach for prediction and optimization of hybrid discontinuous composites performance. In: AIAA aviation 2022 forum. 2022.",
    "stochastic imputation. Technometrics 2022;1\u201312.",
    "[12] Eli\u00e1\u0161 J, Vo\u0159echovsk`y M, Sad\u00edlekv V. Periodic version of the minimax distance criterion for Monte Carlo integration. Adv Eng Softw 2020;149:102900. [13] Drouet V, Balesdent M, Brevault L, Dubreuil S, Morio J. Multi-fidelity algo- rithm for the sensitivity analysis of multidisciplinary problems. J Mech Des 2023;145:1\u201322.",
    "[42] Gorissen D, Crombecq K, Couckuyt I, Dhaene T, Demeester P. A surrogate modeling and adaptive sampling toolbox for computer based design. J Mach Learn Res 2010;11:2051\u20135.",
    "[43] Williams CK, Rasmussen CE. Gaussian processes for machine learning. MA: MIT",
    "[14] Karban P, P\u00e1nek D, Orosz T, Petr\u00e1\u0161ov\u00e1 I, Dole\u017eel I. FEM based robust design optimization with Agros and \u00afArtap. Comput Math Appl 2021;81:618\u201333. [15] Kudela J, Matousek R. Recent advances and applications of surrogate models for finite element method computations: a review. Soft Comput 2022;26:13709\u201333. [16] Chen Y, Dababneh F, Zhang B, Kassaee S, Smith BT, Liu K, et al. Surrogate mod- eling for capacity planning of charging station equipped with photovoltaic panel and hydropneumatic energy storage. J Energy Res Technol 2020;142:050907.",
    "press Cambridge; 2006.",
    "[44] Bouhlel MA, Bartoli N, Regis R, Otsmane A, Morlier J. Efficient Global Opti- mization for high-dimensional constrained problems by using the Kriging models combined with the Partial Least Squares method. Eng Optim 2018;50:2038\u201353. [45] Bouhlel MA, He S, Martins J. Scalable gradient-enhanced artificial neural networks for airfoil shape design in the subsonic and transonic regimes. Struct Multidiscip Optim 2020;61:1363\u201376.",
    "[17] Jasa J, Bortolotti P, Zalkind D, Barter G. Effectively using multifidelity optimization for wind turbine design. Wind Energy Sci 2022;7:991\u20131006. [18] Wang W, Tao G, Ke D, Luo J, Cui J. Transpiration cooling of high pres- sure turbine vane with optimized porosity distribution. Appl Therm Eng 2023;223:119831.",
    "[46] Kwan LS, Pitrou A, Seibert S. Numba: A LLVM-based python JIT compiler. In: Proceedings of the second workshop on the LLVM compiler infrastructure in HPC. 2015.",
    "[47] Zaefferer M, Horn D. A first analysis of kernels for Kriging-based optimization",
    "in hierarchical search spaces. 2018, arXiv.",
    "[19] Savage T, Almeida-Trasvina HF, del R\u00edo-Chanona EA, Smith R, Zhang D. An adaptive data-driven modelling and optimization framework for complex chemical process design. Comput Aided Chem Eng 2020;48:73\u20138.",
    "[48] Jin R, Chen W, Sudjianto A. An efficient algorithm for constructing optimal design of computer experiments. J Statist Plann Inference 2005;2:545\u201354. [49] Garnett R, Osborne M, Hennig P. Active learning of linear embeddings for Gaussian processes. In: Uncertainty in artificial intelligence - Proceedings of the 30th conference. 2013.",
    "[20] Chan A, Pires AF, Polacsek T. Trying to elicit and assign goals to the right actors.",
    "In: Conceptual modeling: 41st international conference. 2022.",
    "[21] Hutter F, Osborne MA. A kernel for hierarchical parameter spaces. 2013, arXiv. [22] Bussemaker JH, Ciampa PD, Nagel B. System architecture design space explo- ration: An approach to modeling and optimization. In: AIAA aviation 2020 forum. 2020.",
    "[50] Jones D. A taxonomy of global optimization methods based on response surfaces.",
    "J Global Optim 2001;21:345\u201383.",
    "[51] Lafage R. egobox, a Rust toolbox for efficient global optimization. J Open Source",
    "Softw 2022;7:4737.",
    "[23] Fouda MEA, Adler EJ, Bussemaker J, Martins JRRA, Kurtulus DF, Boggero L, et al. Automated hybrid propulsion model construction for conceptual aircraft design and optimization. In: 33rd congress of the international council of the aeronautical sciences. 2022.",
    "[52] Jones DR, Schonlau M, Welch WJ. Efficient global optimization of expensive",
    "black-box functions. J Global Optim 1998;13:455\u201392.",
    "[53] Deng X, Lin CD, Liu K, Rowe RK. Additive Gaussian process for computer models with qualitative and quantitative factors. Technometrics 2017;59:283\u201392. [54] Cuesta-Ramirez J, Le Riche R, Roustant O, Perrin G, Durantin C, Gliere A. A comparison of mixed-variables Bayesian optimization approaches. Adv Model Simul Eng Sci 2021;9:1\u201329.",
    "[24] Bussemaker JH, Bartoli N, Lefebvre T, Ciampa PD, Nagel B. Effectiveness of surrogate-based optimization algorithms for system architecture optimization. In: AIAA aviation 2021 forum. 2021.",
    "[55] Rebonato R, Jaeckel P. The most general methodology to create a valid correlation matrix for risk management and option pricing purposes. J Risk 2001;2:17\u201327.",
    "[25] Balandat M, Karrer B, Jiang D, Daulton S, Letham B, Wilson A, et al. BoTorch: A framework for efficient Monte-Carlo Bayesian optimization. Adv Neural Inf Process Syst 2020;33:21524\u201338.",
    "[56] Rapisarda F, Brigo D, Mercurio F. Parameterizing correlations: a geometric",
    "[26] Adams B, Bohnhoff W, Dalbey K, Ebeida M, Eddy J, Eldred M, et al. Dakota, a multilevel parallel object-oriented framework for design optimization, pa- rameter estimation, uncertainty quantification, and sensitivity analysis: Version 6.13 user\u2019s manual. Technical report, Albuquerque, NM (United States: Sandia National Lab.(SNL-NM); 2020.",
    "interpretation. IMA J Manag Math 2007;18:55\u201373.",
    "[57] Bouhlel MA, Bartoli N, Regis R, Otsmane A, Morlier J. An improved approach for estimating the hyperparameters of the Kriging model for high- dimensional problems through the Partial Least Squares method. Math Probl Eng 2016;2016:6723410.",
    "[27] Roustant O, Ginsbourger D, Deville Y. DiceKriging, DiceOptim: Two R packages for the analysis of computer experiments by Kriging-based metamodeling and optimization. J Stat Softw 2012;51:1\u201355.",
    "[58] Cheng GH, Younis A, Hajikolaei KH, Wang GG. Trust region based mode pursuing sampling method for global optimization of high dimensional design problems. J Mech Des 2015;137:021407.",
    "[28] Zhang Y, Tao S, Chen W, Apley D. A latent variable approach to Gaus- sian process modeling with qualitative and quantitative factors. Technometrics 2020;62:291\u2013302.",
    "[59] Karlsson R, Bliek L, Verwer S, de Weerdt M. Continuous surrogate-based optimization algorithms are well-suited for expensive discrete problems. In: Artificial intelligence and machine learning. 2021.",
    "[29] Chang TH, Wild SM. ParMOO: A Python library for parallel multiobjective",
    "[60] Pelamatti J, Brevault L, Balesdent M, Talbi E-G, Guerin Y. Bayesian optimization",
    "simulation optimization. J Open Source Softw 2023;8:4468.",
    "of variable-size design space problems. Opt Eng 2021;22:387\u2013447.",
    "[30] Garrido-Merch\u00e1n EC, Hern\u00e1ndez-Lobato D. Dealing with categorical and integer-valued variables in Bayesian optimization with Gaussian processes. Neurocomputing 2020;380:20\u201335.",
    "[61] Hebbal A, Brevault L, Balesdent M, Talbi E-G, Melab N. Bayesian optimization using deep Gaussian processes with applications to aerospace system design. Opt Eng 2021;22:321\u201361.",
    "[31] Halstrup M. Black-box optimization of mixed discrete-continuous optimization",
    "[62] Wildberger N. A rational approach to trigonometry. Math Horiz 2007;15:16\u201320. [63] Cho H, Kim Y, Lee E, Choi D, Lee Y, Rhee W. Basic enhancement strategies when using bayesian optimization for hyperparameter tuning of deep neural networks. IEEE Access 2020;8:52588\u2013608.",
    "problems (Ph.D. thesis), TU Dortmund; 2016.",
    "[32] Roustant O, Padonou E, Deville Y, Cl\u00e9ment A, Perrin G, Giorla J, et al. Group kernels for gaussian process metamodels with categorical inputs. SIAM J Uncertain Quant 2020;8:775\u2013806.",
    "[64] Zuniga MM, Sinoquet D. Global optimization for mixed categorical-continuous variables based on Gaussian process models with a randomized categorical space exploration step. INFOR Inf Syst Oper Res 2020;58:310\u201341.",
    "[33] Zhou Q, Qian PZG, Zhou S. A simple approach to emulation for computer models with qualitative and quantitative factors. Technometrics 2011;53:266\u201373. [34] Saves P, Diouane Y, Bartoli N, Lefebvre T, Morlier J. A mixed-categorical correlation kernel for Gaussian process. Neurocomputing 2023;550:126472.",
    "[65] Lindauer M, Eggensperger K, Feurer M, AB, Deng D, Benjamins C, et al. SMAC3: A versatile Bayesian optimization package for hyperparameter optimization. J Mach Learn Res 2022;23:1\u20139.",
    "[35] Pelamatti",
    "J, Brevault L, Balesdent M, Talbi E-G, Guerin Y. Efficient global optimization of constrained mixed variable problems. J Global Optim 2019;73:583\u2013613.",
    "[66] Picheny V, Berkeley J, Moss H, Stojic H, Granta U, Ober S, et al. Trieste: Efficiently exploring the depths of black-box functions with TensorFlow. 2023, arXiv.",
    "[36] Horn D, Stork J, ler N-JS, Zaefferer M. Surrogates for hierarchical search spaces: The Wedge-Kernel and an automated analysis. In: Proceedings of the genetic and evolutionary computation conference. 2019.",
    "[67] Cowen-Rivers AI, Ly W, Wang Z, Tutunov R, Jianye H, Wang J, et al. HEBO:",
    "Heteroscedastic evolutionary Bayesian optimisation. 2020, arXiv.",
    "[68] Jiang H, Shen Y, Li Y, Zhang W, Zhang C, Cui B. OpenBox: A Python toolkit for",
    "[37] Hung Y, Joseph VR, Melkote SN. Design and analysis of computer experiments",
    "generalized black-box optimization. 2023, arXiv.",
    "with branching and nested factors. Technometrics 2009;51:354\u201365.",
    "AdvancesinEngineeringSoftware188(2024)10357116",
    "P. Saves et al.",
    "[78] Platt J, Penny S, Smith T, Chen T, Abarbanel H. A systematic exploration of reservoir computing for forecasting complex spatiotemporal dynamics. Neural Netw 2022;153:530\u201352.",
    "[69] Kandasamy K, Vysyaraju KR, Neiswanger W, Paria B, Collins C, Schneider J, et al. Tuning hyperparameters without grad students: Scalable and robust bayesian optimisation with dragonfly. J Mach Learn Res 2020;21:3098\u2013124.",
    "[79] Charayron R, Lefebvre T, Bartoli N, Morlier J. Multi-fidelity Bayesian optimiza- tion strategy applied to overall drone design. In: AIAA scitech 2023 forum. 2023.",
    "[70] Roy S, Crossley WA, Stanford BK, Moore KT, Gray JS. A mixed integer efficient global optimization algorithm with multiple infill strategy - Applied to a wing topology optimization problem. In: AIAA scitech 2019 forum. 2019.",
    "[80] Charayron R, Lefebvre T, Bartoli N, Morlier J. Towards a multi-fidelity and multi-objective Bayesian optimization efficient algorithm. Aerosp Sci Technol 2023;142:108673.",
    "[71] M\u00fcller J, Shoemaker CA, Pich\u00e9 R. SO-MI: A surrogate model algorithm for computationally expensive nonlinear mixed-integer black-box global optimization problems. Comput Oper Res 2013;40:1383\u2013400.",
    "[81] Wold H. Soft modelling by latent variables: The non-linear iterative partial least",
    "[72] Tran T, Sinoquet D, Da Veiga S, Mongeau M. Derivative-free mixed binary necklace optimization for cyclic-symmetry optimal design problems. Opt Eng 2021.",
    "squares (NIPALS) approach. J Appl Probab 1975;12:117\u201342.",
    "[82] Priem R, Diouane Y, Bartoli N, Dubreuil S, Saves P. High-dimensional efficient global optimization using both random and supervised embeddings. In: AIAA aviation 2023 forum. 2023.",
    "[73] Meliani M, Bartoli N, Lefebvre T, Bouhlel MA, Martins JRRA, Morlier J. Multi- fidelity efficient global optimization: Methodology and application to airfoil shape design. In: AIAA aviation 2019 forum. 2019.",
    "[83] Betz W, Papaioannou I, Straub D. Numerical methods for the discretization of random fields by means of the Karhunen\u2013Lo\u00e8ve expansion. Comput Methods Appl Mech Engrg 2014;271:109\u201329.",
    "[74] Lee H. Gaussian processes. Springer Berlin Heidelberg; 2011, p. 575\u20137. [75] L\u00f3pez-Lopera AF, Idier D, Rohmer J, Bachoc F. Multioutput Gaussian processes with functional data: A study on coastal flood hazard assessment. Reliab Eng Syst Saf 2022;218:108139.",
    "[84] Menz M, Dubreuil S, Morio J, Gogu C, Bartoli N, Chiron M. Variance based sen- sitivity analysis for Monte Carlo and importance sampling reliability assessment with Gaussian processes. Struct Saf 2021;93:102116.",
    "[76] Berthelin G, Dubreuil S, Sala\u00fcn M, Bartoli N, Gogu C. Disciplinary proper orthogonal decomposition and interpolation for the resolution of parameterized multidisciplinary analysis. Internat J Numer Methods Engrg 2022;123:3594\u2013626. [77] Cardoso I, Dubreuil S, Bartoli N, Gogu C, Sala\u00fcn M, Lafage R. Disciplinary surrogates for gradient-based optimization of multidisciplinary systems. In: ECCOMAS Aerobest. 2023.",
    "[85] Ginsbourger D, Le Riche R, Carraro L. Kriging is well-suited to parallelize",
    "optimization. Springer Berlin Heidelberg; 2010, p. 131\u201362.",
    "[86] Roux E, Tillier Y, Kraria S, Bouchard P-O. An efficient parallel global opti- mization strategy based on Kriging properties suitable for material parameters identification. Arch Mech Eng 2020;67."
]