[
    "Large Language Model Augmented Narrative Driven\nRecommendations\nSheshera Mysore\nsmysore@cs.umass.edu\nUniversity of Massachusetts Amherst\nUSA\nAndrew McCallum\nmccallum@cs.umass.edu\nUniversity of Massachusetts Amherst\nUSA\nHamed Zamani\nhzamani@cs.umass.edu\nUniversity of Massachusetts Amherst\nUSA\nABSTRACT\nNarrative-driven recommendation (NDR) presents an information\naccess problem where users solicit recommendations with verbose\ndescriptions of their preferences and context, for example, travelers\nsoliciting recommendations for points of interest while describ-\ning their likes/dislikes and travel circumstances. These requests\nare increasingly important with the rise of natural language-based\nconversational interfaces for search and recommendation systems.\nHowever, NDR lacks abundant training data for models, and current\nplatforms commonly do not support these requests. Fortunately,\nclassical user-item interaction datasets contain rich textual data,\ne.g., reviews, which often describe user preferences and context\n\u2013 this may be used to bootstrap training for NDR models. In this\nwork, we explore using large language models (LLMs) for data\naugmentation to train NDR models. We use LLMs for authoring\nsynthetic narrative queries from user-item interactions with few-\nshot prompting and train retrieval models for NDR on synthetic\nqueries and user-item interaction data. Our experiments demon-\nstrate that this is an effective strategy for training small-parameter\nretrieval models that outperform other retrieval and LLM baselines\nfor narrative-driven recommendation.\nCCS CONCEPTS\n\u2022 Information systems \u2192 Recommender systems; Users and inter-\nactive retrieval; \u2022 Computing methodologies \u2192 Natural language\ngeneration.\nACM Reference Format:\nSheshera Mysore, Andrew McCallum, and Hamed Zamani. 2023. Large\nLanguage Model Augmented Narrative Driven Recommendations. In Sev-\nenteenth ACM Conference on Recommender Systems (RecSys \u201923), Septem-\nber 18\u201322, 2023, Singapore, Singapore. ACM, New York, NY, USA, 7 pages.\nhttps://doi.org/10.1145/3604915.3608829\n1\nINTRODUCTION\nRecommender systems personalized to users are an important com-\nponent of several industry-scale platforms [16, 17, 46]. These sys-\ntems function by inferring users\u2019 interests from their prior inter-\nactions on the platform and making recommendations based on\nthese inferred interests. While recommendations based on historical\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nRecSys \u201923, September 18\u201322, 2023, Singapore, Singapore\n\u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0241-9/23/09...$15.00\nhttps://doi.org/10.1145/3604915.3608829\ninteractions are effective, users soliciting recommendations often\nstart with a vague idea about their desired target items or may\ndesire recommendations depending on the context of use, often\nmissing in historical interaction data (Figure 1). In these scenarios,\nit is common for users to solicit recommendations through long-\nform narrative queries describing their broad interests and context.\nInformation access tasks like these have been studied as narrative-\ndriven recommendations (NDR) for items ranging from books [5]\nand movies [18], to points of interest [1]. Bogers and Koolen [5]\nnote these narrative requests to be common on discussion forums\nand several subreddits1, but, there is a lack of support for these\ncomplex natural language queries in current recommenders.\nHowever, with the emergence of conversational interfaces for\ninformation access tasks, support for complex NDR tasks is likely\nto become necessary. In this context, recent work has noted an\nincrease in complex and subjective natural language requests com-\npared to more conventional search interfaces [13, 34]. Furthermore,\nthe emergence of large language models (LLM) with strong lan-\nguage understanding capabilities presents the potential for fulfilling\nsuch complex requests [9, 33]. This work explores the potential for\nre-purposing historical user-item recommendation datasets, tra-\nditionally used for training collaborative filtering recommenders,\nwith LLMs to support NDR.\nSpecifically, given a user\u2019s interactions, \ud835\udc37\ud835\udc62, with items and\ntheir accompanying text documents (e.g., reviews, descriptions)\n\ud835\udc37\ud835\udc62 = {\ud835\udc51\ud835\udc56}\ud835\udc41\ud835\udc62\n\ud835\udc56=1, selected from a user-item interaction dataset I, we\nprompt InstructGPT, a 175B parameter LLM, to author a synthetic\nnarrative query \ud835\udc5e\ud835\udc62 based on \ud835\udc37\ud835\udc62 (Figure 2). Since we expect the\nquery \ud835\udc5e\ud835\udc62 to be noisy and not fully representative of all the user\nreviews, \ud835\udc37\ud835\udc62 is filtered to retain only a fraction of the reviews based\non a language-model assigned likelihood of \ud835\udc5e\ud835\udc62 given a user doc-\nument, \ud835\udc51\ud835\udc56. Then, a pre-trained LM based retrieval model (110M\nparameters) is fine-tuned for retrieval on the synthetic queries and\nfiltered reviews.\nOur approach, which we refer to as Mint2, follows from the\nobservation that while narrative queries and suggestions are often\nmade in online discussion forums, and could serve as training data,\nthe number of these posts and the diversity of domains for which\nthey are available is significantly smaller than the size and diversity\nof passively gathered user-item interaction datasets. E.g. while\nBogers and Koolen [5] note nearly 25,000 narrative requests for\nbooks on the LibraryThing discussion forum, a publicly available\nuser-item interaction dataset for Goodreads contains interactions\nwith nearly 2.2M books by 460k users [43] .\nWe empirically evaluate Mint in a publicly available test collec-\ntion for point of interest recommendation: pointrec [1]. To train\n1r/MovieSuggestions, r/booksuggestions, r/Animesuggest\n2Mint: Data augMentation with INteraction narraTives.\n777\n",
    "RecSys \u201923, September 18\u201322, 2023, Singapore, Singapore\nMysore, McCallum, Zamani\nFigure 1: An example narrative query soliciting point of\ninterest recommendations. The query describes the users\npreferences and the context of their request.\nFigure 2: The format of the prompt used in Mint for\ngenerating synthetic narrative queries from user-item\ninteraction with a large language model.\nour NDR models, we generate synthetic training data based on\nuser-item interaction datasets from Yelp. Models (110M parameters)\ntrained with Mint significantly outperform several baseline models\nand match the performance of significantly larger LLM baselines\nautoregressively generating recommendations. Code and synthetic\ndatasets are available:3\n2\nRELATED WORK\nData Augmentation for Information Access. A line of recent\nwork has explored using language models to generate synthetic\nqueries for data augmentation to train models for information re-\ntrieval tasks [7, 8, 15, 23, 31]. Here, given a document collection of\ninterest, a pre-trained language model is used to create synthetic\nqueries for the document collection. An optional filtering step ex-\ncludes noisy queries, and finally, a bi-encoder or a cross-encoder is\ntrained for the retrieval task. While earlier work of Ma et al. [31]\ntrain a custom query generation model on web-text datasets, more\nrecent work has leveraged large language models for zero/few-shot\nquestion generation [7, 8, 15, 23]. In generating synthetic queries,\nthis work indicates the effectiveness of smaller parameter LLMs\n(up to 6B parameters) for generating synthetic queries in simpler\ninformation-retrieval tasks [7, 8, 23], and finds larger models (100B\nparameters and above) to be necessary for harder tasks such as\nargument retrieval [15, 23]. Similar to this work, we explore the\ngeneration of synthetic queries with LLMs for a retrieval task. Un-\nlike this work, we demonstrate a data augmentation method for\ncreating effective training data from sets of user documents found in\nrecommendation datasets rather than individual documents. Other\nwork in this space has also explored training more efficient multi-\nvector models from synthetic queries instead of more expensive\ncross-encoder models [39] and generating queries with a diverse\nrange of intents than the ones available in implicit feedback datasets\nto enhance item retrievability [35].\n3https://github.com/iesl/narrative-driven-rec-mint/\nBesides creating queries for ad-hoc retrieval tasks, concurrent\nwork of Leszczynski et al. [25] has also explored the creation of syn-\nthetic conversational search datasets from music recommendation\ndatasets with LLMs. The synthetic queries and user documents are\nthen used to train bi-encoder retrieval models for conversational\nsearch. Our work resembles this in creating synthetic queries from\nsets of user items found in recommendation interaction datasets.\nHowever, it differs in the task of focus, creating long-form narra-\ntive queries for NDR. Finally, our work also builds on the recent\nperspective of Radlinski et al. [36] who make a case for natural\nlanguage user profiles driving recommenders \u2013 narrative requests\ntie closely to natural language user profiles. Our work presents a\nstep toward these systems.\nFinally, while our work explores data augmentation from user-\nitem interactions for a retrieval-oriented NDR task, prior work has\nalso explored data augmentation of the user-item graph for training\ncollaborative filtering models. This work has often explored aug-\nmentation to improve recommendation performance for minority\n[12, 47] or cold-start users [11, 28, 45]. And has leveraged genera-\ntive models [11, 45] and text similarity models [28] for augmenting\nthe user-item graph.\nComplex Queries in Information Access. With the advent\nof performant models for text understanding, focus on complex\nand interactive information access tasks has seen a resurgence\n[2, 29, 32, 48]. NDR presents an example of this \u2013 NDR was first\nformalized in Bogers and Koolen [5] for the case of book recommen-\ndation and subsequently studied in other domains [3, 4, 6]. Bogers\nand Koolen [5] systematically examined narrative requests posted\nby users on discussion forums. They defined NDR as a task requir-\ning item recommendation based on a long-form narrative query\nand prior-user item interactions. While this formulation resembles\npersonalized search [42] and query-driven recommendation [20],\nthe length and complexity of requests differentiate these from NDR.\nOther work has also demonstrated the effectiveness of re-ranking\ninitial recommendations from collaborative filtering approaches\n778\n",
    "Large Language Model Augmented Narrative Driven Recommendations\nRecSys \u201923, September 18\u201322, 2023, Singapore, Singapore\nFigure 3: Mint re-purposes readily available user-item interaction datasets commonly used to train collaborative filtering\nmodels for narrative-driven recommendation. This is done by authoring narrative queries for sets of items liked by a user with\na large language model. The data is filtered with a smaller language model and retrieval models are trained on the synthetic\nqueries and user items.\nbased on the narrative query [18]. More recent work of Afzali et al.\n[1] formulate the NDR task without access to the prior interactions\nof a user while also noting the value of contextual cues contained\nin the narrative request. In our work, we focus on this latter for-\nmulation of NDR, given the lack of focus on effectively using the\nrich narrative queries in most prior work. Further, we demonstrate\nthe usefulness of data augmentation from LLMs and user-item\ninteraction datasets lacking narrative queries.\nBesides this, a range of work has explored more complex, long-\nform, and interactive query formulations for information access;\nthese resemble queries in NDR. Arguello et al. [2] define the tip of\ntongue retrieval task, a known-item search task where user queries\ndescribe the rich context of items while being unable to recall item\nmetadata itself. Mysore et al. [32] formulate an aspect conditional\nquery-by example task where results must match specific aspects of\na long natural language query. And finally, a vibrant body of work\nhas explored conversational critiquing of recommenders where nat-\nural language feedback helps tune the recommendations received\nby users [30, 44, 49].\n3\nMETHOD\n3.1\nProblem Setup\nIn our work, we define narrative-driven recommendation (NDR) to\nbe a ranking task, where given a narrative query \ud835\udc5e made by a user\n\ud835\udc62, a ranking system \ud835\udc53 must generate a ranking \ud835\udc45 over a collection\nof items C. Further, we assume access to a user-item interaction\ndataset I consisting of user interactions with items (\ud835\udc62, {\ud835\udc51\ud835\udc56}\ud835\udc41\ud835\udc62\n\ud835\udc56=1). We\nassume the items \ud835\udc51\ud835\udc56 to be textual documents like reviews or item\ndescriptions. While we don\u2019t assume there to be any overlap in the\nusers making narrative queries or the collection of items C and the\nuser-items interaction dataset I, we assume them to be from the\nsame broad domain, e.g., books, movies, points-of-interest.\n3.2\nProposed Method\nOur proposed method, Mint, for NDR, re-purposes a dataset of\nabundantly available user-item interactions, I = {(\ud835\udc62, {\ud835\udc51\ud835\udc56}\ud835\udc41\ud835\udc62\n\ud835\udc56=1)} into\ntraining data for retrieval models by using LLMs as query gener-\nation models to author narrative queries \ud835\udc5e\ud835\udc62: D = {(\ud835\udc5e\ud835\udc62, {\ud835\udc51\ud835\udc56}\ud835\udc41\ud835\udc62\n\ud835\udc56=1)}.\nThen, retrieval models are trained on the synthetic dataset D (Fig-\nure 3).\n3.2.1\nNarrative Queries from LLMs. To author a narrative query \ud835\udc5e\ud835\udc62\nfor a user in I, we make use of the 175B parameter InstructGPT4\nmodel as our query generation model QGen. We include the text\nof interacted items {\ud835\udc51\ud835\udc56}\ud835\udc41\ud835\udc62\n\ud835\udc56=1 in the prompt for QGen, and instruct it\nto author a narrative query (Figure 2). To improve the coherence\nof generated queries and obtain correctly formatted outputs, we\nmanually author narrative queries for 3 topically diverse users\nbased on their interacted items and include it in the prompt for\nQGen. The same three few shot examples are used for the whole\ndataset I, and the three users were chosen from I. Generating\nnarrative queries based on user interactions may also be considered\na form of multi-document summarization for generating a natural\nlanguage user profile [36].\n3.2.2\nFiltering Items for Synthetic Queries. Since we expect user\nitems to capture multiple aspects of their interests and generated\nqueries to only capture a subset of these interests, we only retain\nsome of the items present in {\ud835\udc51\ud835\udc56}\ud835\udc41\ud835\udc62\n\ud835\udc56=1 before using it for training re-\ntrieval models. For this, we use a pre-trained language model to com-\npute the likelihood of the query given each user item, \ud835\udc43\ud835\udc3f\ud835\udc40 (\ud835\udc5e\ud835\udc62|\ud835\udc51\ud835\udc56),\nand only retain the top \ud835\udc40 highly scoring item for \ud835\udc5e\ud835\udc62, this re-\nsults in \ud835\udc40 training samples per user for our NDR retrieval models:\n{(\ud835\udc5e\ud835\udc62,\ud835\udc51\ud835\udc56)\ud835\udc40\n\ud835\udc56=1}. In our experiments, we use FlanT5 with 3B parame-\nters [14] for computing and follow Sachan et al. [40] for computing\n\ud835\udc43\ud835\udc3f\ud835\udc40 (\ud835\udc5e\ud835\udc62|\ud835\udc51\ud835\udc56). Note that our use of \ud835\udc43\ud835\udc3f\ud835\udc40 (\ud835\udc5e\ud835\udc62|\ud835\udc51\ud835\udc56) represents a query-\nlikelihood model classically used for ad-hoc search and recently\nshown to be an effective unsupervised re-ranking method when\nused with large pre-trained language models [40].\n3.2.3\nTraining Retrieval Models. We train bi-encoder and cross-\nencoder models for NDR on the generated synthetic dataset \u2013 com-\nmonly used models in search tasks. Bi-encoders are commonly used\nas scalable first-stage rankers from a large collection of items. On the\nother hand, cross-encoders allow a richer interaction between query\nand item and are used as second-stage re-ranking models. For both\nmodels, we use a pre-trained transformer language model architec-\nture with 110M parameters, MPnet, a model similar to Bert [41].\nBi-encoder models embed the query and item independently into\nhigh dimensional vectors: q\ud835\udc62 = MPNet(\ud835\udc5e\ud835\udc62), d\ud835\udc56 = MPNet(\ud835\udc51\ud835\udc56) and\nrank items for the user based on the minimum L2 distance between\n4https://platform.openai.com/docs/models/gpt-3, text-davinci-003\n779\n",
    "RecSys \u201923, September 18\u201322, 2023, Singapore, Singapore\nMysore, McCallum, Zamani\nq\ud835\udc62 and d\ud835\udc56. Embeddings are obtained by averaging token embeddings\nfrom the final layer of MPNet, and the same model is used for both\nqueries and items. Cross-encoder models input both the query and\nitem and output a score to be used for ranking \ud835\udc60 = \ud835\udc53Cr([\ud835\udc5e\ud835\udc62;\ud835\udc51\ud835\udc56]),\nwhere \ud835\udc53Cr is parameterized as w\ud835\udc47 dropout\n\ufffd\nW\ud835\udc47 MPNet(\u00b7)\n\ufffd\n. We\ntrain our bi-encoder model with a margin ranking loss: L\ud835\udc35\ud835\udc56 =\n\ufffd\n\ud835\udc62\n\ufffd\ud835\udc40\n\ud835\udc56=1 max[\ud835\udc3f2(q\ud835\udc62, d\ud835\udc56) \u2212 \ud835\udc3f2(q\ud835\udc62, d\n\u2032\n\ud835\udc56) + \ud835\udeff, 0] with randomly sam-\npled negatives \ud835\udc51\n\u2032 and \ud835\udeff = 1. Our cross-encoders are trained with\na cross-entropy loss: L\ud835\udc36\ud835\udc5f = \ufffd\n\ud835\udc62\n\ufffd\ud835\udc40\n\ud835\udc56=1 log(\n\ufffd \ud835\udc52\ud835\udc60\n\ud835\udc51\u2032 \ud835\udc52\ud835\udc60\u2032 ). For training, 4\nnegative example items \ud835\udc51\u2032 are randomly sampled from ranks 100-\n300 from our trained bi-encoder. At test time, we retrieve the top\n200 items with our trained bi-encoder and re-rank them with the\ncross-encoder - we evaluate both these components in experiments\nand refer to them as BiEnc-Mint and CrEnc-Mint.\n4\nEXPERIMENTS AND RESULTS\nNext, we evaluate Mint on a publicly available test collection for\nNDR and present a series of ablations.\n4.1\nExperimental Setup\n4.1.1\nDatasets. We perform evaluations on an NDR dataset for\npoint-of-interest (POI) recommendation Pointrec [1]. Pointrec\ncontains 112 realistic narrative queries (130 words long) obtained\nfrom discussion forums on Reddit and items pooled from baseline\nrankers. The items are annotated on a graded relevance scale by\ncrowd-workers and/or discussion forum members and further vali-\ndated by the dataset authors. The item collection C in Pointrec\ncontains 700k POIs with metadata (category, city) and noisy text\nsnippets describing the POI obtained from the Bing search engine.\nFor test time ranking, we only rank the candidate items in the city\nand request category (e.g., \u201cRestaurants\u201d) of the query available in\nPointrec - this follows prior practice to exclude clearly irrelevant\nitems [1, 26]. We use user-item interaction datasets from Yelp to\ngenerate synthetic queries for training.5 Note also that we limit our\nevaluations to Pointrec since it presents the only publicly avail-\nable, manually annotated, and candidate pooled test collection for\nNDR, to our knowledge. Other datasets for NDR use document col-\nlections that are no longer publicly accessible [24], contain sparse\nand noisy relevance judgments due to them being determined with\nautomatic rules applied to discussion threads [18, 24], lack pooling\nto gather candidates for judging relevance [18, 24], or lack realistic\nnarrative queries [21]. We leave the development of more robust\ntest collections and evaluation methods for NDR to future work.\n4.1.2\nImplementation Details. Next, we describe important details\nfor Mint and leave finer details of the model and training to our\ncode release. To sample user interactions for generating synthetic\nqueries from the Yelp dataset, we exclude POIs and users with\nfewer than ten reviews to ensure that users were regular users of\nthe site with well represented interests. This follows common prior\npractice in preparing user-item interaction datasets for use [27].\nThen we retain users who deliver an average rating greater than\n3/5 and with 10-30 above-average reviews. This desirably biases\nour data to users who commonly describe their likings (rather than\n5https://www.yelp.com/dataset\ndislikes). It also retains the users whose interests are summarizable\nby QGen. In the Yelp dataset, this results in 45,193 retained users.\nNow, 10,000 randomly selected users are chosen for generating syn-\nthetic narrative queries. For these users, a single randomly selected\nsentence from 10 of their reviews is included in the prompt (Figure\n2) to QGen, i.e., \ud835\udc41\ud835\udc62 = 10. After generating synthetic queries, some\nitems are filtered out (\u00a73.2.2). Here, we exclude 40% of the items\nfor a user. This results in about 60,000 training samples for training\nBiEnc-Mint and CrEnc-Mint. These decisions were made manu-\nally by examining the resulting datasets and the cost of authoring\nqueries. The expense of generating \ud835\udc5e\ud835\udc62 was about USD 230.\n4.1.3\nBaselines. We compare BiEnc-Mint and CrEnc-Mint mod-\nels against several standard and performant retrieval model base-\nlines. These span zero-shot/unsupervised rankers, supervised bi-\nencoders, unsupervised cross-encoders, and LLM baselines. BM25:\nA standard unsupervised sparse retrieval baseline based on term\noverlap between query and document, with strong generalization\nperformance across tasks and domains [38]. Contriver: A BERT-base\nbi-encoder model pre-trained for zero-shot retrieval with weakly su-\npervised query-document pairs [22]. MPNet-1B: A strong Sentence-\nBert bi-encoder model initialized with MPNet-base and trained on\n1 billion supervised query-document pairs aggregated from numer-\nous domains [37]. BERT-MSM: A BERT-base bi-encoder fine-tuned\non supervised question-passage pairs from MSMarco. UPR: A two-\nstage approach that retrieves items with a Contriver bi-encoder\nand re-ranks the top 200 items with a query-likelihood model using\na FlanT5 model with 3B parameters [14, 40]. This may be seen\nas an unsupervised \u201ccross-encoder\u201d model. Grounded LLM: A re-\ncently proposed two-stage approach which autoregressively gener-\nates ten pseudo-relevant items using an LLM (175B InstructGPT)\nprompted with the narrative query and generates recommenda-\ntions grounded in C by retrieving the nearest neighbors for each\ngenerated item using a bi-encoder [19]. We include one few-shot\nexample of a narrative query and recommended items in the prompt\nto the LLM. We run this baseline three times and report average\nperformance across runs. We report NDCG at 5 and 10, MAP, MRR,\nand Recall at 100 and 200. Finally, our reported results should be\nconsidered lower bounds on realistic performance due to the un-\njudged documents (about 70% at \ud835\udc58 = 10) in our test collections\n[10].\n4.2\nResults\nTable 1 presents the performance of the proposed method compared\nagainst baselines. Here, bold numbers indicate the best-performing\nmodel, and superscripts indicate statistical significance computed\nwith two-sided t-tests at \ud835\udc5d < 0.05.\nHere, we first note the performance of baseline approaches. We\nsee BM25 outperformed by Contriver, a transformer bi-encoder\nmodel trained for zero-shot retrieval; this mirrors prior work [22].\nNext, we see supervised bi-encoder models trained on similar pas-\nsage (MPNet-1B) and question-answer (BERT-MSM) pairs outper-\nform a weakly supervised model (Contriver) by smaller margins.\nFinally, the Grounded LLM outperforms all bi-encoder baselines, in-\ndicating strong few-shot generalization and mirroring prior results\n[19]. Examining the Mint models, we first note that the BiEnc-\nMint sees statistically significant improvement compared to BM25\n780\n",
    "Large Language Model Augmented Narrative Driven Recommendations\nRecSys \u201923, September 18\u201322, 2023, Singapore, Singapore\nTable 1: Performance of the proposed method, Mint, for point-of-interest recommendation on Pointrec. The superscripts\ndenote statistically significant improvements compared to specific baseline models.\nPointrec\nModel\nParameters\nNDCG@5\nNDCG@10\nMAP\nMRR\nRecall@100\nRecall@200\n1BM25\n-\n0.2682\n0.2464\n0.1182\n0.2685\n0.4194\n0.5429\n2Contriver\n110M\n0.2924\n0.2776\n0.1660\n0.3355\n0.4455\n0.5552\n3MPNet-1B\n110M\n0.3038\n0.2842\n0.1621\n0.3566\n0.4439\n0.5657\n4BERT-MSM\n110M\n0.3117\n0.2886\n0.1528\n0.3320\n0.4679\n0.5816\n5Grounded LLM\n175B+110M\n0.3558\n0.3251\n0.1808\n0.3861\n0.4797\n0.5797\n6UPR\n110M+3B\n0.3586\n0.3242\n0.1712\n0.4013\n0.4489\n0.5552\nBiEnc-Mint\n110M\n0.34891\n0.32631\n0.18901\n0.39821\n0.49141\n0.6221\nCrEnc-Mint\n2\u00d7110M\n0.372512\n0.348912\n0.219214\n0.43171\n0.5448123\n0.6221\nand outperforms the best bi-encoder baselines by 11-13% on preci-\nsion measures and 5-7% on recall measures. Specifically, we see a\nmodel trained for question-answering (BERT-MSM) underperform\nBiEnc-Mint, indicating the challenge of the NDR task. Further,\nBiEnc-Mint, trained on 5 orders of magnitude lesser data than\nMPNet-1B, sees improved performance \u2013 indicating the quality of\ndata obtained from Mint. Furthermore, BiEnc-Mint also performs\nat par with a 175B LLM while offering the inference efficiency of a\nsmall-parameter bi-encoder. Next, we see CrEnc-Mint outperform\nthe baseline bi-encoders, BiEnc-Mint, UPR, and Grounded LLM\nby 4-21% on precision measures and 7-13% on recall measures \u2013\ndemonstrating the value of Mint for training NDR models.\n4.3\nAblations\nIn Table 2, we ablate various design choices in Mint. Different\nchoices result in different training sets for the BiEnc and CrEnc\nmodels. Also, note that in reporting ablation performance for CrEnc,\nwe still use the performant BiEnc-Mint model for obtaining nega-\ntive examples for training and first-stage ranking. Without high-\nquality negative examples, we found CrEnc to result in much poorer\nperformance.\nNo item filtering. Since synthetic queries are unlikely to rep-\nresent all the items of a user, Mint excludes user items {\ud835\udc51\ud835\udc56}\ud835\udc41\ud835\udc62\n\ud835\udc56=1\nwhich have a low likelihood of being generated from the document\n(\u00a73.2.2). Without this step, we expect the training set for training\nretrieval models to be larger and noisier. In Table 2, we see that\nexcluding this step leads to a lower performance for BiEnc and\nCrEnc, indicating that the quality of data obtained is important for\nperformance.\n6B LLM for QGen. Mint relies on using an expensive 175B pa-\nrameter InstructGPT model for QGen. Here, we investigate the\nefficacy for generating\ud835\udc5e\ud835\udc62 for {\ud835\udc51\ud835\udc56}\ud835\udc41\ud835\udc62\n\ud835\udc56=1 with a 6B parameter Instruct-\nGPT model (text-curie-001). We use an identical setup to the\n175B LLM for this. In Table 2, we see that training on the synthetic\nnarrative queries of the smaller LLM results in worse models \u2013 of-\nten underperforming the baselines in Table 1. This indicates the\ninability of a smaller model to generate complex narrative queries\nwhile conditioning on a set of user items. This necessity of a larger\nLLM for generating queries in complex retrieval tasks has been\nobserved in prior work [15, 23].\n6B LLM for Item Queries. We find a smaller 6B LLM to result\nin poor quality data when used to generate narrative queries con-\nditioned on {\ud835\udc51\ud835\udc56}\ud835\udc41\ud835\udc62\n\ud835\udc56=1. Here we simplify the text generation task \u2013\nusing a 6B LLM to generate queries for individual items \ud835\udc51\ud835\udc56. This\nexperiment also mirrors the setup for generating synthetic queries\nfor search tasks [7, 15]. Here, we use 3-few shot examples and sam-\nple one item per user for generating \ud835\udc5e\ud835\udc62. Given the lower cost of\nusing a smaller LLM, we use all 45,193 users in our Yelp dataset\nrather than a smaller random sample. From Table 2, we see that this\nresults in higher quality queries than using smaller LLMs for gen-\nerating narrative queries from {\ud835\udc51\ud835\udc56}\ud835\udc41\ud835\udc62\n\ud835\udc56=1. The resulting BiEnc model\nunderperforms the BiEnc-Mint, indicating the value of generating\ncomplex queries conditioned on multiple items as in Mint for NDR.\nWe see that CrEnc approaches the performance of CrEnc-Mint\u2013\nnote, however, that this approach uses the performant BiEnc-Mint\nfor sampling negatives and first stage ranking. We leave further\nexploration of using small parameter LLMs for data augmentation\nfor NDR models to future work.\n5\nCONCLUSIONS\nIn this paper, we present Mint, a data augmentation method for the\nnarrative-driven recommendation (NDR) task. Mint re-purposes\nhistorical user-item interaction datasets for NDR by using a 175B pa-\nrameter large language model to author long-form narrative queries\nwhile conditioning on the text of items liked by users. We evaluate\nbi-encoder and cross-encoder models trained on data from Mint on\nthe publicly available Pointrec test collection for narrative-driven\npoint of interest recommendation. We demonstrate that the result-\ning models outperform several strong baselines and ablated models\nand match or outperform a 175B LLM directly used for NDR in a\n1-shot setup.\nHowever, Mint also presents some limitations. Given our use of\nhistorical interaction datasets for generating synthetic training data\nand the prevalence of popular interests in these datasets longer,\ntailed interests are unlikely to be present in the generated syn-\nthetic datasets. In turn, causing retrieval models to likely see poorer\nperformance on these requests. Our use of LLMs to generate syn-\nthetic queries also causes the queries to be repetitive in structure,\nlikely causing novel longer-tail queries to be poorly served. These\nlimitations may be addressed in future work.\n781\n",
    "RecSys \u201923, September 18\u201322, 2023, Singapore, Singapore\nMysore, McCallum, Zamani\nTable 2: Mint ablated for different design choices on Pointrec.\nPointrec\nAblation\nNDCG@5\nNDCG@10\nMAP\nMRR\nRecall@100\nRecall@200\nBiEnc-Mint\n0.3489\n0.3263\n0.1890\n0.3982\n0.5263\n0.6221\n\u2212 No item filtering\n0.2949\n0.2766\n0.1634\n0.3505\n0.4979\n0.5951\n\u2212 6B LLM for QGen\n0.2336\n0.2293\n0.1125\n0.2287\n0.426\n0.5435\n\u2212 6B LLM for Item Queries\n0.3012\n0.2875\n0.1721\n0.3384\n0.4800\n0.5909\nCrEnc-Mint\n0.3725\n0.3489\n0.2192\n0.4317\n0.5448\n0.6221\n\u2212 No item filtering\n0.3570\n0.3379\n0.2071\n0.4063\n0.5366\n0.6221\n\u2212 6B LLM for QGen\n0.2618\n0.2421\n0.1341\n0.3118\n0.4841\n0.6221\n\u2212 6B LLM for Item Queries\n0.3792\n0.3451\n0.2128\n0.4098\n0.5546\n0.6221\nBesides this, other avenues also present rich future work. While\nMint leverages a 175B LLM for generating synthetic queries, smaller\nparameter LLMs may be explored for this purpose - perhaps by\ntraining dedicated QGen models. Mint may also be expanded to\nexplore more active strategies for sampling items and users for\nwhom narrative queries are authored - this may allow more effi-\ncient use of large parameter LLMs while ensuring higher quality\ntraining datasets. Next, the generation of synthetic queries from\nsets of documents may be explored for a broader range of retrieval\ntasks beyond NDR given its promise to generate larger training\nsets \u2013 a currently underexplored direction. Finally, given the lack of\nlarger-scale test collections for NDR and the effectiveness of LLMs\nfor authoring narrative queries from user-item interaction, fruitful\nfuture work may also explore the creation of larger-scale datasets\nin a mixed-initiative setup to robustly evaluate models for NDR.\nACKNOWLEDGMENTS\nWe thank anonymous reviewers for their invaluable feedback. This\nwork was partly supported by the Center for Intelligent Informa-\ntion Retrieval, NSF grants IIS-1922090 and 2143434, the Office of\nNaval Research contract number N000142212688, an Amazon Alexa\nPrize grant, and the Chan Zuckerberg Initiative under the project\nScientific Knowledge Base Construction. Any opinions, findings\nand conclusions or recommendations expressed here are those of\nthe authors and do not necessarily reflect those of the sponsors.\nREFERENCES\n[1] Jafar Afzali, Aleksander Mark Drzewiecki, and Krisztian Balog. 2021. POINTREC:\nA Test Collection for Narrative-Driven Point of Interest Recommendation. In\nProceedings of the 44th International ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval (Virtual Event, Canada) (SIGIR \u201921). As-\nsociation for Computing Machinery, New York, NY, USA, 2478\u20132484.\nhttps:\n//doi.org/10.1145/3404835.3463243\n[2] Jaime Arguello, Adam Ferguson, Emery Fine, Bhaskar Mitra, Hamed Zamani, and\nFernando Diaz. 2021. Tip of the Tongue Known-Item Retrieval: A Case Study in\nMovie Identification. In Proceedings of the 6th international ACM SIGIR Conference\non Human Information Interaction and Retrieval. ACM. https://dlnext.acm.org/\ndoi/10.1145/3406522.3446021\n[3] Toine Bogers, Maria G\u00e4de, Marijn Koolen, Vivien Petras, and Mette Skov. 2018.\n\u201cWhat was this Movie About this Chick?\u201d A Comparative Study of Relevance\nAspects in Book and Movie Discovery. In Transforming Digital Worlds: 13th Inter-\nnational Conference, iConference 2018, Sheffield, UK, March 25-28, 2018, Proceedings\n13. Springer, 323\u2013334.\n[4] Toine Bogers, Maria G\u00e4de, Marijn Koolen, Vivien Petras, and Mette Skov. 2019.\n\u201cLooking for an amazing game I can relax and sink hours into...\u201d: A Study of\nRelevance Aspects in Video Game Discovery. In Information in Contemporary\nSociety: 14th International Conference, iConference 2019, Washington, DC, USA,\nMarch 31\u2013April 3, 2019, Proceedings 14. Springer, 503\u2013515.\n[5] Toine Bogers and Marijn Koolen. 2017. Defining and Supporting Narrative-Driven\nRecommendation. In Proceedings of the Eleventh ACM Conference on Recommender\nSystems (Como, Italy) (RecSys \u201917). Association for Computing Machinery, New\nYork, NY, USA, 238\u2013242. https://doi.org/10.1145/3109859.3109893\n[6] Toine Bogers and Marijn Koolen. 2018. \u201cI\u2019m looking for something like...\u201d:\nCombining Narratives and Example Items for Narrative-driven Book Recommen-\ndation. In Knowledge-aware and Conversational Recommender Systems Workshop.\nCEUR Workshop Proceedings.\n[7] Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022.\nInPars: Unsupervised Dataset Generation for Information Retrieval. In Proceedings\nof the 45th International ACM SIGIR Conference on Research and Development\nin Information Retrieval (Madrid, Spain) (SIGIR \u201922). Association for Computing\nMachinery, New York, NY, USA, 2387\u20132392.\nhttps://doi.org/10.1145/3477495.\n3531863\n[8] Leonid Boytsov, Preksha Patel, Vivek Sourabh, Riddhi Nisar, Sayani Kundu,\nRamya Ramanathan, and Eric Nyberg. 2023. InPars-Light: Cost-Effective Unsu-\npervised Training of Efficient Rankers. arXiv:2301.02998\n[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter,\nChris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners.\nIn Advances in Neural Information Processing Systems, H. Larochelle, M. Ran-\nzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates,\nInc., 1877\u20131901.\nhttps://proceedings.neurips.cc/paper_files/paper/2020/file/\n1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf\n[10] Chris Buckley and Ellen M. Voorhees. 2004. Retrieval Evaluation with Incomplete\nInformation. In Proceedings of the 27th Annual International ACM SIGIR Conference\non Research and Development in Information Retrieval (Sheffield, United Kingdom)\n(SIGIR \u201904). Association for Computing Machinery, New York, NY, USA, 25\u201332.\nhttps://doi.org/10.1145/1008992.1009000\n[11] Dong-Kyu Chae, Jihoo Kim, Duen Horng Chau, and Sang-Wook Kim. 2020. AR-\nCF: Augmenting Virtual Users and Items in Collaborative Filtering for Addressing\nCold-Start Problems. In Proceedings of the 43rd International ACM SIGIR Con-\nference on Research and Development in Information Retrieval (Virtual Event,\nChina) (SIGIR \u201920). Association for Computing Machinery, New York, NY, USA,\n1251\u20131260. https://doi.org/10.1145/3397271.3401038\n[12] Lei Chen, Le Wu, Kun Zhang, Richang Hong, Defu Lian, Zhiqiang Zhang, Jun\nZhou, and Meng Wang. 2023. Improving Recommendation Fairness via Data\nAugmentation. In Proceedings of the ACM Web Conference 2023 (Austin, TX,\nUSA) (WWW \u201923). Association for Computing Machinery, New York, NY, USA,\n1012\u20131020. https://doi.org/10.1145/3543507.3583341\n[13] Li Chen, Zhirun Zhang, Xinzhi Zhang, and Lehong Zhao. 2022. A Pilot Study\nfor Understanding Users\u2019 Attitudes Towards a Conversational Agent for News\nRecommendation. In Proceedings of the 4th Conference on Conversational User\nInterfaces (Glasgow, United Kingdom) (CUI \u201922). Association for Computing\nMachinery, New York, NY, USA, Article 36, 6 pages.\nhttps://doi.org/10.1145/\n3543829.3544530\n[14] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus,\nEric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling\ninstruction-finetuned language models. arXiv preprint arXiv:2210.11416 (2022).\n[15] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov,\nKelvin Guu, Keith Hall, and Ming-Wei Chang. 2023. Promptagator: Few-shot\n782\n",
    "Large Language Model Augmented Narrative Driven Recommendations\nRecSys \u201923, September 18\u201322, 2023, Singapore, Singapore\nDense Retrieval From 8 Examples. In The Eleventh International Conference on\nLearning Representations. https://openreview.net/forum?id=gmL46YMpu2J\n[16] Abhinandan S. Das, Mayur Datar, Ashutosh Garg, and Shyam Rajaram. 2007.\nGoogle News Personalization: Scalable Online Collaborative Filtering. In Pro-\nceedings of the 16th International Conference on World Wide Web (Banff, Alberta,\nCanada) (WWW \u201907). Association for Computing Machinery, New York, NY, USA,\n271\u2013280. https://doi.org/10.1145/1242572.1242610\n[17] James Davidson, Benjamin Liebald, Junning Liu, Palash Nandy, Taylor Van Vleet,\nUllas Gargi, Sujoy Gupta, Yu He, Mike Lambert, Blake Livingston, and Dasarathi\nSampath. 2010. The YouTube Video Recommendation System. In Proceedings of\nthe Fourth ACM Conference on Recommender Systems (Barcelona, Spain) (RecSys\n\u201910). Association for Computing Machinery, New York, NY, USA, 293\u2013296. https:\n//doi.org/10.1145/1864708.1864770\n[18] Lukas Eberhard, Simon Walk, Lisa Posch, and Denis Helic. 2019. Evaluating\nNarrative-Driven Movie Recommendations on Reddit. In Proceedings of the 24th\nInternational Conference on Intelligent User Interfaces (Marina del Ray, California)\n(IUI \u201919). Association for Computing Machinery, New York, NY, USA, 1\u201311. https:\n//doi.org/10.1145/3301275.3302287\n[19] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022. Precise Zero-Shot\nDense Retrieval without Relevance Labels. arXiv preprint arXiv:2212.10496 (2022).\n[20] Negar Hariri, Bamshad Mobasher, and Robin Burke. 2013. Query-Driven Context\nAware Recommendation. In Proceedings of the 7th ACM Conference on Recom-\nmender Systems (Hong Kong, China) (RecSys \u201913). Association for Computing\nMachinery, New York, NY, USA, 9\u201316. https://doi.org/10.1145/2507157.2507187\n[21] Seyyed Hadi Hashemi, Jaap Kamps, Julia Kiseleva, Charles LA Clarke, and Ellen M\nVoorhees. 2016. Overview of the TREC 2016 Contextual Suggestion Track.. In\nTREC.\n[22] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo-\njanowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised Dense Infor-\nmation Retrieval with Contrastive Learning. Transactions on Machine Learning\nResearch (2022). https://openreview.net/forum?id=jKN1pXi7b0\n[23] Vitor Jeronymo, Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, Roberto Lotufo,\nJakub Zavrel, and Rodrigo Nogueira. 2023. InPars-v2: Large Language Models as\nEfficient Dataset Generators for Information Retrieval. arXiv:2301.01820\n[24] Marijn Koolen, Toine Bogers, Maria G\u00e4de, Mark Hall, Iris Hendrickx, Hugo\nHuurdeman, Jaap Kamps, Mette Skov, Suzan Verberne, and David Walsh. 2016.\nOverview of the CLEF 2016 Social Book Search Lab. In Experimental IR Meets Mul-\ntilinguality, Multimodality, and Interaction, Norbert Fuhr, Paulo Quaresma, Teresa\nGon\u00e7alves, Birger Larsen, Krisztian Balog, Craig Macdonald, Linda Cappellato,\nand Nicola Ferro (Eds.). Springer International Publishing, Cham, 351\u2013370.\n[25] Megan Leszczynski, Ravi Ganti, Shu Zhang, Krisztian Balog, Filip Radlinski,\nFernando Pereira, and Arun Tejasvi Chaganty. 2023. Generating Synthetic Data\nfor Conversational Music Recommendation Using Random Walks and Language\nModels. arXiv:2301.11489\n[26] Xin Liu, Yong Liu, Karl Aberer, and Chunyan Miao. 2013. Personalized Point-of-\nInterest Recommendation by Mining Users\u2019 Preference Transition. In Proceedings\nof the 22nd ACM International Conference on Information & Knowledge Manage-\nment (San Francisco, California, USA) (CIKM \u201913). Association for Computing Ma-\nchinery, New York, NY, USA, 733\u2013738. https://doi.org/10.1145/2505515.2505639\n[27] Yiding Liu, Tuan-Anh Nguyen Pham, Gao Cong, and Quan Yuan. 2017. An\nExperimental Evaluation of Point-of-Interest Recommendation in Location-Based\nSocial Networks. Proc. VLDB Endow. 10, 10 (jun 2017), 1010\u20131021. https://doi.\norg/10.14778/3115404.3115407\n[28] Federico L\u00f3pez, Martin Scholz, Jessica Yung, Marie Pellat, Michael Strube, and\nLucas Dixon. 2021. Augmenting the user-item graph with textual similarity\nmodels. arXiv preprint arXiv:2109.09358 (2021).\n[29] Xing Han Lu, Siva Reddy, and Harm de Vries. 2023. The StatCan Dialogue\nDataset: Retrieving Data Tables through Conversations with Genuine Intents. In\nProceedings of the 17th Conference of the European Chapter of the Association for\nComputational Linguistics. Association for Computational Linguistics, Dubrovnik,\nCroatia, 2799\u20132829. https://aclanthology.org/2023.eacl-main.206\n[30] Kai Luo, Scott Sanner, Ga Wu, Hanze Li, and Hojin Yang. 2020. Latent Linear\nCritiquing for Conversational Recommender Systems. In The Web Conference.\n[31] Ji Ma, Ivan Korotkov, Yinfei Yang, Keith Hall, and Ryan McDonald. 2021. Zero-shot\nNeural Passage Retrieval via Domain-targeted Synthetic Question Generation.\nIn Proceedings of the 16th Conference of the European Chapter of the Associa-\ntion for Computational Linguistics: Main Volume. Association for Computational\nLinguistics, Online, 1075\u20131088. https://doi.org/10.18653/v1/2021.eacl-main.92\n[32] Sheshera Mysore, Tim O\u2019Gorman, Andrew McCallum, and Hamed Zamani. 2021.\nCSFCube - A Test Collection of Computer Science Research Articles for Faceted\nQuery by Example. In Thirty-fifth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track (Round 2). https://doi.org/10.48550/arXiv.\n2103.12906\n[33] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda\nAskell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with human feedback. In\nAdvances in Neural Information Processing Systems, S. Koyejo, S. Mohamed,\nA. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates,\nInc., 27730\u201327744. https://proceedings.neurips.cc/paper_files/paper/2022/file/\nb1efde53be364a73914f58805a001731-Paper-Conference.pdf\n[34] Andrea Papenmeier, Dagmar Kern, Daniel Hienert, Alfred Sliwa, Ahmet Aker,\nand Norbert Fuhr. 2021. Starting Conversations with Search Engines - Interfaces\nThat Elicit Natural Language Queries. In Proceedings of the 2021 Conference on\nHuman Information Interaction and Retrieval (Canberra ACT, Australia) (CHIIR\n\u201921). Association for Computing Machinery, New York, NY, USA, 261\u2013265. https:\n//doi.org/10.1145/3406522.3446035\n[35] Gustavo Penha, Enrico Palumbo, Maryam Aziz, Alice Wang, and Hugues\nBouchard. 2023. Improving Content Retrievability in Search with Controllable\nQuery Generation. In Proceedings of the ACM Web Conference 2023 (Austin, TX,\nUSA) (WWW \u201923). Association for Computing Machinery, New York, NY, USA,\n3182\u20133192. https://doi.org/10.1145/3543507.3583261\n[36] Filip Radlinski, Krisztian Balog, Fernando Diaz, Lucas Dixon, and Ben Wedin.\n2022. On Natural Language User Profiles for Transparent and Scrutable Rec-\nommendation. In Proceedings of the 45th International ACM SIGIR Conference\non Research and Development in Information Retrieval (Madrid, Spain) (SIGIR\n\u201922). Association for Computing Machinery, New York, NY, USA, 2863\u20132874.\nhttps://doi.org/10.1145/3477495.3531873\n[37] Nils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings\nusing Siamese BERT-Networks. In Proceedings of the 2019 Conference on Em-\npirical Methods in Natural Language Processing. Association for Computational\nLinguistics. https://arxiv.org/abs/1908.10084\n[38] Stephen Robertson and Hugo Zaragoza. 2009.\nThe Probabilistic Relevance\nFramework: BM25 and Beyond. Found. Trends Inf. Retr. 3, 4 (apr 2009), 333\u2013389.\nhttps://doi.org/10.1561/1500000019\n[39] Jon Saad-Falcon, Omar Khattab, Keshav Santhanam, Radu Florian, Martin\nFranz, Salim Roukos, Avirup Sil, Md Arafat Sultan, and Christopher Potts. 2023.\nUDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation\nof Rerankers. arXiv:2303.00807 [cs.IR]\n[40] Devendra Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wen-tau\nYih, Joelle Pineau, and Luke Zettlemoyer. 2022. Improving Passage Retrieval\nwith Zero-Shot Question Generation. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing. Association for Computational\nLinguistics, Abu Dhabi, United Arab Emirates, 3781\u20133797. https://aclanthology.\norg/2022.emnlp-main.249\n[41] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. 2020. MPNet: Masked\nand Permuted Pre-training for Language Understanding. In Advances in Neural\nInformation Processing Systems, Vol. 33. https://proceedings.neurips.cc/paper_\nfiles/paper/2020/file/c3a690be93aa602ee2dc0ccab5b7b67e-Paper.pdf\n[42] Jaime Teevan, Susan T. Dumais, and Eric Horvitz. 2005. Personalizing Search via\nAutomated Analysis of Interests and Activities. In Proceedings of the 28th Annual\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval (Salvador, Brazil) (SIGIR \u201905). Association for Computing Machinery,\nNew York, NY, USA, 449\u2013456. https://doi.org/10.1145/1076034.1076111\n[43] Mengting Wan and Julian McAuley. 2018. Item Recommendation on Monotonic\nBehavior Chains. In Proceedings of the 12th ACM Conference on Recommender\nSystems (Vancouver, British Columbia, Canada) (RecSys \u201918). Association for\nComputing Machinery, New York, NY, USA, 86\u201394.\nhttps://doi.org/10.1145/\n3240323.3240369\n[44] Haonan Wang, Chang Zhou, Carl Yang, Hongxia Yang, and Jingrui He. 2021.\nControllable Gradient Item Retrieval. In Web Conference.\n[45] Qinyong Wang, Hongzhi Yin, Hao Wang, Quoc Viet Hung Nguyen, Zi Huang,\nand Lizhen Cui. 2019. Enhancing Collaborative Filtering with Generative Aug-\nmentation. In Proceedings of the 25th ACM SIGKDD International Conference\non Knowledge Discovery & Data Mining (Anchorage, AK, USA) (KDD \u201919). As-\nsociation for Computing Machinery, New York, NY, USA, 548\u2013556.\nhttps:\n//doi.org/10.1145/3292500.3330873\n[46] Jiajing Xu, Andrew Zhai, and Charles Rosenberg. 2022. Rethinking Personalized\nRanking at Pinterest: An End-to-End Approach. In Proceedings of the 16th ACM\nConference on Recommender Systems (Seattle, WA, USA) (RecSys \u201922). Association\nfor Computing Machinery, New York, NY, USA, 502\u2013505.\nhttps://doi.org/10.\n1145/3523227.3547394\n[47] Yuxin Ying, Fuzhen Zhuang, Yongchun Zhu, Deqing Wang, and Hongwei Zheng.\n2023. CAMUS: Attribute-Aware Counterfactual Augmentation for Minority Users\nin Recommendation. In Proceedings of the ACM Web Conference 2023 (Austin, TX,\nUSA) (WWW \u201923). Association for Computing Machinery, New York, NY, USA,\n1396\u20131404. https://doi.org/10.1145/3543507.3583538\n[48] Hamed Zamani, Johanne R Trippas, Jeff Dalton, and Filip Radlinski. 2022. Con-\nversational information seeking. arXiv preprint arXiv:2201.08808 (2022).\n[49] Jie Zou, Yifan Chen, and Evangelos Kanoulas. 2020. Towards Question-Based\nRecommender Systems. In Proceedings of the 43rd International ACM SIGIR Confer-\nence on Research and Development in Information Retrieval (Virtual Event, China)\n(SIGIR \u201920). Association for Computing Machinery, New York, NY, USA, 881\u2013890.\nhttps://doi.org/10.1145/3397271.3401180\n783\n"
]