[
    "AI Model for Computer games based on Case Based \nReasoning and AI Planning  \nVlado Menkovski \nAthens Information Technology \n0.8km Markopoulou Ave. \nPeania, 19002, Greece \nvmen@ait.edu.gr \nDimitrios Metafas \nAthens Information Technology \n0.8km Markopoulou Ave. \nPeania, 19002, Greece \ndmeta@ait.edu.gr \nAbstract\nMaking efficient AI models for games with imperfect \ninformation can be a particular challenge. Considering the large \nnumber of possible moves and the incorporated uncertainties \nbuilding game trees for these games becomes very difficult due to \nthe exponential growth of the number of nodes at each level. This \neffort is focused on presenting a method of combined Case Based \nReasoning (CBR) with AI Planning which drastically reduces the \nsize of game trees. Instead of looking at all possible combinations \nwe can focus only on the moves that lead us to specific strategies \nin effect discarding meaningless moves. These strategies are \nselected by finding similarities to cases in the CBR database. The \nstrategies are formed by a set of desired goals. The AI planning is \nresponsible for creating a plan to reach these goals. The plan is \nbasically a set of moves that brings the player to this goal. By \nfollowing these steps and not regarding the vast number of other \npossible moves the model develops Game Trees which grows \nslower so they can be built with more feature moves restricted by \nthe same amount of memory.  \nCategories and Subject Descriptors \nI.2.1 [Applications and Expert Systems]: Games\nGeneral Terms\nAlgorithms, Performance. \nKeywords\nGame AI, Case Based Reasoning, AI Planning, Game Trees \n1. Introduction \nThe goal of this effort is to explore a model for design and \nimplementation of an AI agent for turn based games. This model \nprovides for building more capable computer opponents that rely \non strategies that closely resemble human approach in solving \nproblems opposed to classical computational centric heuristics in \ngame AI. In this manner the computational resources can be \nfocused on more sensible strategies for the game play.  \nWith the advancement in computer hardware increasingly \nmore computing power is left for executing AI algorithms in \ngames. In the past AI in games was mainly a cheating set of \ninstructions that simulated the increasing difficulty in the game \nenvironment so that the player had the illusion of real counterpart. \nImprovement in available memory and processing power allows \nimplementation of more intelligent algorithms for building the \ngame environment as well as direct interaction with the human \nplayers.   \nIn this particular research the emphasis is put on the \ninteraction between the AI agent and a computer player in the \nrealm of the game rules. It is particularly focused on turn based \ngames that have the elements of uncertainty like dice or concealed \ninformation. At the beginning a description of Game AI \nalgorithms are given; such as Game Trees and Minimax. The \nfollowing section describes an approach of using AI Planning to \nimprove building Game Trees in games with imperfect \ninformation where Game Trees tend to be very large with high \ngrowth ratio. Section 4 discusses another approach that provides a \nsignificant reduction to the number of considered moves in order \nto find the favorable strategy of the AI player. This approach uses \nAI Planning techniques and Case Base Reasoning (CBR) to plan \nfor different scenarios in predetermined strategies which would be \nanalogous to human player experience in the particular game. The \nCBR database illustrates a set of past experiences for the AI \nproblem and the AI Planning illustrates the procedure to deal with \nthe given situation in the game. In the next two sections \nimplementations and evaluations of both approaches are given. \nThe AI Planning approach is implemented with the Tic-tac-toe \ngame and the combined AI Planning and CBR approach is \nimplemented with a model for the Monopoly game. The last part \ncontains conclusions and future work ideas.  \n2. Game Trees and Minimax \nGame Trees are common model for evaluating how different \ncombinations of moves from the player and his opponents will \naffect the future position of the player and eventually the end \nresult of the game. An algorithm that decides on the next move by \nevaluating the results from the built Game Tree is minimax [1]. \nMinimax assumes that the player at hand will always choose the \nbest possible move for him, in other words the player will try to \nselect the move that maximizes the result of the evaluation \nfunction over the game state. So basically the player at hand needs \nto choose the best move overall while taking into account that the \nnext player(s) will try to do the same thing. Minimax tries to \nmaximize the minimum gain. Minimax can be applied to multiple \nPermission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are \nnot made or distributed for profit or commercial advantage and that \ncopies bear this notice and the full citation on the first page. To copy \notherwise, or republish, to post on servers or to redistribute to lists, \nrequires prior specific permission and/or a fee. \nDIMEA\u201908, September 10\u201312, 2008, Athens, Greece. \nCopyright 2008 ACM 978-1-60558-248-1/08/09... $5.00 \nInteractive and Adaptable Media\n295\n3rd International Conference on Digital Interactive Media in Entertainment and Arts\n",
    "levels of nodes on the game tree, where the leaves bring the final \nknown (or considered) game state.  \nThe minimax theorem states: \nFor every two-person, zero-sum game there is a mixed strategy \nfor each player, such that the expected payoff for both is the same \nvalue V when the players use these strategies. Furthermore, V is \nthe best payoff each can expect to receive from a play of the \ngame; that is, these mixed strategies are the optimal strategies for \nthe two players. \nThis theorem was established by John von Neumann, who is \nquoted as saying \"As far as I can see, there could be no theory of \ngames \u2026 without that theorem \u2026 I thought there was nothing \nworth publishing until the Minimax Theorem was proved\" [2]. \nA simple example of minimax can be observed by building a \ngame tree of the tic-tac-toe game. The tic-tac-toe game is a simple \ngame which can end by the first player wining, the second player \nwining or a tie. There are nine positions for each of the players in \nwhich at each turn the player puts X or O sign. If the player has \nthree adjacent signs in a row, column or the two diagonals he or \nshe wins. This game has limited number of position and it is well \nsuited for building the whole game tree. The leaves of this tree \nwill be final positions in the game. A heuristics evaluation \nfunction will also need to be written to evaluate the value of each \nnode along the way. \n3. AI Planning for building Game Trees \n3.1.1 AI Planning \nAI Planning also referred as Automated Planning and \nScheduling is a branch of Artificial Intelligence that focuses on \nfinding strategies or sequences of actions that reach a predefined \ngoal [3]. Typical execution of AI Planning algorithms is by \nintelligent agents, autonomous robots and unmanned vehicles. \nOpposed to classical control or classification AI Planning results \nwith complex solutions that are derived from multidimensional \nspace. \n AI Planning algorithms are also common in the video game \ndevelopment. They solve broad range of problems from path \nfinding to action planning. A typical planner takes three inputs: a \ndescription of the initial state of the world, a description of the \ndesired goal, and a set of possible actions. Some efforts for \nincorporating planning techniques for building game trees have \nalso shown up, similar to the approach explored in this effort. In \naddition Cased Based Reasoning [4] techniques are also gathering \npopularity in developing strategies based in prior knowledge \nabout the problems in the games. One of the benefits from \nHierarchical Task Network (HTN) [5] planning is the possibility \nto build Game Trees based on HTN plans; this method is \ndescribed in the following section. \n3.2 Game Trees with AI Planning \nAn adaptation of the HTN planning can be used to build \nmuch smaller and more efficient game trees. This idea has already \nbeen implemented in the Bridge Baron a computer program for \nthe game of Contact Bridge [6]. \nComputer programs based on Game Tree search techniques \nare now as good as or better than humans in many games like \nChess [7] and checkers [8], but there are some difficulties in \nbuilding a game tree for games that have imperfect information \nand added uncertainty like card or games with dice. The main \nproblem is the enormous number of possibilities that the player \ncan choose from in making his move. In addition some of the \nmoves are accompanied with probabilities based on the random \nelements in the games. The number of possible moves \nexponentially grows with each move so the depth of the search \nhas to be very limited to accommodate for the memory \nlimitations.  \nThe basic idea behind using HTN for building game trees is \nthat the HTN provides the means of expressing high level goals \nand describing strategies how to reach those goals. These goals \nmay be decomposed in goals at lower level called sub-goals. This \napproach closely resembles the way a human player usually \naddresses a complex problem. It is also good for domains where \nclassical search for solution is not feasible due to the vastness of \nthe problem domain or uncertainties. \n3.2.1 Hierarchical Task Networks \nThe Hierarchical Task Network, or HTN, is an approach to \nautomated planning in which the dependency among actions can \nbe given in the form of networks [9] [Figure 1]. \nA simple task network (or just a task network for short) is an \nacyclic digraph \ufffd \ufffd \ufffd\ufffd\ufffd \ufffd\ufffd in which U is the node set, E is the \nedge set, and each node \ufffd \ufffd \ufffd contains a task \ufffd\ufffd. The edges of \ufffd\ndefine a partial ordering of U. If the partial ordering is total, then \nwe say that \ufffd is totally ordered, in which case \ufffd can be written as \na sequence of tasks \ufffd \ufffd \ufffd\ufffd\ufffd\ufffd \ufffd\ufffd\ufffd \ufffd \ufffd \ufffd\ufffd\ufffd.\nFigure 1: Simple Hierarchical Task Network \nA Simple Task Network (STN) method is a 4-tuple of its name, \ntask, precondition and a task network. The name of the method \nlets us refer unambiguously to substitution instances of the \nmethod, without having to write the preconditions and effects \nexplicitly. The task tells what kind of task can be applied if the \npreconditions are met. The preconditions specify the conditions \nthat the current state needs to satisfy in order for the method to be \napplied. And the network defines the specific subtasks to \naccomplish in order to accomplish the task. \nA method is relevant for a task if the current state satisfies the \npreconditions of a method that implements that task. This task can \nbe then substituted with the instance of the method. The \nsubstitution is basically giving the method network as a solution \nfor the task. \nIf there is a task \u201cGo home\u201d and the distance to home is 3km \n[Figure 2] and there exists a method walk-to and this method has a \nprecondition that the distance is less than 5km, then a substation \nto the task \u201cGo home\u201d can be made with this method instance.  \nFigure 2: HTN Method \nBuy milk\nGo to (shop)\nPurchase \nGo to (home)\nGo-to (from, to)\nWalk (to)\nIf (to \u2013 from) < 5km \n296\nDIMEA 2008\n3rd International Conference on Digital Interactive Media in Entertainment and Arts\n",
    "If the distance is larger than 5km another meth\nto be substituted [Figure 3]. \nFigure 3: HTN Method 2 \nAn STN planning domain is a set of operatio\nmethods M. A STN planning problem is a 4-tu\nstate S0, the task network w called initial task\nSTN domain. A plan \ufffd \ufffd \ufffd\ufffd\ufffd\ufffd \ufffd \ufffd \ufffd\ufffd\ufffd is a soluti\nproblem if there is a way to decompose w into \u03c0\nand each decomposition is applicable in the ap\nthe world. The algorithm that is capable to \nnetworks into plans is called Total-forward-deco\n[9] or Partial-forward-decomposition (PFD). H\ncases where one does not want to use a forwa\nprocedure. HTN planning is generalization of S\ngives the planning procedure more freedom\nconstruct the task networks.  \nIn order to provide this freedom, a bookke\nis needed to represent constraints that the plann\nnot yet enforced. The bookkeeping is done by\nunenforced constraints explicitly in the task netw\nThe HTN generalizes the definition of a\nSTN. A task network is the pair \ufffd \ufffd \ufffd\ufffd\ufffd \ufffd\ufffd w\ntask nodes and C is a set of constraints. Eac\nspecifies a requirement that must be satisfied by\na solution to a planning problem.  \nThe definition of a method in HTN also\ndefinition used in STN planning. A HTN pla\nname, task, subtasks, and constraints. The s\nconstraints form the task network. The HTN plan\nidentical to STN planning domains except they u\ninstead of STN methods. \nCompared to classical planners the prim\nHTN planners is their sophisticated knowledge r\nreasoning capabilities. They can represent and \nnon-classical planning problems; with a good\nguide them, they can solve classical planning p\nmagnitude more quickly than classical or neoc\nThe primary disadvantage of HTN is the nee\nauthor to write not only a set of planning opera\nof methods. \n3.2.2 HTN Planning in building Game \nFor a HTN planning algorithm to be adap\ntrees we need to define the domain (set of H\noperators) which is the domain of the game. Thi\na knowledge representation of the rules of the\nenvironments and possible strategies of game pla\nIn this domain the game rules as well as kn\ntackle specific task are defined.   The implem\nTree building with HTN is called Tign\nimplementation \nuses \na \nprocedure \nsimila\ndecomposition, but adapted to build up a game \nDrive(to\nIf(t\nGo-to (from, to) \nIf(to \u2013 from) < 5km \nWalk (to) \nhod instance needs \nons O and a set of \nuple of the initial \nk network and the \nion for a planning \n\u03c0 if \u03c0 is executable \nppropriate state of \ndecompose these \nomposition (TFD) \nHowever there are \nard-decomposition \nSTN planning that \nm about how to \neeping mechanism \nning algorithm has \ny representing the \nwork. \na task network in \nwhere \ufffd is a set of \nh constraint in C \ny every plan that is \no generalizes the \nan is a 4-tuple of \nsubtasks and the \nnning domains are \nuse HTN methods \nmary advantage of \nrepresentation and \nsolve a variety of \nd set of HTNs to \nproblems orders of \nclassical planners. \ned of the domain \nators but also a set \nTrees\nted to build game \nHTN methods and \nis is in some sense \ne game, the game \nay.\nnown strategies to \nmentation of Game \nnum2 [9]. This \nar \nto \nforward-\ntree rather than a \nplan. The branches of the game tree rep\nthe methods. Tignum2 applies all met\nstate of the world to produce new\ncontinues recursively until there are n\nhave not already been applied to th\nworld.  \nIn the task network generated by Tignu\nactions will occur is determined by th\nBy listing the actions in the order \nnetwork can be \u201cserialized\u201d into a gam\n4. Case Based Reasoning in\n4.1 Case Based Reasoning\nCase-based reasoning (CBR) is a \nArtificial Intelligence (AI), both as \nproblems and as a basis for standalone \nCase-based reasoning is a paradigm\nsolving and learning that has became \napplied subfield of AI of recent yea\nintuition that problems tend to recur. I\nare often similar to previously en\ntherefore, that past solutions may be of\n[10].  \nCBR is particularly applicable to probl\navailable, even when the domain is n\nfor a deep domain model. Helpdesks,\nsystems have been the most successfu\nto determine a fault or diagnostic \nattributes, or to determine whether or\nrepair is necessary given a set of past s\nFigure 5: Game Tree built fr\nFigure 4: HTN to Game Tr\n)\nto \u2013 from) < 200km \npresent moves generated by \nthods applicable to a given \nw states of the world and \nno applicable methods that \nhe appropriate state of the \num2, the order in which the \ne total-ordering constraints. \nthey will occur, the task \nme tree [Figure 4] [Figure 5]. \nn Game Strategies\nwell established subfield of \na mean for addressing AI \nAI technology.\nm for combining problem-\none of the most successful \nars. CBR is based on the \nIt means that new problems \nncountered problems and, \nf use in the current situation \nlems where earlier cases are \nnot understood well enough \n, diagnosis or classification \nul areas of application, e.g., \nan illness from observed \nr not a certain treatment or \nolved cases [11]. \nrom HTN\nree Algorithm\nInteractive and Adaptable Media\n297\n3rd International Conference on Digital Interactive Media in Entertainment and Arts\n",
    "Central tasks that all CBR methods have to deal with are [12]: \"to \nidentify the current problem situation, find a past case similar to \nthe new one, use that case to suggest a solution to the current \nproblem, evaluate the proposed solution, and update the system by \nlearning from this experience. How this is done, what part of the \nprocess that is focused, what type of problems that drives the \nmethods, etc. varies considerably, however\".  \nWhile the underlying ideas of CBR can be applied \nconsistently \nacross \napplication \ndomains, \nthe \nspecific \nimplementation of the CBR methods \u2013in particular retrieval and \nsimilarity functions\u2013 is highly customized to the application at \nhand. \n4.2 CBR and Games \nMany different implementations of CBR exist in games. \nCBR technology is nicely suited for recognizing complex \nsituations much easier and more elegant than traditional parameter \ncomparison or function evaluation. There are especially evident \ncases in real time strategies where different attack and defense of \nglobal strategies are nicely defined by CBR datasets and later used \nin the running games. Also intelligent bots behavior is also \nanother typical example. Depending on the number of enemy bots \nthe layout of the terrain and position of human players the CBR \nsystem finds the closest CBR case and employs that strategy \nagainst the human players which in prior evaluation was proved to \nbe highly efficient. \n5. Game Trees with AI Planning \u2013 Tic-tac-toe \nIn order to show the expressive power of AI Planning in \ndefining strategies for games, and the use of these plans to build \nGame Trees I implemented an algorithm that builds Game Trees \nfor the Tic-Tac-Toe game. \nThe game tree of Tic-Tac-Toe shows 255,168 possible \ngames of which 131,184 are won by X (the first player), 77904 \nare won by O and the rest 46,080 are draw [13]. All these games \ncan be derived from building a complete Game Tree.  \nEven though it is possible to build a complete game tree of \nTic-tac-toe it is definitely not an optimal solution. Many of the \nmoves in this tree would be symmetrical and also there are a many \nmoves that would be illogical or at least a bad strategy to even \nconsider.  \nSo what strategy should X (the first player) choose in order \nto win the game? \nThere are few positions that lead to certain victory. These \npositions involve simultaneous attack on two positions so the \nother player could not defend, basically the only trick in Tic-Tac-\nToe. \nFigure 6: Tic-tac-toe winning strategy positions \nPosition 1 leads to victory if the two of the three fields: top \nmiddle, bottom left corner and bottom right corner are free \n[Figure 6]. \nPosition 2 lead to victory if two of the three fields: top right \ncorner, bottom right corner and bottom middle are free [Figure ]. \nAnd in the third position if the two of center, middle top and \nmiddle left are available the position is a certain victory. \nThere are many different arrangements of the player\u2019s tokens \nthat give equivalent positions as these three positions. By using \nplanning we do not need to consider all possible layouts but just \nconsider these three similar to what a human would consider. \n The game starts from an empty table. \nThe two relevant strategies that would lead to these positions \nare to take one corner or to take the center [Figure 7]. \nFigure 7: Tic-tac-toe Two starting moves \nThe center position as we can see in the simulation results \nlead to a bigger number of victorious endings but it is also a \nstraight forward strategy with obvious defense strategy. \nAt this point we need to consider the moves of the opponent. \nIf we take the left branch the opponent moves can be a center, a \ncorner or a middle field. We also need to differentiate with a \nmove to a corner adjacent with our like top left or bottom right or \nacross the center to bottom right [Figure 8]. \nFigure 8: Tic-tac-toe opponent response to corner move \nIn cases one and two, we have a clear path to executing \nstrategy 3 so we need to capture the diagonally opposite field. \nAnd as for the third case the best way to go is to capture the center \nand go for strategy 1 or 2 depending of the opponent\u2019s next move.  \nFigure 9: Tic-tac-toe move 2 after corner opening \nThe first move leads to certain victory, O will have to go to \nthe center and X will achieve strategy 3 [Figure 9]. The second \nmove is a possible way to strategy 3 if O makes a mistake in the \nnext loop, so X goes to the opposite corner. For the third case \nsince O is playing a valid strategy the only move that leaves a \npossible mistake from O would be to take the center and wait for \nO to go to the middle and then achieve strategy 1 or 3 which will \nbe a symmetric situation to the one that we will find if we \nbranched with the center. \nFigure 10: Tic-tac-toe opponent response to center move \nIf we go back to the second branch [Figure 10], a possible \nway for the second player to engage is corner or middle. The first \n298\nDIMEA 2008\n3rd International Conference on Digital Interactive Media in Entertainment and Arts\n",
    "move is a valid strategy for O and can be mee\ncorner move from X to try a mistake from O in \nthe same as in the third case above from the pre\nanother move would be go to the middle wh\nachieves strategy 1 or 2.  \nFigure 11: Tic-tac-toe Move 2 after cent\nThe fist move will lead to win if O moves \ndraw if it goes for the corners [Figure 11]. In t\nhas to block the lower left corner which leave\nmiddle left or corner left which are strategy 1 and\nTo sum the strategies for the planning, first \ncorner strategy for the beginning. Then for the ce\nthe corners with the particularly the one oppo\nholds. If the center is empty for the second strate\nwe go for the opposite corner. After this point w\nopponent or try to implement strategies 1, 2 or\nvictory.  \nPlan 1: Take center  \nPreconditions: Center empty \nPlan 2: Take corner  \nPreconditions: All corners empty \nPlan 3: Take corner after center \nPreconditions: We have center take corner oppos\nopponent has \nPlan 4: Take diagonal corner \nPreconditions: We have a corner, the opponent ha\n the corner opposite to the one we have is free. \nPlan 5: Block \nPrecondition: The opponent has tree tokens in a r\nagonal \nPlan 6: Win \nPreconditions: We have two tokens in a row, colu\nnd the third place is free \nPlan 7: Tie \nPreconditions: If all places are taken, it\u2019s a tie. \n5.1 Hierarchical Task Network \nTop level task is Play [Figure 12]. This is a \ncan be derived into: Win, Block, Tie or Sear\nSearch for plan is derived to both Plan 1 and Pla\nPlan 4, which later leads to a call for the oppon\nrecursive call to Play. \nFigure 12: Tic-tac-toe HT\net with a opposite \nthe future exactly \nevious branch, and \nhere X eventually \nter opening\nto the middle or a \nthe second case O \nes X to go for the \nd 2.\nwe have center or \nenter we try to get \nosite to the one O \negy we go for it or \nwe either block the \nr 3 which lead to \nsite to the  one the \nas the ce\u2212nter and\nrow, colu\u2212mn or di\nmn or dia\u2212gonal a\na complex task and \nrch for Plan. The \nan 2 or Plan 3 and \nnent\u2019s move and a \nTN\nThis HTN when executed will re\ngame scenarios. By creating nodes from\nthem with branches with the move of t\ntree for the Tic-tac-toe game over whi\nalgorithm. \nThis set up with 7 plans with 3 ta\nfor Tic-tac-toe which considers all pos\nplayer with only 457 games, 281 of w\nand 0 where the second opponent w\nreduction over the 255, 168 possible g\ntree. These reductions can be very use\ncomputing capabilities but also we pr\nthat planning can be very efficient if d\ntrees by applying reasoning very \nreasoning. \nFurther improvements to the gam\nthe opponents moves are also planned\nall the meaningless and symmetrical m\n6. Game AI in Monopoly \n6.1 Overview of the AI Imp\nThe AI agent is responsible for \nplayers in the game. The core principle\na Game Tree with all the sensible move\nmake from the current point of time\nminimax algorithm the agent selects t\nwould bring the computer player mo\nwith the highest probability. Building \nthat would be big enough to consider \nis obstructed by the vastness of poss\nwith all the possible random landings \nnodes of the game tree exponentially\ntackle this problem the AI agents \ndiscussed technologies: Case Based Re\nThe technologies are employed \nFirst the agent searches the CBR datab\nlargest similarity with the current state\nassociated with a playing strategy. Th\nthat the planner needs to build plans f\nconsecutive player moves that bring th\nway only moves that are part of that str\nbeing a small fraction of the overall po\nedges of the game tree at each level dec\nAt each level of the game tree the\nof a single player. After the strateg\nconsidered the response to those strate\nby the opponent(s). The move of the \nprobability distribution of the dice as \nplayer. A more general strategy needs\nopponent\u2019s (human player) moves sin\nthe expertise of the opponent. This ge\nmore plausible moves than the focused\nAfter covering all opponents t\ndeducting a feature move of the com\nCBR selected plan strategy. After \nstrategies and reaching a reasonable s\ninto account the memory limits an\nprobabilities that the move is possible\nthe dice the building of the Game Tre\nalgorithm searches the Game Tree \nfavorable move for the AI player usi\nThe process is repeated each time the A\nesult with plans for possible \nm each position and linking \nthe player we create a game \nich we can run the minimax \narget strategies creates a tree \nssible moves for the second \nwhich X wins 176 are draw \nwins. This is a significant \names with a complete game \neful for devices with limited \nrove a very important point \ndesigning meaningful game \nsimilar to human player \nme tree are also possible if \nd, in other words if we drop \nmoves of the opponent. \nplementation\nthe moves of the artificial \ne of the AI agent is building \nes that all the players would \ne forward. Then using the \nthe move that in the future \nost favorable game position \na Game Tree in this game \nsufficient number of moves \nsible moves in combination \nof the dice. The number of \ny grows at each level. To \nincorporates two already \neasoning and AI Planning. \nin the following manner. \nbase to find the case with the \ne of the board. This case is \nhe strategy consists of goal \nfor, and the plans consist of \nhe player to that goal. This \nrategy are considered, those \nossible moves the number of \ncreases immensely. \ne model considers the moves \ngies of the AI player are \negies needs to be considered \nopponent(s) depends of the \nwell as the strategy of the \ns to be implemented for the \nnce we cannot be aware of \neneral strategy would bring \nd strategy of the AI player.  \nthe agent comes back to \nmputer player by using the \ncreating several loops of \nsize of a Game Tree taking \nnd the rapidly decreasing \ne due to the distribution of \nee stops. Then the minimax \nand decides on the most \ning the minimax algorithm. \nAI player is up. \nInteractive and Adaptable Media\n299\n3rd International Conference on Digital Interactive Media in Entertainment and Arts\n",
    "Buying, auctioning and trading game moves are always \naccompanied by return of investment calculations in making the \nplans. These calculations represent adaptation of the more general \nplanning associated with the cases in the CBR database. These \nadaptations are necessary due to the fact that the cases do not \nidentically correspond to the situation on the table. In addition \ncalculating the game position value of each node of the game tree \nis done by heuristic functions that incorporate economic \ncalculations of net present value, cash, and strategic layout and so \non. For example railroads in monopoly are known to be \nstrategically effective because they bring constant income even \nthough the income can be smaller than building on other \nproperties.  \n6.2 Details on the CBR Implementation \nThe implementation of the CBR is by using the JColibri2 \nplatform.  JColibri2 is an object-oriented framework in Java for \nbuilding CBR systems that is an evolution of previous work on \nknowledge intensive CBR [14].  \nFor this implementation we need to look into three particular \nclasses of the JColibri2 platform. The StandardCBRApplication, \nConnector, CBRQuery. For a JColibri2 implementation the \nStandardCBRApplication interface needs to be implemented.  \nThe CBR cycle executed accepts an instance of CBRQuery. \nThis class represents a CBR query to the CBR database. The \ndescription component (instance of CaseComponent) represents \nthe description of the case that will be looked up in the database. \nAll \ncases \nand \ncase \nsolutions \nare \nimplementing \nthe \nCaseComponent interface. \nThe JColibri2 platform connects to the CBR database via a \nConnector class. Each connector implements all the necessary \nmethods for accessing the database, retrieval of cases, storing and \ndeletion of cases. This implementation uses a custom XML \nstructure for holding the CBR cases. Since the game will not \nupdate the CBR database only read it, a XML solution satisfies \nthe needs. The XML file to a certain extent is similar to the XML \nrepresentation of the board. We are interested in finding one \nCBRCase that is the most similar case to the situation in the game \nat the time of the search. This procedure is done in the cycle \nmethod of the CBRApplication. The JColibri2 CBR comparison is \ndone by Nearest Neighbor (NN) search method.  \nJColibri2 offers implementations for NN search algorithms \nof simple attributes. These implementations are called local \nsimilarities. For complex attributes like in our case global \ncustomized similarity mechanisms need to be implemented. \nThe MonopolyDescription class [Figure 13] is basically a \nserialization of the GameState. It holds all the information about \nthe state of the board, the players, their amount of cash etc.  \nFigure 13: Class diagram of the Monopoly Case component \nmodels \nOn the other hand the MonopolySolution class holds the \nthree particular attributes that are needed for the planning, the \nplanning Domain, State and TaskList. \nThe game is implemented by using the Model-View-\nController software development pattern. The controller is \nresponsible for implementing the game rules and handling all of \nthe events in the game like roll of dice, input commands for \ntrading, auctioning and etc from the players. The View layer is \nresponsible for displaying the board and all of the input widgets \non to the game screen, and the models are data structures \nrepresenting the game state [Figure 14]. \nFigure 14: Class diagram of the Monopoly models \n6.2.1 Complex Similarity representation in CBR \nThe similarity measurement part of the Nearest Neighbor \nalgorithm JColibri2 is implemented by implementing the \nLocalSimiralrityFunction \nand \nthe \nGlobalSimiralityFunction \ninterface. A local similarity function is applied to simple attributes \nby the NN algorithm, and a global similarity function is applied to \ncompound attributes. In the case of our implementation the \nattributes of the MonopolyDescription are compound attributes \ndescribing the state of the board, number of players, amount of \ncash for every player and etc. Since MonopolyDescription is a \ncustom CaseComponent a global similarity function needs to be \nimplemented to accurately find the distance between different \nCBR cases. \nThe similarity mechanism is inseparable core element of the \nCBR system. This mechanism represents how the CBR decides \nwhich strategy is best suited for the particular situation by \n300\nDIMEA 2008\n3rd International Conference on Digital Interactive Media in Entertainment and Arts\n",
    "calculating the distance or similarity to other cases in the \ndatabase.  \nFor the monopoly implementation we need to consider \nseveral basic strategies. Monopoly is based on investing in \nproperties and receiving revenues from those investments. One of \nthe basic strategies of the game is to build a set of properties that \nwill bring constant income larger than the one of the opponents. \nSo in time the opponents will have to declare bankruptcy. But on \nthe other hand over investment can lead to too stretched resources \nwith low income that will eventually drove the player to \nbankruptcy. To decide on these two we need a clear separation \ninto two groups of cases in the CBR database. The first group of \ncases will represent a situation on the board where the player has \nsignificant income per loop formed of one or more color group \nproperties, maybe railroads, some buildings on them and so on. It \nis important to note that in this case the player is better situated \nthan his opponents so he only needs to survive long enough to win \nthe game. In the other group of cases either the opponent is not \nwell positioned on the board or its opponents are better situated. \nIn this case further investments are necessary to improve the \nsituation so the player can have a chance of winning in the long \nrun.  \nThese metrics can be owning color groups, valuing groups of \nrailroads, evaluating the other opponents as well, and considering \nthe amount of cash. As it is obvious in monopoly the number of \nstreets is not as nearly as important as the combination of streets \nthe player owns. It is also important to note that one CBR case \ndoes not hold only a single strategy in place, but its solution can \nhave multiple different strategic goals. For example one CBR case \nmight simultaneously say buy this land to form a color group but \nalso trade some other unimportant property to increase cash \namount.  \nThe cases do not represent all possible combinations of board \npositions. They are only representation of typical game scenarios. \nThe CBR Case solutions do not give exact instructions in general \nbut rather strategic goals. For example one CBR Solution might \nsay trade the streets that you only have one of each for the ones \nthat you have two of that color already. Then the planner based on \nthe situation on the board needs to decompose this high level task \nto a low level operations. Like offer \"Mediterranean Avenue\" for \n\"Reading Railroad\" and offer $50. The exact amounts and actual \nstreets are left to the planer to evaluate.  \nThe monopoly CBR database is currently in development on \na monopoly clone game called Spaceopoly. The cases are \narchitected based on human player experience and knowledge. \nThere is a plan of making a number of slightly different strategies \nthat differ on the style of playing and then running simulation \ntests that would determine the particular validity of each database \nas well as validity of certain segments of the strategy or even \nparticular cases in the database.  \nThe actual execution of the strategies will not differ from \nstrategy to strategy since the plan execution is more related to the \nstructure and rules of the game than to the actual playing strategy. \n6.3 Details on the Planning Implementation \nFor the purpose of planning this implementation uses a \nmodification of the JSHOP2 planner. The Java Simple \nHierarchical Ordered Planner 2 is a domain independent HTN \nplanning system [15].  \nJSHOP2 uses ordered task decomposition in reducing the \nHTN to list of primitive tasks which form the plans. An ordered \ntask decomposition planner is an HTN planner that plans for tasks \nin the same order that they will be executed. This reduces the \ncomplexity of reasoning by removing a great deal of uncertainty \nabout the world, which makes it easy to incorporate substantial \nexpressive power into the planning algorithm. In addition to the \nusual HTN methods and operators, the planners can make use of \naxioms, can do mixed symbolic/numeric conditions, and can do \nexternal function calls. \n In order for the JSHOP2 planer to generate plans it needs \ntree crucial components: Domain, State and Tasks. The Domain \ndefines all the functionalities that the particular domain offers. \nThese are simple and complex tasks. The complex tasks also \ncalled methods create the hierarchy with the fact that they can be \nevaluated by simple tasks of other complex tasks. This is how a \nhierarchical structure of tasks is formed. The problem reduction is \ndone by reducing the high level complex tasks to simpler until all \nthe tasks are primitive. The list of primitive tasks forms the plan. \nThe State represents the state of the system. It is a simple \ndatabase of facts that represent the state of the system. The State \nis necessary to determine the way the problems or tasks are \nreduced to their primitive level. The reduction is done by \nsatisfying different prerequisites set in the methods; these \nprerequisites are defined in the state. The Tasks are high level \ntasks or methods defined in the Domain. The planner based on the \nState and the goals selects one or more high level tasks that need \nto be reduced to plans [Figure  15]. \nFigure 15: Diagram of a Planner \nThe plans then generate the game moves. The number of \nmoves generated by the plans is just a fraction of the possible \nmoves at that point. This reduces the game tree providing the \nopportunity to generate smaller and deeper game trees and making \nmore efficient decisions in general.  \n7. Conclusion \nEven though the results from the CBR database are not \ncomplete at this time partial strategies are implemented as cases \nand recognized during game play by the CBR system. These \nsmaller local strategies coupled with more global higher level \nstrategies that are particularly important at the beginning of the \ngame would form a complete CBR database and represent a \nknowledge engineered style of playing of the AI player.  \nThe AI Planning approach is a proven method by the tic-tac-\ntoe experiment and is suitable for implementing the strategies \nassociated with the CBR cases. \nThis approach in general benefits from both technologies, \nCBR as well as AI Planning and comprises an elegant solution. \nEven though AI Planning can be enough as a single technology \nfor some simpler problems like tic-tac-toe the complexity of \nMonopoly would mean that the Planner would have to incorporate \nCore Planner \nTasks\nPlan\nState\nInteractive and Adaptable Media\n301\n3rd International Conference on Digital Interactive Media in Entertainment and Arts\n",
    "large and complex domain and a very big state model. The CBR \napplication helps reduce this complexity by focusing the planning \non smaller domain of the game. Basically the CBR reduces the \noverall goal of the play (wining the game) to smaller more \nconcrete goals suitable to the particular state of the game, thus \nreducing the need for global planning strategies and complex \nplanning domain.  \nFurthermore this symbiosis of technologies gives way for \nmore precise and finely tuned strategies which can be difficult to \ninclude into global plan for the whole game. One simple example \nfor the Monopoly game would be this: Sometimes it\u2019s better to \nstay in jail because rolling double increases the probability of \nlanding on some field (two, four, six, eight, ten or twelve steps \nfrom the jail) that can be of great importance to the rest of the \ngame. These and similar small local strategies can be easily \nrecognized by similar cases in the CBR database.  \nIn other words the system is flexible enough so that new \nstrategies can be incorporated easily missing strategies can be also \nrecognized by the distance metrics as well as wrong assumptions \nin the strategies can be easily recognized. \nOne other important property of the system is that is highly \nconfigurable. The game its self can be diversely different \ndepending on the configuration of the board. Even though the \nplatform is restricted to Monopoly type of games, changing the \nlayout and values of the fields effectively brings completely \ndifferent properties of the game. In addition the CBR database \nrepresents the entire experience of the AI Player. It can be filled \nwith rich set of strategies or even configured with different flavors \nof difficulties of play, this of course coupled with the domain of \nthe planner which can differ from a case to a case as well.  \n8. Future Work \nFurther exploration of this technology would go towards \ncomplete implementation of an AI aware agent for monopoly. \nInitial results from the local cases with more specific strategies \nshow CBR as a capable tool for representing expertise in playing \nthe game. Completing the more general strategies and coupling \nthem with the planning domain will give precise results on the \nbenefits from this architecture. \nThere is also need for exploring the planning of strategies of \nopponents. This task is to some extent different because we \ncannot always expect the opponent to select the best move we \nthink. In the Tic-tac-toe example all possible moves of the \nopponent were taken into consideration, if we used the same \nplanner for the opponent only tie games would result from the \ngame tree. In other words mistakes of the players also need to be \nconsidered.  \nThe CBR Platform brings other functionalities well worth of \nexploring as well. The revision stage of the JColibri2 platform is \nbasically capable of fine tuning strategies or even developing new \nstrategies for the games. A well written underlying AI planning \nmodel with a capable feedback of the game tree evaluation back \nto the CBR revision capability can be an interesting concept in \nautomatic experience acquisition for the AI model. \nThere are also many other fields were combined CBR and \nplanning approach can be incorporated into a problem solution. \nThis combination is analogous in a big extent to a human way of \nreasoning. People in addition to logic of reasoning in situations \nwith lack of information rely to planning strategies and prior \nexperience, exactly the intuition behind CBR \u2013 AI Planning \narchitecture.  \n9. ACKNOWLEDGMENTS \nWe would like to thank Prof. Sofia Tsekeridou for her \ninvolvement in the valuable discussions we had on the topic of \nCBR. \n10. REFERENCES \n[1]\nMinimax. Wikipedia. [Online] [Cited: April 23, 2008.] \nhttp://en.wikipedia.org/wiki/Minimax. \n[2]\nVon Neumann, J: Zur theorie der gesellschaftsspiele Math. \nAnnalen. 100 (1928) 295-320 \n[3]\nAutomated Planning. Wikipedia. [Online] [Cited: April 23, \n2008.] http://en.wikipedia.org/wiki/Automated_planning. \n[4]\nSanchez-Ruiz, Antonio, et al. Game AI for a Turn-based \nStrategy Game with Plan Adaptation and Ontology-based \nretrieval.\n[5]\nK. Erol, J. Hendler, and D. Nau (1994). Semantics for \nhierarchical task-network planning. Technical Report TR-94-\n31, UMIACS. \n[6]\nSmith, S. J. J. and Dana S. Nau, T. A. Throp. A Planning \napproach decrarer play in contract bridge. Computational \nIntelligence. 1996, Vol. 12, 1. \n[7]\nOne Jump Ahead: Challenging Human Supremacy in \nCheckers. J.Schaeffer. s.l. : Springer-Verlag, 1997. \n[8]\nIBM. How Deep Blue works. [Online] 1997. [Cited: April \n23, 2008.] \nhttp://www.research.ibm.com/deepblue/meet/html/d.3.2.html\n[9]\nGhallab, Malik, Nau, Dana and Traverso, Paolo.\nAutomated Planning theory and practice. s.l. : Morgan \nKaufmann Publishers, May 2004. ISBN 1-55860-856-7. \n[10] Case Based Reasoning. Experiences, Lessons and Future. \nLeake, David. s.l. : AAAI Press. MIT Press., 1997. \n[11] Applying case-based reasoning: techniques for enterprise \nsystems. Watson, I. San Francisco, CA, USA : Morgan \nKaufmann Publishers Inc., 1998. \n[12] Plaza, A. Aamodt and E. Case-based reasoning: \nFoundational issues, methodological. AI Communications. \n1994, 7(i). \n[13] Tic-tac-toe. Wikipedia. [Online] [Cited: April 23, 2008.] \nhttp://en.wikipedia.org/wiki/Tic-tac-toe. \n[14] D\u00edaz-Agudo, B. and Gonz\u00e1lez-Calero, P. A. An \narchitecture for knowledge intensive CBR systems. Advances \nin Case-Based Reasoning \u2013 (EWCBR\u201900). New York : \nSpringer-Verlag, Berlin Heidelberg, 2000. \n[15] Ilghami, Okhtay and Nau, Dana S. A General Approach to \nSynthesize Problem-Specific Planners. 2003. \n302\nDIMEA 2008\n3rd International Conference on Digital Interactive Media in Entertainment and Arts\n"
]