[
    "The Programmer\u2019s Assistant: Conversational Interaction with a\nLarge Language Model for Software Development\nSteven I. Ross\nIBM Research AI\nCambridge, MA, USA\nsteven_ross@us.ibm.com\nFernando Martinez\nIBM Argentina\nLa Plata, Buenos Aires, Argentina\nmartferc@ar.ibm.com\nStephanie Houde\nIBM Research AI\nCambridge, MA, USA\nStephanie.Houde@ibm.com\nMichael Muller\nIBM Research AI\nCambridge, MA, USA\nmichael_muller@us.ibm.com\nJustin D. Weisz\nIBM Research AI\nYorktown Heights, NY, USA\njweisz@us.ibm.com\nABSTRACT\nLarge language models (LLMs) have recently been applied in soft-\nware engineering to perform tasks such as translating code between\nprogramming languages, generating code from natural language,\nand autocompleting code as it is being written. When used within\ndevelopment tools, these systems typically treat each model in-\nvocation independently from all previous invocations, and only a\nspecific limited functionality is exposed within the user interface.\nThis approach to user interaction misses an opportunity for users\nto more deeply engage with the model by having the context of\ntheir previous interactions, as well as the context of their code,\ninform the model\u2019s responses. We developed a prototype system\n\u2013 the Programmer\u2019s Assistant \u2013 in order to explore the utility of\nconversational interactions grounded in code, as well as software\nengineers\u2019 receptiveness to the idea of conversing with, rather than\ninvoking, a code-fluent LLM. Through an evaluation with 42 partici-\npants with varied levels of programming experience, we found that\nour system was capable of conducting extended, multi-turn discus-\nsions, and that it enabled additional knowledge and capabilities\nbeyond code generation to emerge from the LLM. Despite skepti-\ncal initial expectations for conversational programming assistance,\nparticipants were impressed by the breadth of the assistant\u2019s capa-\nbilities, the quality of its responses, and its potential for improving\ntheir productivity. Our work demonstrates the unique potential of\nconversational interactions with LLMs for co-creative processes\nlike software development.\nCCS CONCEPTS\n\u2022 Human-centered computing \u2192 HCI theory, concepts and\nmodels; \u2022 Software and its engineering \u2192 Designing software;\n\u2022 Computing methodologies \u2192 Generative and developmental\napproaches.\nThis work is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivs International 4.0 License.\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\n\u00a9 2023 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0106-1/23/03.\nhttps://doi.org/10.1145/3581641.3584037\nKEYWORDS\ncode-fluent large language models, foundation models, conversa-\ntional interaction, human-centered AI\nACM Reference Format:\nSteven I. Ross, Fernando Martinez, Stephanie Houde, Michael Muller, and Justin\nD. Weisz. 2023. The Programmer\u2019s Assistant: Conversational Interaction\nwith a Large Language Model for Software Development. In 28th Inter-\nnational Conference on Intelligent User Interfaces (IUI \u201923), March 27\u201331,\n2023, Sydney, NSW, Australia. ACM, New York, NY, USA, 24 pages. https:\n//doi.org/10.1145/3581641.3584037\n1\nINTRODUCTION\nSoftware development is a highly skilled task that requires knowl-\nedge, focus, and creativity [27, 28]. Many techniques have been\ndeveloped to enhance the productivity of software engineers, such\nas advanced code repositories [86], knowledge repositories [39],\nQ&A sites [1], and pair programming practices [18]. Collaborative\nsoftware engineering is especially promising, given that profes-\nsional software development is rarely a solo activity and relevant\nknowledge and expertise are typically distributed widely within an\norganization [68]. Many efforts have focused on incorporating col-\nlaborative technologies into software development environments\n(e.g. [8, 25, 26, 58, 101]).\nThe pioneering work of Rich and Waters on The Programmer\u2019s\nApprentice [70] presented a novel concept of a knowledgeable auto-\nmated assistant \u2013 in effect, an artificial collaborative partner \u2013 that\ncould help software engineers with writing code, designing soft-\nware systems, and creating requirements specifications. At the time,\nAI technologies and computing resources were not sufficient to\nfully implement their vision. In the intervening years, an increase in\ncomputational power, the availability of large corpora of language\nand code data, and the development of deep neural networks have\nmade new approaches to achieving their goals worth exploring.\nRecently, models leveraging the transformer architecture [96]\nhave been developed to perform domain-specific software engi-\nneering tasks, such as translating code between languages [75],\ngenerating documentation for code [36, 38, 97, 98], and generating\nunit tests for code [92] (see Talamadupula [90] and Allamanis et al.\n[5] for surveys). Recently developed foundation models \u2013 large\nlanguage models that can be adapted to multiple tasks and which\nexhibit emergent behaviors for which they have not been explic-\nitly trained [14] \u2013 have also proven to be capable with source code.\n491\n",
    "IUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nRoss et al.\nWhile the intent of training LLMs such as GPT-2 [64] and GPT-3 [17]\nwas to give them mastery of natural language, it quickly became\napparent that the presence of code in their training corpora had\ngiven them the ability to generate code based on natural language\ndescriptions [49]. The Codex model [24] was then produced by fine-\ntuning GPT-3 on a large corpus of source code data, leading to the\ndevelopment of Copilot [32], a tool that helps software engineers by\nautocompleting code as it is being written. Experimentation with\nCopilot has shown its ability to perform additional tasks, such as\nexplaining code, generating documentation, and translating code\nbetween languages [6].\nAlthough autocompletion interfaces are useful and valuable\nwhen the system can discern the developer\u2019s intent, there are many\ninstances where that is insufficient. For example, the developer\nmay have a good idea of what they want to do, but may be unclear\non what functions, libraries, or even algorithms to employ. They\nmay even have general programming questions that need to be\nanswered before they are able to write any code.\nIn this paper, we seek to understand whether modern develop-\nments in code-fluent foundation models \u2013 large language models\nthat have been fine-tuned on source code data \u2013 are sufficient to\nsupport a conversational agent that can act as an assistant in the\nsoftware development process. We developed the Programmer\u2019s\nAssistant to explore the capabilities that conversational interaction\ncould enable and the extent to which users would find conversa-\ntional assistance with programming tasks desirable and useful.\nWe hypothesize that a conversational system may provide a\nflexible and natural means for interacting with a code-fluent LLM.\nConversational interaction could enable users to pursue their ques-\ntions in a multiple exchange dialog (as observed by Barke et al.\n[13]) that allows them to ask follow-up questions and refine their\ninquiries. A conversational programming assistant could ask the\nuser clarifying or disambiguating questions to help it arrive at the\nbest answer. It could also provide multiple types of assistance to\nthe user beyond simply generating code snippets, such as engaging\nin general discussion of programming topics (e.g. [22, 71]) or help-\ning users improve their programming skills (as observed in other\nstudies of automating technologies [99]).\nOur paper makes the following contributions to the IUI commu-\nnity:\n\u2022 We provide empirical evidence that a conversational pro-\ngramming assistant based on a state-of-the-art, code-fluent\nfoundation model provides valuable assistance to software\nengineers in a myriad of ways: by answering general pro-\ngramming questions, by generating context-relevant code,\nby enabling the model to exhibit emergent behaviors, and by\nenabling users to ask follow-up questions that depend upon\ntheir conversational and code contexts.\n\u2022 We show how different interaction models \u2013 conversation,\ndirect manipulation, and search \u2013 provide complementary\ntypes of support to software engineers with tradeoffs be-\ntween the user\u2019s focus and attention, the relevance of sup-\nport to their code context, the provenance of that support,\nand their ability to ask follow-up questions.\n\u2022 We motivate the need to further understand how to design\nhuman-centered AI systems that enhance the joint perfor-\nmance of the human-AI collaboration.\n2\nRELATED WORK\nWe discuss three areas of related work that have either motivated\nour study of conversational programming assistance or provided\nthe technical foundations for it. We begin by briefly summarizing\nRich and Waters\u2019 visionary work on the Programmer\u2019s Appren-\ntice [70], followed by summarizing work on code-fluent foundation\nmodels and human-centered evaluations of how these models im-\npact software engineers\u2019 work. Finally, we discuss conversational\ninteraction and how it might be employed to provide more flexible\nand sophisticated assistance to software engineers.\n2.1\nThe Programmer\u2019s Apprentice\nOur work is inspired by the vision laid out by Rich and Waters [70],\nwhich describes an artificial agent that can act as an intelligent as-\nsistant for software engineers by providing advice, catching errors,\nand handling routine details throughout the software development\nprocess. The Programmer\u2019s Apprentice [70] relied on a knowledge\nbase of \u201cclich\u00e9s,\u201d which are formal, structured versions of what\nare known today as software design patterns [31]. It used a hy-\nbrid reasoning system capable of special-purpose reasoning based\non frames and a plan calculus, along with general purpose logical\nreasoning. Although natural language interaction was envisioned,\nthe original prototype implementation ultimately used a stylized\ncommand language. We view our work as a conceptual successor\nto the Programmer\u2019s Apprentice, as it enables the natural language\ninteraction that the Programmer\u2019s Apprentice lacked.\n2.2\nCode-fluent Foundation Models and\nHuman-Centered Evaluations of\nProgramming Assistance\nGenerative models based on the transformer architecture [96] have\nrecently been applied to the domain of software engineering. Code-\nfluent large language models are capable of generating code from\nnatural language descriptions [105], translating code from one lan-\nguage to another [75], generating unit tests [92], and even gener-\nating documentation for code [36, 38, 97, 98]. These models are\nprobabilistic systems, and as such, do not always produce perfect\nresults (e.g. code that is free of syntax or logical errors). Nonetheless,\nWeisz et al. [102] found that software engineers are still interested\nin using such models in their work, and that the imperfect outputs\nof these models can even help them produce higher-quality code\nvia human-AI collaboration [103].\nNew tools based on code-fluent LLMs are actively being devel-\noped. GitHub Copilot1 is described as \u201cYour AI pair programmer.\u201d It\nis optimized for the code autocompletion use case: given a starting\nsnippet such as a method\u2019s documentation, signature, or partial\nimplementation, Copilot completes the implementation. Copilot is\nbased on the OpenAI Codex model [24], a 12 billion parameter ver-\nsion of GPT-3 [17, 49], fine-tuned on code samples from 54 million\npublic software repositories on GitHub. Empirical evaluations of\n1https://github.com/features/copilot\n492\n",
    "The Programmer\u2019s Assistant\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nthis model have shown that, although the quality of its outputs is\nquite good, those outputs may still be problematic [57]. Echoing the\nresults from Weisz et al. [103], human-centered evaluations of Copi-\nlot have found that it increases users\u2019 feelings of productivity [109],\nand that almost a third (27%) of its proposed code completions were\naccepted by users. In a contrasting evaluation, Vaithilingam et al.\n[95] found that while most participants expressed a preference\nto use Copilot in their daily work, it did not necessarily improve\ntheir task completion times or success rates. Yet, in a study by\nKalliamvakou [40], developers working with Copilot were able to\nimplement a web server in Javascript 55% faster than developers\nwho did not use Copilot.\nA grounded theory analysis of how programmers interact with\nCopilot [13] found that their interactions varied depending upon\nwhether they were accelerating tasks that they already knew how\nto do or if they were exploring solutions to problems that they were\nless sure about. Autocompletion was effective when developers\nwere operating in \u201cacceleration mode\u201d and relied on the model\nto produce short completions that could be verified quickly. In\n\u201cexploration mode,\u201d however, the interaction was more awkward.\nDevelopers would communicate with Copilot by typing comments\nand seeing what Copilot generated in response. Then, they would\nmodify their comments to explore other ways of prompting a re-\nsponse. Ultimately, the comments used to prompt the model would\nbe deleted after the relevant code was generated, indicating that\ntheir value was largely in driving a back-and-forth, yet context free,\ndialog with the model to coerce it to produce the desired results\nthrough an iterative refinement process. In this paper, we fully\ncommit to a context-aware conversational style of interaction with\na code-fluent LLM and assess the value it provides to users.\n2.3\nConversational Interaction and Analysis\n2.3.1\nConversational Interaction. Using natural language to inter-\nact with technology has had a long research history [2], starting in\nthe 1960s with pattern-matching approaches like Eliza [104], and\ncontinuing to today with state-of-the-art large language model-\nbased conversational systems [107] such as Meena [3] and Blender-\nBot [84]. These systems are intended to address the problem of\nopen-domain dialog, with a goal of realistically engaging in con-\nversation, but not particularly in a goal-directed or task-oriented\nmanner.\nTask-oriented chatbots are typically built with frameworks such\nas the Microsoft Bot Framework2, Google DialogFlow3, and IBM\nWatson Assistant4. They operate using pre-defined dialogue trees\nand use natural language processing to detect conversational intents\nand extract contextual entities. This structure enables the creation of\nspecial purpose, but fairly limited and rigid, conversational agents.\nThere have been several recent attempts to investigate conver-\nsational programming assistance. Kuttal et al. [42] conducted a\nWizard of Oz study in which a pair programmer was replaced with\na conversational agent, and they found that \u201cagents can act as ef-\nfective pair programming partners.\u201d The PACT system [106] is a\nchatbot that assists programmers adjusting to new programming\n2https://dev.botframework.com/\n3https://cloud.google.com/dialogflow\n4https://www.ibm.com/products/watson-assistant/artificial-intelligence\nenvironments. PACT is structured as a discrete question-answering\nsystem based on a neural machine translation approach, but it\ndoesn\u2019t maintain a conversational context.\n2.3.2\nConversation Analysis. Conversation is a form of interaction\nbetween people that enables robust communication. Conversation\nAnalysis [76] is a method for understanding the natural structure\nof human conversational interaction. It catalogs different patterns\nof conversational acts and how they are utilized by interlocutors in\norder to attain a wide variety of goals. Recently, Conversation Anal-\nysis has been adapted to describe patterns of interactions between\nhumans and artificial conversational agents in order to aid in the\ndesign of chatbots [50]. We apply techniques from Conversation\nAnalysis in our study of conversational programming assistance.\n3\nTHE PROGRAMMER\u2019S ASSISTANT\nIn order to explore conversational programming assistance, we\ncreated a functional prototype system called The Programmer\u2019s\nAssistant. Our prototype, shown in Figure 1, combines a code editor\nwith a chat interface. The code editor was implemented using the\nMicrosoft Monaco Editor5 embedded in a React wrapper6. The\nchat user interface was implemented using the React-Chatbot-Kit7\nframework. To drive the conversational interaction, we employed\nOpenAI\u2019s Codex model [24], accessed through its web API.\nWe developed our prototype as a lightweight coding environ-\nment in order to examine the user experience of interacting with\na conversational assistant. Our work was exploratory in nature,\nand thus we did not have specific design goals for the prototype\nbeyond integrating a code editor with a code-fluent LLM. We also\ndid not attempt to target the prototype for a specific class of users\n(e.g. novices or experts) or use cases (e.g. writing code vs. learning a\nnew programming language), as we wanted any value provided by\nconversational assistance to emerge from our user study. We also\ndid not implement the ability to run or debug code in our prototype\nas we wanted to explore the nature of the conversational interac-\ntion rather than having users focus extensively on the production\nof working code.\nWhen designing how users would interact with the Program-\nmer\u2019s Assistant, we decided that it should be available on demand\nand not monitor the user\u2019s work in progress or give unsolicited\nsuggestions or advice, in keeping with the conversational agent\ninteraction model proposed by Ross et al. [73, 74]. This approach\nwas supported by feedback from prospective users who were con-\ncerned about the assistant providing criticism of unfinished efforts\nin progress or distracting them while they worked. Instead, we\nforce initiative onto the user and only have the assistant respond\nto their requests. In this way, the assistant can provide help when\nrequested without undesirable interruptions that can distract or\ninterfere with the user\u2019s flow.\nWhen a user interacts with the assistant, we keep track of their\nselection state in the code editor. If a user sends a message to the\nassistant without any code selected in the editor, then that message\n(along with the prior conversational context) is passed directly to\nthe model. If a user sends a message to the assistant with new code\n5https://microsoft.github.io/monaco-editor/\n6https://www.npmjs.com/package/@monaco-editor/react\n7https://fredrikoseberg.github.io/react-chatbot-kit-docs/\n493\n",
    "IUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nRoss et al.\nselected in the editor (i.e. code that wasn\u2019t previously selected when\nthey sent their last message), then that code is appended to the\nmessage before being communicated to the model.\nThe model may produce multiple types of responses to a user\u2019s\nmessage. We treat each type of response differently in the UI.\n\u2022 Responses that do not contain code are always rendered in\nthe chat UI (Figure 1E).\n\u2022 Responses containing short code snippets (\u2264 10 lines) are\nrendered inline in the chat UI (Figure 1G).\n\u2022 Responses containing longer code snippets (> 10 lines) show\nthe code in a pop-up window (Figure 2A), with a proxy entry\nin the chat transcript (Figure 2B) that allows users to re-\ndisplay the code window after it has been closed. Non-code\ntext in the response remains in the chat transcript.\nThe assistant never directly modifies the contents of the user\u2019s\nsource code; rather, any code the user desires to transfer from the\nchat takes place via copy/paste.\nFigure 1 shows a screenshot of a real, sample conversation, in\nwhich the user asks a question that results in an inline response,\nthen requests an explanation of some code in the editor, and then re-\nquests further elaboration. Figure 2 shows an example conversation\nthat resulted in the generation of a longer code sample, shown in a\npopup window. This example shows how the assistant produced an\nincomplete solution, followed by criticism from the user regarding\nthe missing code, and resulting in an apology and the generation\nof a complete solution.\n3.1\nSupporting Conversational Interaction\nWe enabled Codex to conduct a conversational interaction by prompt-\ning it with a conversational transcript and a request to produce\nthe next conversational turn. The prompt establishes a pattern of\nconversation between a user and a programming assistant named\nSocrates. It provides several examples of Socrates responding to gen-\neral coding questions, generating code in response to a request, and\naccepting code as input. It establishes a convention for delimiting\ncode in the conversation, making it easy to parse for display in the\nUI. It also establishes an interaction style for the assistant, directing\nit to be polite, eager, helpful, and humble, and to present its re-\nsponses in a non-authoritative manner8. Because of the possibility\nthat the model might produce erroneous answers or incorrect code\n(as discussed in Weisz et al. [102]), we felt it was important that the\nassistant convey a sense of uncertainty to encourage users to not\naccept its results uncritically to avoid over-reliance (e.g. as observed\nin Moroz et al.\u2019s study of Copilot [51], and discussed more gener-\nally in Ashktorab et al. [9]) as well as automation bias [45, 46, 65].\nWe present the full text of the prompt used for the assistant in\nAppendix D.\n3.2\nArchitecture & UI Design\nThe Programmer\u2019s Assistant communicates with the Codex API via\na proxy server that forwards requests from the React client. The\nproxy also rate-limits access to conform to the API\u2019s policy, and\nit logs UI events from the client (e.g. requests, responses, and UI\n8The assistant\u2019s use of non-authoritative responses was encoded into the LLM prompt;\noutput token probabilities from the LLM were not utilized to influence the assistant\u2019s\nresponse.\ninteractions) in a back-end database. To address inconsistencies\nin the style or formatting of code generated by Codex, the proxy\nserver reformats all code segments using the Black code formatter\n9 before transmitting them to the client UI.\nThe client maintains the transcript of the ongoing conversation.\nEach time the user sends a message in the chat, the client constructs\na new prompt for the model by concatenating the initial prompt,\nthe chat transcript, and the user\u2019s new utterance, and makes a re-\nquest for the model to complete the transcript. This completion\nrequest also specifies a stop sequence of tokens to prevent the\nmodel from generating both sides of the conversation (e.g. what\nthe model thinks the user\u2019s next utterance might be after the as-\nsistant\u2019s response). Given the API\u2019s limitation on context length\n(4,096 tokens for both the prompt and model response), we silently\n\u201cforget\u201d older exchanges in the chat transcript when constructing\nthe prompt to ensure that our completion request remains within\nbounds. Nonetheless, the entire conversational history remains\nvisible to the user in the UI.\nThe client UI provides a loose coupling between the source code\neditor and the chat interface. Users can hide the chat pane when\nthey wish to focus solely on their code, and re-engage with it\nwhen they desire assistance. Code selected in the editor is included\nin the conversation in order to couple the code context with the\nconversation. Easily-accessible buttons are provided in the UI to\ncopy code responses from the assistant to the clipboard.\n3.3\nHandling Model Limitations\nWhile developing the Programmer\u2019s Assistant, and in early pilot\ntesting, we experienced some quirks and shortcomings of the model\nand our approach to using it for conversational interaction. One\nlimitation stemmed from the fact that the model sometimes pro-\nduced incorrect responses (e.g. code with syntax errors), incomplete\nresponses (e.g. code that was missing functionality), irrelevant re-\nsponses (e.g. responses not related to the user\u2019s question), or insub-\nstantial responses (e.g. \u201cI don\u2019t know\u201d). Because of the probabilistic\nnature of model inference, re-prompting the model would some-\ntimes produce a more correct or appropriate response. Thus, we\nadded the ability for users to \u201ctry again,\u201d either by asking in the chat\nor by clicking a button in the UI (Figure 1C). This feature removes\nthe assistant\u2019s last response from the context presented to the model\nand then re-invokes the model with an increased temperature10.\nAlthough it is possible for transformer models such as Codex to\nproduce multiple possible responses to a single prompt, we only\nrequest a single response in order to speed up response time as\nwell as to preserve the token budget for conversational context.\nThus, the \u201ctry again\u201d feature provides an alternate way to produce\na wider variety of responses.\nDuring pilot testing, we noticed that the assistant sometimes\nhappened to generate the same response to multiple, unrelated re-\nquests. In these cases, the assistant tended to get \u201cstuck\u201d in a pattern\nof repeating the same response and was unable to resume normal\nconversation. To avoid this problem, we automatically execute a\n9https://black.readthedocs.io/en/stable/\n10Temperature is a parameter in a generative model that specifies the amount of\nvariation in the generation process. Higher temperatures result in greater variability\nin the model\u2019s output.\n494\n",
    "The Programmer\u2019s Assistant\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nA\nB\nH\nG\nF\nE\nC\nD\nFigure 1: The Programmer\u2019s Assistant. The user interface provides a code editor on the left (A) and a chat pane on the right (B).\nThe \u201ctry again\u201d button (C) allows users to ask the assistant to generate an alternate response to the most recent question. The\n\u201cstart over\u201d button (D) resets the conversational context for the assistant, but maintains the chat transcript in the UI. In this\nexample, we show the assistant introduce itself to the user (E). Next, the user asks a general programming question (F), for\nwhich the assistant provides an inline code response (G). The user then asks a question about code selected in the editor (H),\nfollowed by a series of follow-up questions.\n\u201ctry again\u201d operation in the background when we see identical\nconsecutive responses from the assistant.\nFinally, we noticed that the accumulation of conversational con-\ntext sometimes resulted in the assistant becoming fixated on some\nportion of the earlier conversation. For example, it might respond\nto a question with portions of the prompt or of earlier conversation,\nand become less responsive to newer requests. To address this issue,\nwe introduced a \u201cstart over\u201d feature, accessible via the chat or by\nclicking a button in the UI (Figure 1D), that resets the context to the\noriginal prompt, forgetting the rest of the conversational history.\nWe preserve the chat transcript in the UI, but delineate the break in\nthe assistant\u2019s memory with an annotation in the chat transcript.\nThese annotations are added both for \u201ctry again\u201d and \u201cstart over.\u201d\n3.4\nSample Conversation\nWe provide a real sample conversation with the Programmer\u2019s\nAssistant in Listing 1. This conversation begins with the assistant\ngreeting the user (line 1). Next, the user asks a general Python\nprogramming question (line 4), to which the assistant responds with\na non-authoritative remark (\u201cI think...\u201d) and a code snippet (line\n9). The user next asks a follow-up question that depends on their\nprevious question and the assistant\u2019s response (line 11), to which\nthe assistant provides another code snippet (line 15), satisfying the\nuser\u2019s request.\nThe user then switches topics and asks the assistant to write\na Fibonacci function (line 17), and the assistant again responds\nwith a non-authoritative remark (\u201cI will give it a try,\u201d line 20) and\na block of code. The user then asks how the function works (line\n30) and the assistant provides an adequate description (line 32).\n495\n",
    "IUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nRoss et al.\nFigure 2: Code Preview. Long code responses (> 10 lines) are presented in a pop-up window (A), with a button to copy the code\nand close the window. This code can be re-accessed from the chat transcript by clicking the proxy icon (B).\nNext, the user asks the assistant to re-implement the function in a\ndifferent way (line 37), again leveraging the ability to ask follow-up\nquestions. The assistant produces an alternative implementation\nthat conforms to the user\u2019s request (line 41). The user follows up\nwith a question that depends on multiple past utterances and re-\nsponses in the chat transcript (line 47), and the assistant produces\na relevant response (line 49). The conversation closes with the user\nthanking the assistant (line 53) and the assistant acknowledging\ntheir gratitude (line 55).\nListing 1: A conversation with the Programmer\u2019s Assistant.\nCode presented by the assistant is listed in bold face.\n1\nSocrates: Hello! My name is Socrates your programming\n2\nassistant.\nWhat can I help you with today?\n3\n4\nUser:\nHow do I get a list of the keys in a\n5\nPython dictionary?\n6\n7\nSocrates: I think you would do something like this:\n8\n9\nlist(my_dict.keys())\n10\n11\nUser:\nWhat if I just wanted to iterate over them?\n12\n13\nSocrates: I think you would do something like this:\n14\n15\nfor key in my_dict:\n16\n17\nUser:\nWrite a function that returns the nth value\n18\nin the Fibonacci sequence.\n19\n20\nSocrates: I will give it a try.\n21\n22\ndef fib(n):\n23\nif n == 0:\n24\nreturn 0\n25\nelif n == 1:\n26\nreturn 1\n27\nelse:\n28\nreturn fib(n - 1) + fib(n - 2)\n29\n30\nUser:\nHow does this work?\n31\n32\nSocrates: The Fibonacci sequence is defined by the\n33\nrecurrence relation F(n) = F(n-1) + F(n-2).\n34\nThis function implements that recurrence\n35\nrelation.\n36\n37\nUser:\nCan this be done without recursion?\n38\n496\n",
    "The Programmer\u2019s Assistant\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\n39\nSocrates: I think so. How about this?\n40\n41\ndef fib(n):\n42\na,b = 0,1\n43\nfor i in range(n):\n44\na,b = b,a+b\n45\nreturn\n46\n47\nUser:\nWhich version is better?\n48\n49\nSocrates: I think the recursive version is more\n50\nelegant , but the iterative version is more\n51\nefficient.\n52\n53\nUser:\nThanks!\n54\n55\nSocrates: You 're welcome!\n4\nEMPIRICAL STUDY OF CONVERSATIONAL\nPROGRAMMING ASSISTANCE\nWe conducted an empirical user study of the Programmer\u2019s Assis-\ntant to assess whether conversational assistance provides value in\na software engineering context11. Our inquiry focused on the user\nexperience and value of conversational interactions grounded in\ncode. We therefore designed a qualitative study to investigate atti-\ntudes toward a conversational programming assistant: do people\nenjoy interacting conversationally, what kinds of questions do they\nask, and how does the experience compare to other forms of pro-\ngramming support such as searching the web? We note that prior\nstudies (e.g. [103, 105, 109]) conducted quantitative examinations\nof the use of LLMs in code work; our study is akin to Weisz et al.\u2019s\nqualitative examination of software engineers\u2019 attitudes toward\nworking with models that may fail to produce working code [102].\nTo address our questions, we deployed the Programmer\u2019s Assis-\ntant within our organization \u2013 a global technology company \u2013 and\ninvited people to try it out and give us feedback on their experience.\nWe invited people with varying levels of programming skill in order\nto obtain a wide range of feedback on the kinds of use cases for\nwhich the tool could provide assistance.\n4.1\nTasks\nWe set up the Programmer\u2019s Assistant as a playground environment\nthat participants could try out with a few sample programming\nproblems. We created a tutorial to orient participants to the assis-\ntant, its capabilities, and how to interact with it. We also created\nfour programming challenges focused on writing code, document-\ning code, and writing tests for code. We designed these challenges\nto expose participants to a broad range of the assistant\u2019s capabilities.\nFor each of these challenges, we explicitly did not evaluate metrics\nsuch as the participant\u2019s productivity, the quality of their solutions,\nor the time taken to produce them, as the focus of our study was to\nunderstand the utility of conversational interaction. We selected\nPython as the language used for the tutorial and challenges because\nof its general popularity [21] and the fact that it was well-supported\nby our underlying LLM [24].\n4.1.1\nTutorial. All participants were first introduced to the Pro-\ngrammer\u2019s Assistant through a tutorial. The tutorial walked each\n11For historical context, we note that our study was completed before the public\nrelease of ChatGPT [60], which has subsequently demonstrated the application of\nconversational assistance for programming tasks [34].\nparticipant through 10 sample interactions to give them a feeling\nfor what the assistant could do and how to interact with it. The\ntutorial demonstrated how to ask questions, how to request code to\nbe generated, and how to evaluate existing code. It did not specifi-\ncally cover how to generate documentation or unit tests. Tutorial\ninstructions were provided within the code editor. We include the\nspecific text used for the tutorial in Appendix B.\n4.1.2\nProgramming Challenges. After completing the tutorial, par-\nticipants unlocked four programming challenges. Two of the chal-\nlenges involved coding problems (writing a queue class and writing\ncode to create a scatterplot of data in a CSV file), one involved doc-\numenting a given function (an implementation of a graph search\nalgorithm), and one involved writing unit tests for a given func-\ntion (computing the greatest common divisor of two arguments).\nAlthough the Programmer\u2019s Assistant was visible and available for\nuse, we provided no specific requirement that it actually be used to\ncomplete the challenges.\nAfter participants completed their solution to a challenge, they\nsubmitted it by clicking a button in the UI. The code editor used\nin the Programmer\u2019s Assistant was not a fully-functional IDE and\ndid not provide syntax checking or the ability to run, test, or debug\ncode. Due to these limitations, participants were asked to submit\ntheir solutions when they felt they had completed the challenge to\ntheir own satisfaction.\n4.2\nParticipants\nTo recruit participants for our study, we posted internal advertise-\nments in various communications channels focused on software\nengineering. Our advertisements stated that we were evaluating a\nconversational programming assistant, but were kept deliberately\nvague in order to minimize the impact on peoples\u2019 expectations of\nthe experience.\nOur advertisement yielded a pool of 140 potential participants.\nIn order to recruit a diverse sample, we used a screening survey\nthat asked about their job role, their familiarity with and recency\nof use of Python, and their availability to participate in our study.\nWe accepted participants into the study on a rolling basis, selecting\nparticipants to capture a range of programming experiences and\nensure balanced gender representation. We conducted periodic re-\nviews to determine whether we were learning something new from\neach participant or if we had reached the point of saturation [7]. We\nstopped collecting data after running 42 participants as we were no\nlonger observing any new behaviors or gleaning any new insights.\nThe Programmer\u2019s Assistant implementation and configuration\nwere held constant over the course of the study; no changes to the\nUI design or LLM prompt were made.\nOur participants had the following self-identified characteristics:\n\u2022 Job role: 19 Software Engineers, 12 Researcher/Scientists, 3\nSoftware Architects, 2 Data Scientists, 1 Machine Learning\nEngineer, 1 Systems Test Engineer, 1 Business Analyst, 1\nManager, 1 Marketer, and 1 Consultant.\n\u2022 Gender: 21 Female, 19 Male, 1 Gender Variant / Non-con-\nforming, and 1 Preferred not to say.\n\u2022 Python Experience: 17 participants had 3+ years of Python\nexperience, 11 had 1-3 years, 11 had less than 1 year, and 3\nwere not familiar with Python.\n497\n",
    "IUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nRoss et al.\n\u2022 Recency of Python Use: 29 participants had written Python\ncode within the past month, 4 within the past year, 5 within\nthe past 5 years, and 4 had not written Python code within\nthe past 5 years.\nWe provide full demographic information for individual partici-\npants in Appendix E.\n4.3\nProcedure\nParticipants completed the study on their own time, independently\nand without moderation. Each participant was provided with a web\nlink to a pre-study survey that described the nature of the study and\nthe tasks that they would be expected to perform. They were then\ndirected to the Programmer\u2019s Assistant to complete the tutorial and\nthe four programming challenges. When participants indicated they\nwere finished with the challenges12, they were directed to a final\npost-study survey. Complete sessions generally required about an\nhour of effort, though some participants spread their effort across\na longer period of time and across multiple sessions. Participants\nwere compensated for their time at a rate equivalent to US $15/hr.\n4.4\nMeasures\nWe collected a variety of data in our study from three sources:\n(1) Surveys. We employed three surveys in the study: a pre-\nstudy survey to collect demographic information, a pre-task\nsurvey to gauge expectations of the conversational user ex-\nperience, and a post-task survey to assess actual user expe-\nrience. We describe these survey questions in the relevant\ncontext of our results, and we provide a complete listing of\nall survey instruments in Appendix A.\n(2) Event logs. The Programmer\u2019s Assistant was instrumented\nto collect data on participants\u2019 usage. The event logs pro-\nvided timestamped records of interaction events, including\nconversational exchanges, hiding/showing the assistant, use\nof the \u201ctry again\u201d and \u201cstart over\u201d features, and use of copy/-\npaste.\n(3) Conversation logs. From the event logs, we extracted con-\nversational transcripts between each participant and the\nProgrammer\u2019s Assistant.\n5\nRESULTS\n5.1\nData & Analysis\nWe collected a wealth of data in our study: 126 survey responses\nfrom three surveys per participant, containing 296 written com-\nments in open-ended survey questions, and 4,877 instances of 23\ndifferent types of UI events, including 1,699 conversational ex-\nchanges13 in the event logs. We also compute, for each participant,\ncounts or durations for 21 different metrics from the event logs.\nIn our analysis, we deliberately exclude the portion of our data\ncollected during the tutorial exercise. We exclude this data because\nthat activity was guided by the tutorial instructions, not by our\nparticipants\u2019 own initiative. Thus, our final sample consists of 3,172\n12We did not enforce that participants actually complete all of the challenges. Never-\ntheless, all participants but one did submit solutions to all of the challenges.\n13We refer to a participant\u2019s utterance, followed by the assistant\u2019s response, as a\nconversational exchange.\nevents, including 968 conversational exchanges in the event logs;\nno survey data was excluded.\nOur primary analysis of this data is qualitative, as our partici-\npants provided us with a rich source of interesting feedback and\nthought-provoking insights in their comments. Where applicable,\nwe supplement this data with quantitative data from the survey\nand the event logs, as well as chat transcript data from the con-\nversation logs. In this way, we triangulate [47] across our three\ndata sources, using the open-ended survey data as a foundation.\nWhen we quote participants, either from their qualitative survey\nresponses or the conversational transcripts, we reproduce their\nwords exactly as typed, including typos, misspellings, grammati-\ncal errors, capitalization, and potential trigger words, and we only\nmake minor clarifying edits where needed, delineated by square\nbrackets.\nIn order to set the context for our analysis, we first describe\nhow we used reflexive thematic analysis to analyze participants\u2019\nresponses to the open-ended survey questions. We then describe\nour analysis of the conversation logs and our development of a\ncoding guide based on Conversation Analysis [76], and specifically,\nMoore and Arar\u2019s Natural Conversation Framework [50].\n5.1.1\nThematic Analysis of Qualitative Survey Responses. We con-\nducted a reflexive thematic analysis to analyze the responses to\nour seven open-ended survey questions. We followed the process\ndescribed by Braun and Clarke [16] in which researchers immerse\nthemselves in the data, generate codes for material that seems\ninteresting, and then iteratively group and refine codes through\ncollaborative discussion in order to identify higher-level themes.\nInitially, four authors performed open-coding on the open-ended\nsurvey responses. Through discussion, these codes were grouped\nand consolidated into a single set, which were then re-applied to\nthe data by two authors. After another round of discussion, these\nauthors identified a set of 12 higher-level themes. Some themes had\nclear parallels to quantitative survey questions or event log data,\nand thus represented clear instances where we were able to trian-\ngulate across data sources. Other themes surprised us. We structure\nour presentation of the results based on these 12 themes, grouped\ninto three different aspects of the user experience: expectations\nand experience, utility of conversational assistance, and patterns of\ninteraction and mental models.\n5.1.2\nConversation Analysis via the Natural Conversation Frame-\nwork. In order to understand the content and structure of the\nconversations that took place between our participants and the\nProgrammer\u2019s Assistant, we turned to the Natural Conversation\nFramework [50] (NCF). We developed a codebook for the event\nlogs, beginning with 21 different categories of utterances from the\nNCF. Nine NCF categories \u2013 Acknowledgment, Apology, Confirma-\ntion, Expression of Gratitude, Farewell, Greeting, Self-Identification,\nWelfare Check, and Welfare Report \u2013 appeared twice in our code-\nbook to distinguish cases in which the utterance was made by the\nhuman participant vs. the assistant. Other NCF categories were\nsplit to provide nuanced detail about the interaction; for example,\nwe distinguished three different kinds of NCF requests, depending\nupon whether they were stated as Requests for Action (e.g. \u201cWould\nyou...\u201d), Commands of Action (e.g. \u201cWrite a function that...\u201d), or\nExpressions of Desire (e.g. \u201cI want...\u201d). We also added 18 additional\n498\n",
    "The Programmer\u2019s Assistant\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nInterlocutor\nOrientation\nCodes\nHuman\nSocial\nAcknowledgment, Apology, Criticism, Expression of Gratitude, Farewell, Greeting, Politeness,\nPraise, Self Identification, Small Talk, Welfare Check, Welfare Report\nTask\nAsks Question, Asserts Information, Capability Check, Command of Action, Expression of Desire, Identi-\nfies Error, Request for Action, Requests Elaboration, Requests Explanation\nMeta / UI\nChat Context Required, Confirmation, Copy, Copy (Extraneous), Erroneous Input, Includes Selection,\nIncludes Extraneous Selection, Missing Selection, Paste, Paste (Extraneous), Pasted Code in Chat, Spelling\nError, Start Over, Try Again\nAssistant\nAppears Fixated, Claims Ignorance, Didn\u2019t Understand, Grants Request (Complete), Grants Request\n(Incomplete), Offers Help, Provided Wrong Answer, Requests Details, Requests Paraphrase, Response\nIncludes Code, Spews Garbage\nTable 1: Event log codebook. Our codebook contained 46 unique codes, applied separately to participant utterances (Human)\nand assistant responses (Assistant). Codes in bold were applied to both participant and assistant responses. Human codes were\nclassified as demonstrating either a social or task orientation to the assistant.\ncodes to identify meta-information such as utterances that included\ncode, utterances that referenced selected code, utterances that im-\nplicitly or explicitly referenced earlier portions of the conversation,\nor non-verbal UI activities such as copies, pastes, and invocations\nof \u201ctry again\u201d and \u201cstart over.\u201d Finally, we classified a subset of the\nhuman-applied codes based on whether they represented a par-\nticipant\u2019s task or social orientation toward the assistant. We list\nour codes in Table 1, but note that not all of them ended up being\nrelevant to our analysis.\nWhen coding conversational data, we applied individual codes\nat the level of each conversational utterance. We allowed multiple\ncodes to be applied to each utterance to account for utterances that\nperformed multiple functions (e.g. greeting and self-identification).\nIn order to ensure consistency in how our codebook was applied,\ntwo authors coded a 10% sample of the 968 conversational ex-\nchanges, achieving a satisfactory level of inter-rater reliability (Krip-\npendorf\u2019s \ud835\udefc = 0.77, where agreement was conservatively defined\nas having all of the same codes applied to both utterances in a\nconversational exchange).\n5.2\nExpectations and Experience\nPilot testing of the Programmer\u2019s Assistant suggested that software\nengineers would be skeptical of a conversational programming\nassistant and its ability to provide useful assistance. Our study\nrevealed that, for most participants, their actual experience after\nusing the tool was better than they had anticipated. Participants\nwere surprised at the quality of the assistant\u2019s responses and they\nappreciated how its integration with the code editor reduced the\namount of context switching they needed to do in the UI. Some\nparticipants struggled with the code selection feature, although\nothers appreciated the ability to ask questions related to selected\ncode.\n5.2.1\nUsage. All of our participants engaged with the Program-\nmer\u2019s Assistant while working on the challenges, despite there\nbeing no requirement to do so. Forty-one participants submitted\nsolutions to all four challenges, and one participant, P14, only sub-\nmitted solutions for one of the four challenges. Participants spent\nan average of 68 minutes engaged with the assistant, as measured\nby the amount of time the Programmer\u2019s Assistant window was in\nfocus.\nParticipants made an average of 23.0 utterances (SD = 15.1 ut-\nterances) to the assistant. On average, 6.2 of their utterances (SD =\n4.3 utterances) contained a code selection. The average latency per\nrequest14 was 6.7 seconds (SD = 3.1 seconds).\nWe saw a 66.3% rate of acceptance of generated code, where we\nconsidered code to be accepted if the participant performed a copy\nimmediately after the code was generated. This acceptance rate is\nmuch higher than the 27% acceptance rate reported for Copilot [109].\nWe believe one reason we observed a higher acceptance rate is\nbecause Copilot\u2019s completion suggestions are generated proactively,\nwhereas the Programmer\u2019s Assistant\u2019s suggestions are generated\nupon request. When copying generated code from the assistant,\nparticipants most often copied the entirety of the generated code,\nand only in 5.8% of cases did they copy a smaller portion of it.\n5.2.2\nUser Experience Expectations & Changed Attitudes. Prior to\nrunning our study, we had reason to believe that participants would\nbe skeptical of a conversational programming assistant. Before de-\nveloping the Programmer\u2019s Assistant, we showed potential users\nmockups of a program editor with an integrated chatbot feature.\nThese prototypes elicited uniformly negative reactions. People told\nus about their frustrating experiences with conventional chatbots\nand raised doubts about the knowledge, capabilities, and value\nof a conversational programming assistant. This skepticism moti-\nvated us to develop the Programmer\u2019s Assistant in order to evaluate\nwhether the conversational experience, as powered by a state-of-\nthe-art code-fluent LLM, would be better than people had antici-\npated. During pilot testing, we received feedback that the Program-\nmer\u2019s Assistant provided a much better conversational experience\ncompared to testers\u2019 previous experiences with chatbots. Thus, in\ndesigning our study, we felt it important to first gauge participants\u2019\nexpectations of a conversational interaction around code, and then\nmeasure their experience after the fact.\n14This time includes additional time added by our proxy server to ensure our confor-\nmance to the API rate limitation.\n499\n",
    "IUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nRoss et al.\nWe developed a short inventory of six scale items to measure user\nexperience of code work15. The scale was administered twice: once\nbefore participants were exposed to the Programmer\u2019s Assistant\n(but after they had been briefed that they would interact with an AI\nchatbot), and once after completing the programming challenges.\nThe items were presented with the appropriate tense: Do you expect\n(Did you find that) the Programmer\u2019s Assistant: (a) will be (was)\neasy to use; (b) will understand (understood) your requests; (c) will\nprovide (provided) high quality responses; (d) will help (helped)\nyou to write better code; (e) will help (helped) you to write code\nmore quickly; (f) will be (was) enjoyable to use. Each item was rated\non a 4-point scale of extent: Not at all (1), A little (2), Somewhat (3),\nA great deal (4).\nA factor analysis revealed the items on this scale measured a\nsingle construct, which we identify as user experience (Cronbach\u2019s\n\ud835\udefc = 0.87). Thus, we computed two scores of user experience (UX)\nfor each participant: a pre-task UX score computed as the average\nof their six pre-task expectation scale responses, and a post-task\nUX score computed as the average of their six post-task experience\nscale responses.\nWe found that participants had lower initial expectations for\ntheir experience with a conversational programming assistant (pre-\ntask UX M (SD) = 3.0 (0.62) of 4) than their experience actually\nwas (post-task UX M (SD) = 3.6 (0.32) of 4). A paired sample t-test\nshows that this difference was significant, \ud835\udc61(41) = 5.94, \ud835\udc5d < .001,\nCohen\u2019s \ud835\udc51 = 0.92 (large). Measured another way, 32 participants\n(76.2%) had post-task UX ratings that were higher than their pre-\ntask expectations, demonstrating a significant shift in attitudes\ntoward conversational programming assistance.\nHowever, the UX ratings alone fail to capture participants\u2019 nu-\nanced expectations of the assistant and the reasons for their shifted\nattitudes after using it. Participants expressed a variety of expecta-\ntions of the assistant before using it, including that it would be easy\nto use (P30) and produce correct responses (P30), understand the\nproblem and what is being asked of it (P8, P9, P11), not interfere\nwith their flow state (P5), produce imperfect or questionable out-\nputs (P6, P21), improve with feedback (P31), provide generic and\nunhelpful answers (P17) or only answer basic questions (P40), and\nproduce responses quickly (P40).\nP17 expected \u201cto be frustrated very quickly and that what I\u2019d think\nwould be relatively common questions would be responded to with\ngeneric, unhelpful answers.\u201d P6 explained, \u201cI didn\u2019t have very good\nexperiences with chatbots. I think I\u2019ll need to spend more time in\nreviewing and fixing the suggestions than in writing the code myself\nfrom scratch.\u201d P11 had a more balanced view, that \u201cIt\u2019ll do some tasks\nreally well, but others will not be as reliable.\u201d\nAfter interacting with the Programmer\u2019s Assistant, many par-\nticipants commented on how the experience was better than they\nanticipated, because it \u201cseemed to be able to handle complex issues\u201d\n(P10) and \u201cwas a great help\u201d (P8). P20 felt it was \u201cincredible!\u201d P6 and\nP17, who were both initially skeptical, reported having a positive\nexperience. For P6, \u201cIt absolutely exceeded all my expectations, in\n15Our scale items were modeled from scales published in Weisz et al. [103, Table 9 \u2013\nAI Support] that measured constructs including ease of use (item 3), response quality\n(item 1), the production of higher-quality code (item 5), and the ability to write code\nmore rapidly (item 4). We added additional items to cover the constructs of request\nunderstanding and enjoyment, and we cast all items on a 4-point scale of extent.\nall aspects that I could have imagined and more!\u201d P17 provided a\nmore quantitative assessment: \u201cInitial expectations: 3 Actual: 9.5.\u201d\nP38 was emphatic in their evaluation: \u201cI was blown away how well\nit allowing me to structure how I want the code to look and work and\njust giving me the thing I asked for.\u201d\nMany participants described a sense of surprise in their experi-\nences. P9 was surprised by how well it understood their requests:\n\u201cI was surprised at how well the Programmer Assistant\nwas able to understand my requests and generate good\ncode/documentation/tests. It understood major concepts\nand was able to explain it to me in a clear way, and it\nwas also able to understand and write functional code.\nIt even was able to help me review my answer. I was also\nsurprised at how well it could understand the context\nof what I was asking in follow-up questions when I did\nnot specify exactly what I was talking about, but rather\nreferencing our prior conversation (such as, \u2018what does\nthat mean\u2019).\u201d (P9)\nSimilarly, P6 was surprised that they liked the conversational\ninteraction when they expected that they wouldn\u2019t:\n\u201cI though[t] I wouldn\u2019t like the chatbot interaction and\nthat I would prefer something like the tool I\u2019ve seen in\nthose demos [of Copilot]. But surprisingly, after using\nthe chatbot (and seeing the results: easy to use, it un-\nderstands well, I felt it like a partner) I like this kind of\nhelp.\u201d (P6)\n5.2.3\nQuality of Assistant\u2019s Responses. In order to gauge the quality\nof responses produced by the Programmer\u2019s Assistant, we examined\nthe 910 task-oriented requests made by participants in the study.\nFor the vast majority (80.2%), the assistant produced a correct re-\nsponse (Grants Request (Complete)); in other cases, the assistant\u2019s\nresponse was incorrect (9.6%; Provided Wrong Answer), correct but\nincomplete (4.4%; Grants Request (Incomplete)), or the assistant\ndidn\u2019t understand (3.4%; Didn\u2019t Understand), claimed ignorance of\nthe subject (1.5%; Claims Ignorance), or produced another type of\nresponse (0.9%; Appears Fixated, Spews Garbage).\nParticipants also reported experiencing this variability in the\nquality of the assistant\u2019s responses. Some participants described\nhow the assistant provided \u201cdetailed answers\u201d (P17) and \u201chigh qual-\nity outputs\u201d (P18) that were \u201csurprisingly good\u201d (P2). P6 felt it was\n\u201cincredible to see the quality of the responses,\u201d and P3 even explored\nthe assistant\u2019s capabilities outside the scope of the challenges and\nfound that it could handle those as well:\n\u201cIt was surprising the quality of the code and the ability\nto answer all my questions correctly. Although I think\nthe challenges may be biased towards what the Assistant\nis able to do, it was a great experience because I asked\nmany other things and it was able to answer correctly.\u201d\n(P3)\nOf course, the Programmer\u2019s Assistant wasn\u2019t perfect, and some\nparticipants did run into issues. For P35, \u201cThe documentation gener-\nation did not perform very well.\u201d P16 questioned the accuracy of the\nknowledge encoded in the model: \u201cDoes the model need to be up-\ndated? It said latest python version is 3.7 but google says it\u2019s 3.10.\u201d In\nsome instances, participants needed to ask their question multiple\n500\n",
    "The Programmer\u2019s Assistant\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\ntimes to get a good response: \u201cyou need to ask many times if you\nwant to get an answer and also a detailed explanation\u201d (P3). P27 felt,\n\u201cit was annoying when I asked it to try again and it would give me\nthe same response.\u201d P22 struggled because, \u201cIt didn\u2019t seem to handle\nmultiple sentences well.\u201d\nP28 perhaps offered the most scathing criticism, that, \u201cIt makes\nmistakes often enough to be not very practical.\u201d However, despite\nthe production of poorer-quality responses, other participants felt\nthat the assistant was still helpful. P36 reported that, \u201cOnly minor\ntweaks were normally needed to correct any issues.\u201d Similarly, P38\ndescribed how the assistant wasn\u2019t able to completely solve their\nproblem, but provided a useful start:\n\u201cThere was only one hickup I noticed where when I\nasked it to memoize fibonacci it couldn\u2019t, but it dropped\nthe building blocks on my lap for me to finish so that\nwas fine, that was like minutes of effort on my part.\u201d\n(P38)\n5.2.4\nUI Design & Affordances. Participants made many comments\non our specific UI design and the affordances provided (or not\nprovided) in our chat-augmented editor. Overall, the integration\nbetween the chat pane and the code editor was \u201cvery good\u201d (P23),\nwith a \u201cnice interface between the code pane and the assistant pane\u201d\n(P17) that \u201cmakes it really convenient\u201d (P35).\nPrior research by Brandt et al. [15] has shown how keeping\ndevelopers focused in their IDE improves productivity, and our\nparticipants expressed similar sentiments. P40 remarked, \u201cIt allows\nme to stay in one browser window/tab!\u201d and P12 hinted at how the\ninterface might preserve their flow state by \u201cprevent[ing] me from\ngetting distracted when looking into an issue in another tab.\u201d\nSome aspects of our user interface were confusing to participants,\nsuch as the mechanism for selecting code to be included in the\nconversational context. P7 remarked, \u201cIt\u2019s was a little confusing\ndoing the selection part for it to tell me what a function does, but...\nit gave me code that was insanely easy to copy and paste.\u201d Other\nparticipants appreciated the code selection mechanism, such as P11:\n\u201cI enjoyed the code selection feature, and found that very easy to use.\u201d\nIn the event logs, we identified 20 instances in which a participant\nunintentionally included selected code in the conversation when\nit wasn\u2019t needed (Includes Extraneous Selection), 12 instances in\nwhich a code selection was omitted when it was needed to provide\ncontext for the question (Missing Selection), and 16 instances in\nwhich a participant copy/pasted code directly into the chat rather\nthan selecting it in the editor (Pasted Code in Chat). Although\nthese cases represent a small fraction of the 227 instances in which\na code selection was required and included in the conversation\n(Includes Selection), their presence does indicate that more attention\nis needed to the interaction design of code selection.\nAnother issue regarded the awareness of the \u201ctry again\u201d and\n\u201cstart over\u201d features. The \u201ctry again\u201d feature was only used by 14\nparticipants, who used it a total of 63 times over the course of\nthe study. Some participants used it specifically when they got an\nanswer which they saw as clearly wrong, while others used it to\nget a variety of possible answers before proceeding. The \u201cstart over\u201d\nfeature was used even less, by 5 participants who used it a total of\n6 times. Despite our effort to surface these conversational features\nin the UI via shortcut buttons, they may not have been sufficiently\nnoticeable or salient: \u201cThe \u2018try again\u2019 button is not so reachable, often\ntimes I forgot it exists\u201d (P23). By contrast, at least one participant\nwas successful with these features:\n\u201cat some point it had issue with challenge 3 and I had to\nstart over. Just asking \u2018try again\u2019 was not enough and I\nwas getting always the same (wrong and not related)\nanswer. starting again solved the issue!\u201d (P20)\n5.3\nUtility of Conversational Assistance\nOur next set of themes concerns the utility provided by conversa-\ntional programming assistance. Participants felt the assistant was\nhighly valuable and desired to use it in their own work. They felt it\nwould be most helpful for smaller or narrowly-scoped tasks, but\nable to provide a wide variety of types of assistance. The fact that\nthe interaction model was conversational and grounded in code\nwere valuable aspects, as was the ability for the assistant to bolster\nusers\u2019 learning about programming topics through that interac-\ntion. Participants did question whether they could trust and rely\nupon the assistant\u2019s responses, echoing a similar theme discussed\nin Weisz et al. [102].\n5.3.1\nValue & Appropriate Tasks. Participants rated the value of\nthe Programmer\u2019s Assistant highly (M (SD) = 8.6 (1.4) of 10). Many\nparticipants asked questions such as, \u201cCan I have it in my editor\nplease?\u201d (P15), or made comments that, \u201cI would enjoy using it in\nthe future\u201d (P36), \u201cI would love to be able to... have access to it for\nmy coding\u201d (P37), and \u201cI\u2019d love to use this tool as part of my usual\nprogramming workflow if I could!\u201d (P39). Some of the reasons why\nparticipants found it valuable are because it \u201chelp[s] me remember\nhow to do things in certain languages that normally I would just\nGoogle\u201d (P9) and \u201cIt helps me to avoid silly syntax errors and can\nwhen I cannot remember exact function/method names and required\narguments\u201d (P40). We did not observe any differences in value\nratings based on participants\u2019 familiarity with or recency of using\nPython.\nParticipants described a wide variety of tasks for which they\nfelt the assistant would be useful. These tasks included \u201cordinary\u201d\n(P23), \u201csimpler\u201d (P2), and \u201csmall, repetitive\u201d (P4) tasks such as \u201cquick\nlookups\u201d (P25) for \u201cshort chunks of code\u201d (P11) or for \u201cnarrowed\nquestions\u201d (P26). Participants also felt the assistant was useful for\n\u201csmall containable novel algorithms\u201d (P38) and \u201clittle coding problems\u201d\n(P4).\nSeveral kinds of task assistance were reported as being valuable,\nsuch as explaining code (P31), implementing business logic in a\nUI (P38), understanding what code does (P19, P37), and recalling\nlanguage syntax, method names, and arguments (P12, P15, P20, P40,\nP42). P27 felt that the assistant was \u201cMore helpful when recognizing\na specific well known algorithm but not things you make yourself.\u201d\nParticipants also made recommendations for how to increase\nthe value of the Programmer\u2019s Assistant. P38 suggested, \u201cWhat\nwould blow me away though is if it\u2019s able to help with what I do most\noften which is to integrate, refactor and iterate on an existing system.\u201d\nP16, P26, and P38 all desired more information on the data sources\nused to produce the assistant\u2019s responses. P9 requested to \u201cHave\nthe Programmer\u2019s Assistant examine your code and make proactive\nsuggestions for improving it in the chat.\u201d P36 requested the same,\n501\n",
    "IUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nRoss et al.\nbut cautioned that, \u201cCare would need to be taken to avoid becoming\nan annoyance or disrupting the flow of a coding session.\u201d\nIn the post-task survey, we probed participants on how certain\nchanges to the Programmer\u2019s Assistant would either decrease, in-\ncrease, or result in no change to its value. Over 75% of participants\nfelt that the assistant would be more valuable if it operated in a\nproactive manner, either by making improvement suggestions in\nthe chat or as comments directly in the code. Similarly, 78.6% of\nparticipants felt that having more buttons in the UI for common\nfeatures such as explaining or documenting code would make the\ntool more valuable.\n5.3.2\nConversational Interactions Grounded in Code. One of the\nchallenges in interpreting participants\u2019 comments about the utility\nof the Programmer\u2019s Assistant was in disentangling the extent to\nwhich value was derived from the quality of the underlying model\nversus the integration of conversation in a code context. Indeed,\nparticipants felt that the chat interaction was valuable: 69.0% of\nparticipants felt that eliminating the conversational interaction\nand making the assistant behave more like web search would de-\ncrease its value. Further, our analysis of the conversation transcripts\nrevealed that 42% of the 910 task-oriented utterances from partic-\nipants required historical conversational context (Chat Context\nRequired) in order to be correctly interpreted. Thus, we observe\nthat behaviorally, participants did rely on conversational context\nin their interactions.\nIn the post-task survey, 83% of participants rated the importance\nof the ability to ask follow-up questions as being \u201csomewhat\u201d or \u201ca\ngreat deal.\u201d Several participants specifically commented on the value\nof this conversational context. P39 remarked, \u201cI absolutely loved\nhow you can straight up ask follow-up questions to the Programmers\u2019\nAssistant without having to reiterate the original topic/question.\u201d P15\nexpressed a similar sentiment, saying, \u201cI think the conversational\ncontext was someone helpful, just in communicating that it\u2019s a running\nconversation where my context is remembered.\u201d P9 provided a similar\nanalysis:\n\u201cThis tool was so helpful at answering questions I had\nabout the code in the context of the code I am working\non... I was also impressed with how well it was able to\nremember the context of our conversation, especially\nwhen I asked vague follow-up questions.\u201d (P9)\nIn addition, some participants identified how a conversational\ninteraction grounded in code was useful, \u201cbecause I think to \u2018un-\nderstand\u2019 the dev context could be VERY important\u201d (P31). In fact,\n24.9% of task-oriented utterances included a relevant code selection\n(Includes Selection), showing that participants valued this ability.\nContrasting with these participants, P18 felt that interacting\nwith the assistant conversationally was tedious, and they employed\na more direct approach:\n\u201cI really like the PA. But, I didn\u2019t converse with it like\na chat bot. I often told it what to do (\u2018Document this\ncode.\u2019) as opposed to asking it what to do (\u2018How do I\ndocument this code?\u2019). Talking to it the way that was\nsuggested in the tutorial seemed overly verbose/tedious.\u201d\n(P18)\nDespite these individual differences in interaction preferences,\nP39 envisioned that both interaction styles could be supported in\nthe tool:\n\u201cI think both options should exist: people should be able\nto input their queries like a search bar AND also give\ntheir question as if in conversation.\u201d (P39)\n5.3.3\nLearning Effects. One specific benefit of the Programmer\u2019s\nAssistant identified by participants is its ability to help people\nimprove their programming skills and reinforce knowledge gaps.\nFor example, it can help users \u201cremember how to do things in certain\nlanguages... such as, when I am using a language I haven\u2019t used in a\nwhile\u201d (P9). The assistant can also serve as an memory aid, such\nas when \u201cI use a lot of libraries that I don\u2019t always remember all\nof the functions\u201d (P15). Similarly, P31 said, \u201cNo matter how good\nyou\u2019re as a developer, you can\u2019t (humanly) remember all the API of\nhundreds of libs or new languages... I\u2019d learn new dev lang and new\nlib/frameworks faster.\u201d\nP39 felt the assistant \u201cis perfect for programmers of all levels,\u201d and\nP1 felt it could help them rapidly improve their Python skills:\n\u201cI have wanted to learn python... The main concern\nhow much time spent learning is needed before I could\nactually get some value out of learning python. I have\na feeling this would cut that time down from weeks to\na day or so.\u201d (P1)\nP39 also identified the fact that, because the interactions with\nthe assistant are conversational, it forces people to learn how to\ncommunicate to others about their code:\n\u201cThe conversation aspect promotes proper communi-\ncation, which would really stand to benefit budding\nprogrammers if they want to learn how to explain con-\ncepts more fluently in the future to their colleagues.\u201d\n(P39)\nConversely, P36 suggested that over-reliance on programming\nassistance might have a detrimental effect to one\u2019s learning:\n\u201cIt\u2019s definitely a huge time saver, but over-reliance on it\nmay cause new developers to skip learning the reference\nmaterial themselves and discovering new things and\nsparking new ideas.\u201d (P36)\n5.3.4\nTrust. Many participants raised questions about whether\nthey could trust the responses provided by the Programmer\u2019s Assis-\ntant. P21 asked this question most directly: \u201cwill the code be correct,\nsafe, efficient?\u201d Other participants raised similar questions, such as,\n\u201cI\u2019m wondering how it validates it\u2019s answers, if it can be trusted to\nalways give a working answer\u201d (P10), and \u201cSometimes lack of source\nand context may raise doubts in the mind of the programmer\u201d (P16).\nThese issues of trust were exacerbated by the fact that the Pro-\ngrammer\u2019s Assistant did not allow participants to actually run their\ncode. Because of this limitation, participants had to rely on their\nown knowledge to judge the correctness of the assistant\u2019s responses.\nP19 asserted, \u201cThere is no way to evaluate if the Programmer\u2019s as-\nsistant is giving you the right advise or not other than your own\nknowledge,\u201d and P9 concurred: \u201cI had to trust that it was correct (and\nuse my own prior knowledge).\u201d\n502\n",
    "The Programmer\u2019s Assistant\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nP18 described the potential consequences of allowing the assis-\ntant to write code for them:\n\u201cThe only thing that made me nervous was that it could\nhave introduced a bug that wasn\u2019t immediately appar-\nent. And given I didn\u2019t write the code, I could have easily\nglossed over a mistake when reviewing it. Especially if\nit is also the one writing the test cases.\u201d (P18)\nDespite our efforts to make the Programmer\u2019s Assistant respond\nin non-authoritative ways, we did observe participants sometimes\nuncritically accept generated results that were clearly wrong or\nincomplete. Thus, we did find behavioral evidence for over-reliance.\nListing 2: Building trust through explanations and justifica-\ntions\n1\nP9:\nwhat is a unit test?\n2\n3\nSocrates: A unit test is a test of a single function\n4\nor method.\nIt is typically written by the\n5\nprogrammer who wrote the code.\n6\n7\nP9:\n8\ndef euclid(a, b):\n9\nwhile b != 0:\n10\n(a, b) = (b, a % b)\n11\nreturn a\n12\n13\nhow would I write a unit test for this code?\n14\n15\n[...]\n16\n17\nSocrates: I think you would do something like this:\n18\n19\ndef test_euclid():\n20\nassert euclid(2, 4) == 2\n21\nassert euclid(6, 9) == 3\n22\nassert euclid(17, 12) == 1\n23\nassert euclid(4, 4) == 4\n24\n25\nP9:\nwhat is a euclid function\n26\n27\nSocrates: The euclid function is a function that\n28\ncomputes the greatest common divisor of two\n29\nnumbers.\n30\n31\nP9:\nis this a good unit test for a euclid\n32\nfunction\n33\n34\nSocrates: I believe that it is a good unit test for\n35\na euclid function.\n36\n37\nP9:\nwhy\n38\n39\nSocrates: It seems to me that it tests the edge\n40\ncases of a and b being equal , a being\n41\ngreater than b, and b being greater than a.\n42\n43\n[...]\n44\n45\nP9:\nthanks Socrates this was helpful\n46\n47\nSocrates: Always happy to help.\nOne way to address trust issues is for the assistant to provide\nfurther explanations and justifications that can calibrate a user\u2019s\nconfidence in the assistant\u2019s responses. Such explanations could\nbe requested conversationally, though most participants did not\nattempt to do so. One participant (P9) did ask for such explanations,\nand we show a summary of their transcript in Listing 2. In this\ninstance, P9 asked for a definition of a unit test (line 1), an expla-\nnation of the code being tested (line 25), and justifications of the\nquality of the unit test (lines 31& 37). Thus, we observe that the\nassistant is capable of producing explanations and justifications\nwhen asked.\n5.4\nPatterns of Interaction and Mental Models\nParticipants interacted with the assistant in a variety of ways with\ntwo main patterns of usage standing out: (1) invoking the assis-\ntant to solve the entire programming challenge, and (2) breaking\nthe challenge down into a set of smaller tasks and invoking the\nassistant\u2019s help for each. There were no clear differences in how\nparticipants with differing Python experience approached the tasks.\nParticipants\u2019 mental models of the assistant also varied. Although\nparticipants strongly saw the role of the assistant as being a tool,\ntheir behaviors revealed that in many cases, they actually treated it\nas a social agent. In addition, participants ascribed various mental\ncapacities to the assistant, such as having the ability to understand,\ncompute, and learn.\nParticipants felt the assistant changed the nature of their work\nprocess. For some participants, it enabled them to focus on the\nhigher-level aspects of development because the assistant handled\nlower-level details or provided partial solutions for them to build\nupon. Many participants felt the assistant sped up their work and\nhelped them remain focused on their tasks.\nFinally, participants drew comparisons between the Program-\nmer\u2019s Assistant with other forms of programming support such\nas Copilot and web search. They felt that the conversational style\nof interaction enabled them to discover new, emergent behaviors\nfrom the model that were unavailable from Copilot\u2019s focus on code\nautocompletion. They also felt that the examples provided by the\nassistant were more readily usable within their own code compared\nto browsing for answers within search results, speeding up the\ncoding process. However, some participants advocated for a bal-\nanced approach to the design of programming assistance tools by\nincorporating multiple modes of interaction rather than fixating\non a single one.\n5.4.1\nInteraction Styles and Assistant Role. We observed that par-\nticipants interacted with the Programmer\u2019s Assistant in strikingly\ndifferent ways. Some participants would present the entire chal-\nlenge description to the assistant and then work with the results it\nproduced. Other participants approached the programming chal-\nlenges in a piecemeal fashion, breaking them apart into a set of\nsmaller tasks, then invoking the assistant to aid with each one.\nExperience with Python was not a determinant of how partici-\npants approached the programming challenges, but it did seem to\nimpact how participants interacted with the assistant. Less experi-\nenced participants tended to ask the assistant basic questions such\nas, \u201cWhat is a unit test\u201d (P29, not familiar with Python) and \u201chow\ndo I document a function?\u201d (P27, < 1 year of experience). More expe-\nrienced participants made detailed requests about specific Python\nlibraries or algorithms, such as, \u201cgiven a pandas dataframe with\ntwo columns \u2018Date\u2019 and \u2018Sales\u2019 please use matplotlib to draw me a\nscatterplot\u201d (P38, 3+ years of experience) and \u201cimplement a runge-\nkutta algorithm for solving an ODE with adaptive time steps\u201d (P37,\n3+ years of experience).\nAnother difference we observed in how people interacted with\nthe assistant stemmed from their view on the role it played in their\n503\n",
    "IUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nRoss et al.\ncollaborative process. Some participants, such as P18, treated it\nmore as a tool by issuing commands rather than asking questions.\nAs quoted earlier, they said, \u201cI didn\u2019t converse with it like a chat\nbot.\u201d P5 described their interaction style similarly: \u201cI found myself\nwanting to type search queries into Socrates, not treating it as a person\nbut as a search tool.\u201d\nIn anticipation that participants would have different orienta-\ntions to the assistant and its role, we asked a question on the post-\ntask survey about the different kinds of roles the assistant might\ntake. These roles generally fell into one of two categories: a tool\norientation (a tool, a reference guide, a content generator, a problem\nsolver), and a social orientation (a collaborator, a colleague, a coach,\nan advisor, a reviewer). Participants rated the extent to which they\nviewed the Programmer\u2019s Assistant in each of these roles on a 4-\npoint scale of extent: Not at all (1), A little (2), Somewhat (3), or A\ngreat deal (4).\n0\n25\n50\n75\n100\nTool\nContent\ngenerator\nReference\nguide\nCollaborator\nProblem\nsolver\nAdvisor\nCoach\nReviewer\nColleague\nRole\nPercentage of participants\nRating\nNot at all\nA little\nSomewhat\nA great deal\nFigure 3: Role orientations. Participants overwhelmingly felt\nthat the assistant\u2019s role was of a tool orientation rather than\na social orientation. The chart shows ratings distributions\nacross 9 different roles the assistant might take, sorted by\nthe percentage of participants who rated the extent of that\nrole as \u201cA great deal.\u201d The leftmost role is of a tool, with\n80.9% of participants rating it as \u201ca great deal.\u201d Following\ntool are content generator (69.0%), reference guide (64.3%),\ncollaborator (40.5%), problem solver (35.7%), advisor (21.4%),\ncoach (19.0%), reviewer (16.7%), and colleague (14.3%).\nWe show participants\u2019 ratings of the assistant\u2019s role in Figure 3.\nDespite the fact that their attitudes toward the assistant overwhelm-\ningly reflected a tool orientation, their behaviors reveal that many\nparticipants actually treated the assistant as a social agent. P6 de-\nscribed how \u201cI felt it like a partner,\u201d and P4 told the assistant, \u201cI\ncould not have solved [the challenge] without your help,\u201d to which\nthe assistant responded, \u201cI\u2019m glad I could help.\u201d\nThe literature on Computers as Social Agents (CASA) helps us\ninterpret this result as it demonstrates how computers are often\ntreated like people [56, 67]. LLM-based conversational agents can\nexacerbate this tendency; as they likely have been trained on ex-\namples of social interaction, they can also respond as social agents.\nIn the conversation logs, we identified participants who inter-\nacted with the assistant in a socially-oriented fashion (the social\norientation codes in Table 1). Twenty participants (47.6%) made at\nleast one socially-oriented utterance. An extreme form of this inter-\naction style can be seen in a snippet from P6\u2019s transcript (Listing 3).\nThe 20 participants with a behaviorally-demonstrated social\norientation did not generally differ in their role ratings from other\nparticipants, except that they rated the assistant as more likely to\nbe an advisor (Fisher\u2019s exact test, two-tailed \ud835\udc5d = .02) or a reviewer\n(Fisher\u2019s exact test, two-tailed \ud835\udc5d = .03). However, they did not differ\nin their overwhelmingly-strong ratings of the tool orientations.\nThus, at least for some participants, there seems to be a dissonance\nin their view of the assistant\u2019s role orientation.\nListing 3: Excerpt from P6\u2019s interaction with the Program-\nmer\u2019s Assistant, in which P6 offers their thanks and congrat-\nulations.\n1\nP6:\nthank you! it's been really nice to code\n2\nwith you! or to have you coding! LOL\n3\nSocrates:\nYou 're welcome!\n4\nP6:\nI hope to see you on air soon , Socrates!\n5\nSocrates:\nI hope so too.\n6\nP6:\ncongrats to you research team! you 've made\n7\na good job!\n8\nSocrates:\nI'm glad you think so.\n9\nP6:\nsorry ... they 've made a good job (well ...\n10\nyou too ... ;-) )\n11\nSocrates:\nThank you.\n12\nP6:\nsee you next time! bye!!\n13\nSocrates:\nGoodbye.\n5.4.2\nMental Capacities. Participants made a number of inferences\nabout the Programmer\u2019s Assistant and its capacities for thought.\nMany participants talked about how the assistant possessed a level\nof \u201cunderstanding\u201d (P6, P8, P11, P18, P32) of \u201cthe context\u201d (P9, P21)\nas well as \u201cmajor concepts\u201d (P9) and \u201cknowledge\u201d (P33). P24 was\namazed by the assistant\u2019s ability to \u201ctake a plain english request\nand interpret it properly.\u201d P7 ascribed intelligence to the assistant,\nsaying, \u201cIt was a lot smarter and trained tha[n] I thought it was.\u201d\nOne participant assumed that the assistant \u201cKeeps improving\nthrough (user) feedback\u201d (P31). Another felt that the assistant was\ncapable of computation: \u201cIt understands the problem... It can calculate\nthe results of a function back\u201d (P8).\nHowever, not all participants were convinced of the assistant\u2019s\nability to understand. P37 questioned the assistant\u2019s limitations: \u201cI\nwonder how far beyond boilerplate it can go and if it works for truly\noriginal problems.\u201d\n5.4.3\nImpact of Conversational Assistance on Work Practices. Many\nparticipants discussed how the Programmer\u2019s Assistant shaped\ntheir work practices on the programming challenges. Overall, par-\nticipants felt that the assistant \u201csaves time\u201d (P10), \u201chelps me code\nfaster\u201d (P34), and would \u201cspeed up my productivity\u201d (P19) because \u201cI\ncould focus on validating and improving the code it generated instead\nof having to write it all from scratch\u201d (P18). P37 remarked that, \u201cIt\nopens a whole new door for fast develpment.\u201d P4 discussed how the\nassistant \u201cwas helpful in staying focused on the code,\u201d although for\nP14, \u201cit took [me] time to get into tempo with the tool.\u201d\nP31 pointed out how the assistant would change the nature of\ntheir work:\n\u201cMy job could focus more on higher level aspects and\ntherefore achieving better (quality) results, besides the\n504\n",
    "The Programmer\u2019s Assistant\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\ntime-to-value... Data science (and dev) becomes a more\ncreative-higher level experience.\u201d (P31)\nOther participants discussed a work process in which the assis-\ntant provided incomplete solutions \u2013 the \u201cbuilding blocks\u201d (P38) or\n\u201cinitial draft of code\u201d (P11) \u2013 upon which they could build. P5 aptly\ndescribed this process:\n\u201cIt\u2019s nice to copy well formulated challenges in natural\nlanguage and have the code generator take its best stab\nat it, then edit to our hearts content.\u201d (P5)\nParticipants felt that human review of the assistant\u2019s responses\nwas necessary because \u201cThe answers provided are generally not\nnovel solutions, often look clunky and non-elegant. There may be\nsome unnecessary code. Basically the code would need to be reviewed\u201d\n(P16). P35 also pointed out how \u201cThe code generator was good but\nyou still have to really check it.\u201d P19 discussed how they would turn\nto the assistant as a first source for support, and only if it wasn\u2019t\nable to help would they then turn to other support tools:\n\u201cThe way I will use it is, I will first us[e] the Program-\nmer\u2019s assistant for most of my cases. Only in certain\ncases where Programmer\u2019s assistant cant answer things\nI will turn up to official documentation or stack overflow.\u201d\n(P19)\nHowever, latency was a factor for interactive use of the assistant\nand participants noticed when the assistant took a long time to\nrespond. P19 remarked, \u201cSometimes it took lot of time, like more than\n5 seconds.\u201d P40 also felt \u201cthe response [was] a little slow sometimes...\nin chat mode I expect faster responses.\u201d As discussed in Section 5.2.1,\nthe assistant took an average of 6.7 seconds (SD = 3.1 seconds)\nto respond to a request, and participants did appreciate when the\nassistant produced rapid responses: \u201cI loved how quick it was able\nto pull up answers to questions I had\u201d (P38).\n5.4.4\nConversational Interaction vs. Other Interaction Models. Al-\nthough our study was not intended to make comparative evalu-\nations with the Copilot tool, we nonetheless asked participants\nwhether they were familiar with Copilot, and if so, to comment on\nhow the two tools compared. We also asked a similar question to\ncompare the assistant with another popular form of programming\nassistance, searching the web (via a search engine like Google, or\na Q&A site like Stack Overflow). In discussing the differences be-\ntween these three tools, we note that the primary differentiator is\ntheir interaction model.\nThe interaction model for the Programmer\u2019s Assistant is clearly\nconversational: users ask questions in natural language and are\nprovided with a response in natural language and/or code. The\ninteraction model of Copilot is reminiscent of direct manipulation\ninterfaces [37], in which the user\u2019s actions in the user interface\ndirectly manipulate an object on the screen. Copilot automatically\nmakes autocompletion suggestions as the user types. This auto-\ncompleted code is directly placed in the source editor; thus, the\nuser\u2019s work is contained entirely within the scope of the object\non which they are working (i.e. the source code), which is how\ndirect manipulation interfaces operate. In web search, users enter\na separate search context (e.g. a search engine accessed within a\nweb browser), type in a natural language query, and then forage\namongst search results to identify relevant items of interest [12, 62].\nWhen a desirable item is found, users must translate it into their\ncode environment (e.g. via copy/paste) and possibly edit it to fit\ntheir existing code.\nWe also note that the Programmer\u2019s Assistant and Copilot both\nutilize the same underlying AI model, Codex [24], which means\nthat the only difference between these tools is the user experience.\nThe extent to which Codex was trained on data from programming-\nrelated Q&A web sites is less clear, but for the purposes of our\nanalysis, we focus our discussion solely on the differences in their\ninteraction models16.\nParticipants reported various benefits and drawbacks of a con-\nversational interaction over a direct manipulation interaction. Fore-\nmost, conversation \u201cfelt very natural\u201d (P21) and \u201cfeels much more\nnatural using Natural Language with the AI\u201d (P39). In addition, P39\nfelt that \u201cthe use cases of Programmers\u2019 Assistant seem more open-\nended.\u201d Many participants were surprised at the variety of tasks the\nassistant was capable of performing, from writing unit tests (P19,\nP36, P37) and documentation (P12, P19, P36, P37) to explaining\nwhat code did (P31, P38) and even answering general-knowledge\nquestions (P31). Again, we note that the Programmer\u2019s Assistant\nutilizes the same underlying model as Copilot, yet the conversa-\ntional interface was able to expose a wider variety of emergent\nbehaviors from the model. Multiple participants explored the limits\nof the assistant\u2019s knowledge and abilities beyond our programming\nchallenges. For example, P37 asked it questions about physics and\nordinary differential equations (\u201cODe\u201d as written by P37), and was\nsurprised by the \u201cversatility of what it could answer.\u201d\n\u201cI asked it some physics and ODe question and the\nanswers, though not complete, included the key parts\nneeded to write that code.\u201d (P37)\nP31 probed the assistant on its knowledge of geography and was\nsurprised when the assistant produced a correct answer.\n\u201cI asked something out of SW engineering domain (ge-\nography) and it replied correctly, also by correctly an-\nswering on my nationality.\u201d (P31)\nFor some participants, the ability to assess the assistant\u2019s re-\nsponse before committing to it (i.e. by inserting assistant-generated\ncode into their editor) was a boon. P15 described how the copy/-\npaste boundary provided them with \u201ca bit more control to ask specific\nquestions about what I wanted and to assess before putting it in my\ncode.\u201d Other participants felt that the copy/paste boundary was\nmore inefficient:\n\u201cI think the main difference is the ability of Copilot to\nsuggest code while you type, what make it faster and\neasier to use. While using the Programmer\u2019s Assistant,\nyou need to go to the chat, ask the question, copy the\n16As an aside, our comparison of direct manipulation, search, and conversational inter-\naction models is reminiscent of historical comparisons of text-based and graphical user\ninterfaces [54, 63]. Each modality was shown to have advantages and disadvantages.\nFor example, text-only interfaces can provide accessibility [54, 78] and productivity [53]\nadvantages, whereas graphical user interfaces provide greater discoverability [10, 88].\nSome researchers explored ways to bridge the two interaction modes, such as by\ndeveloping GUI wrappers for command line programs [53, 94] or by developing tools\nthat converted GUI activities into procedural descriptions [55]. Our view is that similar\nbridges can be constructed between direct manipulation, search, and conversational\nmodels of interaction; a user\u2019s interaction with an LLM need not be constrained to a\nsingle interaction model.\n505\n",
    "IUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nRoss et al.\ncode (or rephrase the question if it was not understood\nby the agent), and edit it to match your code.\u201d (P3)\nA large number of participants felt that the conversational inter-\naction was faster than web search (P1, P6, P7, P10, P11, P12, P16,\nP17, P18, P20, P24, P29, P30, P33, P36, P37, P42) because of its ability\nto provide \u201creal-time responses\u201d (P32) that can be \u201capplied exactly to\nyour code\u201d (P33) without having to \u201cparse through lots of text... to get\nwhat you need\u201d (P15). In addition, the assistant provided \u201cMUCH\nfaster, better responses\u201d (P17) that were \u201cmuch more relevant to the\nproblems\u201d (P34) and \u201csimple [and] succinct\u201d (P9), without having to\n\u201csort through answers on your own or read documentation\u201d (P9) or\n\u201clook at many posts before finding the relevant one\u201d (P18).\nDespite these benefits, some participants felt that the assistant\nmight not work well for \u201cmore specific and difficult problems on\na bigger scale\u201d as compared to web search. P9 felt that \u201cthe data\n[of the Programmer\u2019s Assistant] wasn\u2019t as rich\u201d as the web. Other\nparticipants felt that the assistant lacked the \u201cmultiple answers\u201d (P9)\nand \u201crich social commentary\u201d (P19) that accompanies answers on\nQ&A sites:\n\u201cI like to see the different versions proposed on stack\noverflow and the commentary of what makes one solu-\ntion better than another in a given situation.\u201d (P27)\nSome participants promoted a more balanced view that there\nisn\u2019t a single mode of interaction superior to all others. P19 felt that\nweb search would be a fallback when the assistant failed to answer\na question. P39 described how search could be integrated with the\nconversational interaction:\n\u201cI think both options should exist: people should be able\nto input their queries like a search bar AND also give\ntheir question as if in conversation.\u201d (P39)\n6\nDISCUSSION\n6.1\nValue of Conversational Interaction\nWe began our research by asking the question of whether con-\ntemporary developments in code-fluent LLMs could sufficiently\nsupport a conversational programming assistant. We believe that\nour work has demonstrated that they can. Clearly, the Program-\nmer\u2019s Assistant was viewed by our participants as a useful tool that\nprovided real value \u2013 so much so that many participants explic-\nitly requested or expressed the desire to use it in their own work.\nHowever, how much of this value was derived from the model itself\nand its ability to produce high-quality responses to programming\nquestions, versus from participants\u2019 ability to conduct extended\nconversational interactions grounded in their actual source code?\nWe believe that both of these constituent aspects were valuable.\nIndeed, many participants commented on their surprise and satis-\nfaction with the quality of the assistant\u2019s responses (Section 5.2.3).\nHowever, participants also valued the conversational interactions\nthat they had with the assistant. In the event logs, we saw evi-\ndence that participants were leveraging conversational context to\nask follow-up questions as well as leveraging code context by ask-\ning about their code selections (Section 5.3.2). Many participants\nreported that they would find the tool less valuable if the conver-\nsational interaction were removed (Section 5.3.2). Further, conver-\nsation seemed to provide unique value beyond other interaction\nmodels (direct manipulation and search) because of its embedded-\nness in the UI and its ability to surface emergent behaviors of the\nmodel (Section 5.4.4).\nWe do not believe that these different interaction models are\nin competition and we agree with P39\u2019s assessment that assistive\ntools can be built using a plethora of different interaction models.\nFor use cases in which a model is known to produce high-quality\nresults (e.g. code autocompletion for Codex), a direct manipulation\ninterface seems wholly appropriate as it would provide a discov-\nerable and predictable way of invoking the model to produce a\nknown type of result. However, direct manipulation interfaces may\nbe less ideal for surfacing the emergent behaviors of a foundation\nmodel [14], and thus natural language interaction may be more suit-\nable. Many popular text-to-image models, such as DALL-E 2 [66]\nand Stable Diffusion [72], operate in a one-shot fashion, in which\nthe user specifies a prompt, clicks a button, and gets results. Our\nstudy demonstrates how the additional contextual layers of con-\nversational history and the artifact-under-development provide\nadditional value to the co-creative process.\n6.2\nToward Human-AI Synergy\nThe aim of human-centered AI is to \u201cenable[] people to see, think,\ncreate, and act in extraordinary ways, by combining potent user\nexperiences with embedded AI methods to support services that\nusers want\u201d [82]. Building upon this definition, Rezwana and Maher\n[69] posit that, \u201cIn a creative collaboration, interaction dynamics,\nsuch as turn-taking, contribution type, and communication, are the\ndriving forces of the co-creative process. Therefore the interaction\nmodel is a critical and essential component for effective co-creative\nsystems.\u201d [69]. They go on to note that, \u201cThere is relatively little\nresearch about interaction design in the co-creativity field, which\nis reflected in a lack of focus on interaction design in many existing\nco-creative systems.\u201d\nOur study begins to address this gap. While many co-creative\nsystems examine casual tasks or experimental activities (e.g., Spoto\nand Oleynik [87]), our focus was on the co-creative practice of\nprogramming. Our goal was to understand peoples\u2019 attitudes to-\nward a conversational programming assistant, akin to Wang et al.\u2019s\nexamination of data scientists\u2019 attitudes toward automated data\nscience technologies [99]. We found that, despite an initial level of\nskepticism, participants felt that a conversational assistant would\nprovide value by improving their productivity (Section 5.4.3). How-\never, further work is needed to assess the extent to which this type\nof assistance provides measurable productivity increases.\nCampero et al. [19] conducted a survey of papers published in\n2021 that examined human-AI synergy, the notion that a human-AI\nteam can accomplish more by working together than either party\ncould accomplish working alone. They found mixed results, with\nno clear consensus emerging on how to design human-centered AI\nsystems that can guarantee positive synergy. Summarizing from\ntheir discussion,\n\u201cPerhaps achieving substantial synergies among people\nand computers is harder than many people think. Per-\nhaps it requires... new ways of configuring groups that\ninclude people and computers. And perhaps it needs\n506\n",
    "The Programmer\u2019s Assistant\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nmore systematic, focused attention from researchers\nthan it has, so far, received.\u201d [19, p.9]\nWe believe such evaluations of human-AI synergy should go\nbeyond one-shot performance measures. As implied by many of the\nuses cases listed by Seeber et al. [80], human-centered AI systems\nare often deployed in socio-organizational contexts that require\nlongitudinal use [20, 41, 43], such as product design [93], game de-\nsign [4], and engineering [20, Section 3.2.2]. Thus, we would expect\nthat over time and through interaction with each other, human-AI\nteams would improve their performance through a mutual learning\nprocess.\nEvidence for this process surfaced in our study when participants\ndescribed how they could improve their programming skills by\ninteracting with the assistant (Section 5.3.3). We assert that the\nlearning should operate in both directions: not only should people\nimprove their programming skills, but the model itself can also\nimprove based on peoples\u2019 interactions with it. For example, when\nthe assistant provides a code example to the user, and the user\ntakes that example and edits it, those edits constitute feedback that\ncan be used to further fine-tune the model. In addition, through\nlongitudinal use, we believe that human and AI partners can create\nreciprocal representations of one another \u2013 i.e., the human is likely\nto create a mental model of the AI, and the AI may be engineered to\ndevelop a user model for each of its human users [30, 48, 79]. Such a\npair of models is often described as Mutual Theory of Mind [29, 100].\nThis type of capability raises the possibility of personalizing and\nadapting an assistant to the strengths and needs of individual users.\nWith such models, an assistant that knows a user is learning a\nprogramming language could provide natural language explana-\ntions alongside code outputs, whereas an assistant that knows a\nuser is strongly skilled in a programming language might shorten\nor omit those explanations. Similarly, users are likely to update\ntheir mental models of the AI with more experience. We believe the\nspace for exploring how these reciprocal models impact human-AI\nsynergy is rich, and we encourage additional work in this area.\nHuman-centered AI systems that are designed to combine and\nsynergize the distinct skills of humans and AI models cannot suc-\nceed if they diminish the human skills upon which they depend.\nWell-designed human-centered AI systems develop new and com-\nplementary skills for both the human and AI constituents [82, 83],\nand we believe that mutual learning may address concerns that the\nwide deployment and use of AI systems will result in a de-skilling\nof the workforce [77, 108].\nUltimately, the design decisions that go into an interactive AI\nsystem have ethical implications. Our design attempts to augment\nthe user\u2019s knowledge and skills by presenting help on demand,\ncouched in non-authoritative suggestions, which leaves the user\nfirmly in control and ultimately responsible for the work product.\n6.3\nOpportunities for Future Research\nOur work highlights many interesting avenues for future enhance-\nments that could be made to LLM-based conversational assistants\nsuch as our Programmer\u2019s Assistant, as well as future human-\ncentered research on LLM-based conversational assistance.\nOur work employed a code-fluent model that was not specifi-\ncally designed to handle conversational interaction. Fine-tuning\nthe underlying LLM for conversational interaction, such as what\nhas been done with Lamda [91], is one opportunity to improve the\nassistant\u2019s performance. Another opportunity is to align the lan-\nguage model to follow the desiderata proposed by Askell et al. [11]\nand described by Ouyang et al. as, \u201chelpful (they should help the\nuser solve their task), honest (they shouldn\u2019t fabricate information\nor mislead the user), and harmless (they should not cause physical,\npsychological, or social harm to people or the environment)\u201d [61,\np.2]. Glaese et al. [33] propose a slightly different desiderata of\n\u201ccorrect\u201d instead of \u201chonest,\u201d which may be more applicable to the\nsoftware engineering domain, as the ability to produce correct code\nand correct answers about code are both important properties of a\nconversational programming assistant.\nCombining LLMs with search-based approaches to establish ad-\nditional context for the model, such as AlphaCode [44] has done,\nmay also result in more capable systems. These \u201csearches\u201d need not\nbe limited to textual sources, but could be conducted over appro-\npriate semantic stores (e.g. a knowledge graph) and take advantage\nof explicit semantic reasoning services, resulting in an integration\nof symbolic and neural approaches. Further, allowing for \u201cinternal\ndeliberation\u201d of the type shown in Nye et al. [59] could result in\nbetter-reasoned results, as well as better explanations and justifica-\ntions.\nAnother avenue for improvement involves the prompt used to\nconfigure the assistant (Appendix D). Just as the prompt for each\nsuccessive interaction is modified by the growth of the conversa-\ntional transcript, there is no requirement that the initial prompt be\nstatic. It too can be specialized to incorporate aspects of a user model,\nenabling the realization of a Mutual Theory of Mind [29, 100]. Pro-\nviding better UX affordances for visualizing and manipulating the\nactive contexts \u2013 code and conversation \u2013 could provide users with\nmore control over which information contributes to the generation\nof the assistant\u2019s response.\nOur participants clearly indicated that they were interested in\nhaving an assistant that behaved more proactively, in contrast to\nour deliberate design of an assistant that never takes conversational\ninitiative. A more proactive assistant would be able to interrupt or\nremind a user when necessary [23], yet this characteristic raises\nmany challenging issues. How can we calibrate the threshold for\nsuch interruptions? How can users tune the assistant to deliver only\nthose interruptions that the they would find useful (e.g., [28, 81])?\nHow can we help users to regain their prior context after dealing\nwith an interruption (e.g. [89])? Should an assistant be used to\npersuade or nudge the user (e.g. [35])? Who should determine\nthe topic, frequency, and insistence of such persuasion attempts\n(e.g. [52, 85])? Should users have the ability to moderate or defeat\nattempted persuasions, or should those decisions be left to the\norganization?\nFinally, we explored the different kinds of role orientations our\nparticipants had toward the assistant and found that participants\nvaried in their views of it as a tool versus a social agent (e.g. collabo-\nrator or colleague). We posit that peoples\u2019 effectiveness in working\nwith an AI system may be influenced by their role orientation, and\nwe encourage future research in this area.\n507\n",
    "IUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nRoss et al.\n7\nCONCLUSION\nWe developed a prototype system, the Programmer\u2019s Assistant, in\norder to assess the utility of a conversational assistant in a software\nengineering context. The assistant was implemented using a state-\nof-the-art code-fluent large language model, Codex [24], and was\ncapable of generating both code and natural language responses to\nuser inquiries. We further used the prompting mechanism of the\nmodel to set up a conversational interaction in which the model\nuses the conversational history, plus the user\u2019s current utterance,\nin order to generate a response. In this way, users are able to ask\nfollow-up questions in the chat that reference prior utterances and\nresponses. We incorporated the conversational assistant into a code\nediting environment, enabling the conversation to be grounded in\nthe context of the user\u2019s source code.\nWe evaluated this system with 42 participants with varied levels\nof programming skill, and their quantitative and qualitative feed-\nback, coupled with their usage of the system, demonstrated the\nvaried, and sometimes emergent, types of assistance it was able to\nprovide. Many participants noted the high quality of the conversa-\ntional responses, including the assistant\u2019s ability to produce code,\nexplain code, answer general programming questions, and even\nanswer general knowledge questions. Participants felt this type of\nassistance would aid their productivity, and they drew meaningful\ncontrasts between the conversational style of interaction with other\ntools that employ a direct manipulation or search-based interaction\nmodel.\nOur study motivates the use of conversational styles of inter-\naction with large language models by showing how they enable\nemergent behaviors in a co-creative context. The Programmer\u2019s\nAssistant did not always generate perfect code or correct answers;\nnonetheless, participants in our study had an overall positive ex-\nperience working with it on a variety of programming challenges.\nWe believe that our work takes us one step closer to realizing the\nvision of human-centered AI: learning how to design systems that\nmaximize the synergy in human-AI collaborations.\nACKNOWLEDGMENTS\nWe would like to thank Socrates for his tireless assistance during\nthe user study, as well as for suggesting the title of this paper based\non its abstract.\nREFERENCES\n[1] Rabe Abdalkareem, Emad Shihab, and Juergen Rilling. 2017. What Do Developers\nUse the Crowd For? A Study Using Stack Overflow. IEEE Software 34, 2 (2017),\n53\u201360. https://doi.org/10.1109/MS.2017.31\n[2] Eleni Adamopoulou and Lefteris Moussiades. 2020. Chatbots: History, technol-\nogy, and applications. Machine Learning with Applications 2 (2020), 100006.\n[3] Daniel Adiwardana, Minh-Thang Luong, David R. So, Jamie Hall, Noah Fiedel,\nRomal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,\nand Quoc V. Le. 2020. Towards a Human-like Open-Domain Chatbot.\n[4] Safinah Ali, Nisha Elizabeth Devasia, and Cynthia Breazeal. 2022. Escape! Bot:\nSocial Robots as Creative Problem-Solving Partners. In Creativity and Cognition.\n275\u2013283.\n[5] Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. 2018.\nA survey of machine learning for big code and naturalness. ACM Computing\nSurveys (CSUR) 51, 4 (2018), 1\u201337.\n[6] Irene Alvarado, Idan Gazit, and Amelia Wattenberger. 2022. GitHub Next |\nGitHub Copilot Labs. https://githubnext.com/projects/copilot-labs/\n[7] Hikari Ando, Rosanna Cousins, and Carolyn Young. 2014. Achieving saturation\nin thematic analysis: Development and refinement of a codebook. Comprehensive\nPsychology 3 (2014), 03\u2013CP.\n[8] Craig Anslow, Stuart Marshall, James Noble, and Robert Biddle. 2013. Sourcevis:\nCollaborative software visualization for co-located environments. In 2013 First\nIEEE Working Conference on Software Visualization (VISSOFT). IEEE, 1\u201310.\n[9] Zahra Ashktorab, Michael Desmond, Josh Andres, Michael Muller, Naren-\ndra Nath Joshi, Michelle Brachman, Aabhas Sharma, Kristina Brimijoin, Qian\nPan, Christine T Wolf, et al. 2021. AI-Assisted Human Labeling: Batching for\nEfficiency without Overreliance. Proceedings of the ACM on Human-Computer\nInteraction 5, CSCW1 (2021), 1\u201327.\n[10] Catherine A Ashworth. 1996. GUI Users have trouble using graphic conventions\non novel tasks. In Conference Companion on Human Factors in Computing Systems.\n75\u201376.\n[11] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom\nHenighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. 2021.\nA general language assistant as a laboratory for alignment. arXiv preprint\narXiv:2112.00861 (2021).\n[12] Leif Azzopardi, Paul Thomas, and Nick Craswell. 2018. Measuring the utility of\nsearch engine result pages: an information foraging based measure. In The 41st\nInternational ACM SIGIR conference on research & development in information\nretrieval. 605\u2013614.\n[13] Shraddha Barke, Michael B James, and Nadia Polikarpova. 2022. Grounded\nCopilot: How Programmers Interact with Code-Generating Models. arXiv\npreprint arXiv:2206.15000 (2022).\n[14] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora,\nSydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma\nBrunskill, et al. 2021. On the opportunities and risks of foundation models.\narXiv preprint arXiv:2108.07258 (2021).\n[15] Joel Brandt, Mira Dontcheva, Marcos Weskamp, and Scott R Klemmer. 2010.\nExample-centric programming: integrating web search into the development\nenvironment. In Proceedings of the SIGCHI Conference on Human Factors in\nComputing Systems. 513\u2013522.\n[16] Virginia Braun and Victoria Clarke. 2022. Common challenges in Thematic\nAnalysis and how to avoid them. Retrieved August 11 2022 from https://youtu.\nbe/tpWLsckpM78\n[17] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Ka-\nplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,\nTom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\nGray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage Models are\nFew-Shot Learners. In Advances in Neural Information Processing Systems,\nH. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33.\nCurran Associates, Inc., 1877\u20131901. https://proceedings.neurips.cc/paper/2020/\nfile/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf\n[18] Sallyann Bryant, Pablo Romero, and Benedict\" du Boulay. 2006. The Collabora-\ntive Nature of Pair Programming. In Extreme Programming and Agile Processes\nin Software Engineering, Pekka Abrahamsson, Michele Marchesi, and Giancarlo\nSucci (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 53\u201364.\n[19] Andres Campero, Michelle Vaccaro, Jaeyoon Song, Haoran Wen, Abdullah\nAlmaatouq, and Thomas W Malone. 2022. A Test for Evaluating Performance\nin Human-Computer Systems. arXiv preprint arXiv:2206.12390 (2022).\n[20] Gaetano Cascini, Yukari Nagai, Georgi V Georgiev, Jader Zelaya, Niccol\u00f2 Be-\ncattini, Jean-Fran\u00e7ois Boujut, Hernan Casakin, Nathan Crilly, Elies Dekoninck,\nJohn Gero, et al. 2022. Perspectives on design creativity and innovation research:\n10 years later. , 30 pages.\n[21] Stephen Cass. 2022. Top Programming Languages 2022. IEEE Spectrum (23 Aug\n2022). https://spectrum.ieee.org/top-programming-languages-2022\n[22] Cristina Catalan Aguirre, Nuria Gonzalez Castro, Carlos Delgado Kloos, Carlos\nAlario-Hoyos, and Pedro Jos\u00e9 Mu\u00f1oz Merino. 2021. Conversational agent for\nsupporting learners on a MOOC on programming with Java. (2021).\n[23] Ana Paula Chaves and Marco Aurelio Gerosa. 2021. How should my chatbot\ninteract? A survey on social characteristics in human\u2013chatbot interaction design.\nInternational Journal of Human\u2013Computer Interaction 37, 8 (2021), 729\u2013758.\n[24] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared\nKaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex\nRay, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,\nAlethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe\nTillet, Felipe Petroski Such, David W. Cummings, Matthias Plappert, Fotios\nChantzis, Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol,\nIgor Babuschkin, S. Arun Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua\nAchiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight,\nMiles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario\nAmodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evalu-\nating a Large Language Models Trained on Code.\n[25] Li-Te\nCheng,\nR.B.\nDe\nSouza,\nSusanne\nHupfer,\nJohn\nPatter-\nson,\nand\nSteven\nRoss.\n2003.\nBuilding\nCollaboration\ninto\nIDEs:\nEdit>Compile>Run>Debug>Collaborate? Queue 1, 9 (2003).\n508\n",
    "The Programmer\u2019s Assistant\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\n[26] Carl Cook, Warwick Irwin, and Neville Churcher. 2005. A user evaluation\nof synchronous collaborative software engineering tools. In 12th Asia-Pacific\nSoftware Engineering Conference (APSEC\u201905). IEEE, 6\u2013pp.\n[27] Claudio Le\u00f3n de la Barra, Broderick Crawford, Ricardo Soto, Sanjay Misra,\nand Eric Monfroy. 2013. Agile Software Development: It Is about Knowledge\nManagement and Creativity. In Computational Science and Its Applications \u2013\nICCSA 2013, Beniamino Murgante, Sanjay Misra, Maurizio Carlini, Carmelo M.\nTorre, Hong-Quang Nguyen, David Taniar, Bernady O. Apduhan, and Osvaldo\nGervasi (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 98\u2013113.\n[28] Uri Dekel and Steven Ross. 2004. Eclipse as a platform for research on in-\nterruption management in software development. In Proceedings of the 2004\nOOPSLA workshop on Eclipse Technology eXchange (Vancouver, British Columbia,\nCanada), Michael G. Burke (Ed.). ACM, 12\u201316.\n[29] Bobbie Eicher, Kathryn Cunningham, Sydni Peterson Marissa Gonzales, and\nAshok Goel. 2017. Toward mutual theory of mind as a foundation for co-creation.\nIn International Conference on Computational Creativity, Co-Creation Workshop.\n[30] Stephen M Fiore, Eduardo Salas, and Janis A Cannon-Bowers. 2001. Group\ndynamics and shared mental model development. How people evaluate others in\norganizations 234 (2001).\n[31] Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides. 1995. Design\npatterns: elements of reusable object-oriented software. Addison-Wesley.\n[32] GitHub, Inc. 2022. GitHub copilot \u00b7 your AI pair programmer. Retrieved August\n5, 2022 from https://github.com/features/copilot/\n[33] Amelia Glaese, Nat McAleese, Maja Tr\u0119bacz, John Aslanides, Vlad Firoiu,\nTimo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe\nThacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, Ramona\nComanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie\nChen, Doug Fritz, Jaume Sanchez Elias, Richard Green, So\u0148a Mokr\u00e1, Nicholas\nFernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac,\nJohn Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and\nGeoffrey Irving. 2022. Improving alignment of dialogue agents via targeted\nhuman judgements. https://arxiv.org/abs/2209.14375\n[34] Stephanie Glen. 2022.\nChatGPT writes code, but won\u2019t replace devel-\nopers.\nTechTarget (14 12 2022).\nRetrieved 20-Jan-2023 from https:\n//www.techtarget.com/searchsoftwarequality/news/252528379/ChatGPT-\nwrites-code-but-wont-replace-developers\n[35] Samuel Holmes, Anne Moorhead, Raymond Bond, Huiru Zheng, Vivien Coates,\nand Mike McTear. 2018. WeightMentor: a new automated chatbot for weight\nloss maintenance. In Proceedings of the 32nd International BCS Human Computer\nInteraction Conference 32. 1\u20135.\n[36] Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2020. Deep code comment\ngeneration with hybrid lexical and syntactical information. Empirical Software\nEngineering 25, 3 (2020), 2179\u20132217.\n[37] Edwin L Hutchins, James D Hollan, and Donald A Norman. 1985. Direct manip-\nulation interfaces. Human\u2013computer interaction 1, 4 (1985), 311\u2013338.\n[38] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.\nSummarizing source code using a neural attention model. In Proceedings of the\n54th Annual Meeting of the Association for Computational Linguistics (Volume 1:\nLong Papers). 2073\u20132083.\n[39] Andreas Jedlitschka and Markus Nick. 2003. Software Engineering Knowledge\nRepositories. Springer Berlin Heidelberg, Berlin, Heidelberg, 55\u201380.\n[40] Eirini Kalliamvakou. 2022.\nResearch: Quantifying github copilot\u2019s impact\non developer productivity and happiness.\nhttps://github.blog/2022-09-07-\nresearch-quantifying-github-copilots-impact-on-developer-productivity-\nand-happiness/\n[41] Anna Kantosalo et al. 2019. Human-Computer Co-Creativity: Designing, Evalu-\nating and Modelling Computational Collaborators for Poetry Writing. (2019).\n[42] Sandeep Kaur Kuttal, Bali Ong, Kate Kwasny, and Peter Robe. 2021. Trade-\nOffs for Substituting a Human with an Agent in a Pair Programming Context:\nThe Good, the Bad, and the Ugly. In Proceedings of the 2021 CHI Conference on\nHuman Factors in Computing Systems (Yokohama, Japan) (CHI \u201921). Association\nfor Computing Machinery, New York, NY, USA, Article 243, 20 pages.\n[43] Lauramaria Laine. 2021. Exploring Advertising Creatives\u2019 Attitudes Towards\nHuman-AI Collaboration. (2021).\n[44] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser,\nR\u00e9mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago,\nand et al. 2022. Competition-level code generation with AlphaCode.\nhttps:\n//arxiv.org/abs/2203.07814\n[45] Yaosheng Lou and Qi Sun. 2021. Over-reliance on database: A case study of\nusing web of science. Human Behavior and Emerging Technologies 3, 3 (2021),\n454\u2013459.\n[46] David Lyell and Enrico Coiera. 2017. Automation bias and verification complex-\nity: a systematic review. Journal of the American Medical Informatics Association\n24, 2 (2017), 423\u2013431.\n[47] Wendy E Mackay and Anne-Laure Fayard. 1997. HCI, natural science and\ndesign: a framework for triangulation across disciplines. In Proceedings of the\n2nd conference on Designing interactive systems: processes, practices, methods, and\ntechniques. 223\u2013234.\n[48] John E Mathieu, Tonia S Heffner, Gerald F Goodwin, Eduardo Salas, and Janis A\nCannon-Bowers. 2000. The influence of shared mental models on team process\nand performance. Journal of applied psychology 85, 2 (2000), 273.\n[49] Cade Metz. 2022. Meet GPT-3. It Has Learned to Code (and Blog and Ar-\ngue). (Published 2020). https://www.nytimes.com/2020/11/24/science/artificial-\nintelligence-ai-gpt3.html\n[50] Robert J. Moore and Raphael Arar. 2019. Conversational UX Design: A Practi-\ntioner\u2019s Guide to the Natural Conversation Framework. Association for Computing\nMachinery, New York, NY, USA.\n[51] Ekaterina A Moroz, Vladimir O Grizkevich, and Igor M Novozhilov. 2022. The\nPotential of Artificial Intelligence as a Method of Software Developer\u2019s Produc-\ntivity Improvement. In 2022 Conference of Russian Young Researchers in Electrical\nand Electronic Engineering (ElConRus). IEEE, 386\u2013390.\n[52] Michael Muller, Stevean Ross, Stephanie Houde, Mayank Agarwal, Fernando\nMartinez, John Richards, Kartik Talamadupula, and Justin D Weisz. 2022. Drink-\ning Chai with Your (AI) Programming Partner: A Design Fiction about Gener-\native AI for Software Engineering. HAI-GEN Workshop at IUI 2022: 3rd Work-\nshop on Human-AI Co-Creation with Generative Models (2022).\nhttps://hai-\ngen.github.io/2022/\n[53] Sandra R Murillo and J Alfredo S\u00e1nchez. 2014. Empowering interfaces for system\nadministrators: Keeping the command line in mind when designing GUIs. In\nProceedings of the XV International Conference on Human Computer Interaction.\n1\u20134.\n[54] Elizabeth D Mynatt and Gerhard Weber. 1994. Nonvisual presentation of graph-\nical user interfaces: contrasting two approaches. In Proceedings of the SIGCHI\nconference on Human factors in computing systems. 166\u2013172.\n[55] Alok Mysore and Philip J Guo. 2017. Torta: Generating mixed-media gui and\ncommand-line app tutorials using operating-system-wide activity tracing. In\nProceedings of the 30th Annual ACM Symposium on User Interface Software and\nTechnology. 703\u2013714.\n[56] C. Nass and Y. Moon. 2000. Machines and Mindlessness: Social Responses to\nComputers. Journal of Social Issues 56, 1 (2000), 81\u2013103.\n[57] Nhan Nguyen and Sarah Nadi. 2022. An Empirical Evaluation of GitHub Copi-\nlot\u2019s Code Suggestions. In 2022 IEEE/ACM 19th International Conference on\nMining Software Repositories (MSR). IEEE, 1\u20135.\n[58] Martin Nordio, H Estler, Carlo A Furia, Bertrand Meyer, et al. 2011. Collaborative\nsoftware development on the web. arXiv preprint arXiv:1105.0768 (2011).\n[59] Maxwell Nye, Anders Andreassen, Guy Gur-Ari, Henryk Witold Michalewski, Ja-\ncob Austin, David Bieber, David Martin Dohan, Aitor Lewkowycz, Maarten Paul\nBosma, David Luan, Charles Sutton, and Augustus Odena. 2021. Show Your\nWork: Scratchpads for Intermediate Computation with Language Models.\nhttps://arxiv.org/abs/2112.00114.\n[60] OpenAI. 2022. ChatGPT: Optimizing Language Models for Dialogue. OpenAI\nBlog (30 11 2022). Retrieved 20-Jan-2023 from https://openai.com/blog/chatgpt/\n[61] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda\nAskell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training\nlanguage models to follow instructions with human feedback.\nhttps://arxiv.\norg/abs/2203.02155\n[62] Peter Pirolli and Stuart Card. 1999. Information foraging. Psychological review\n106, 4 (1999), 643.\n[63] Larry Press. 1990. Personal computing: Windows, DOS and the MAC. Commun.\nACM 33, 11 (1990), 19\u201326.\n[64] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya\nSutskever, et al. 2019. Language Models are Unsupervised Multitask Learners.\n[65] Alvin Rajkomar, Jeffrey Dean, and Isaac Kohane. 2019. Machine learning in\nmedicine. New England Journal of Medicine 380, 14 (2019), 1347\u20131358.\n[66] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.\n2022. Hierarchical text-conditional image generation with clip latents. arXiv\npreprint arXiv:2204.06125 (2022).\n[67] B. Reeves and C.I. Nass. 1996. The Media Equation: How People Treat Computers,\nTelevision, and New Media Like Real People and Places. CSLI Publications.\n[68] Mawarny Md Rejab, James Noble, and George Allan. 2014. Distributing Expertise\nin Agile Software Development Projects. In 2014 Agile Conference. 33\u201336.\n[69] Jeba Rezwana and Mary Lou Maher. 2021. COFI: A Framework for Modeling\nInteraction in Human-AI Co-Creative Systems.. In ICCC. 444\u2013448.\n[70] Charles H. Rich and Richard C. Waters. 1990. The Programmer\u2019s Apprentice.\nAddison-Wesley Publishing Company, Reading, MA.\n[71] Peter Robe and Sandeep Kaur Kuttal. 2022. Designing PairBuddy\u2014A Conver-\nsational Agent for Pair Programming. ACM Transactions on Computer-Human\nInteraction (TOCHI) 29, 4 (2022), 1\u201344.\n[72] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn\nOmmer. 2022. High-resolution image synthesis with latent diffusion models. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion. 10684\u201310695.\n509\n",
    "IUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nRoss et al.\n[73] Steven Ross, Elizabeth Brownholtz, and Robert Armes. 2004.\nA Multiple-\nApplication Conversational Agent. In Proceedings of the 9th International Con-\nference on Intelligent User Interfaces (Funchal, Madeira, Portugal) (IUI \u201904). Asso-\nciation for Computing Machinery, New York, NY, USA, 319\u2013321.\n[74] Steven Ross, Elizabeth Brownholtz, and Robert Armes. 2004. Voice User Interface\nPrinciples for a Conversational Agent. In Proceedings of the 9th International\nConference on Intelligent User Interfaces (Funchal, Madeira, Portugal) (IUI \u201904).\nAssociation for Computing Machinery, New York, NY, USA, 364\u2013365.\n[75] Baptiste Roziere, Marie-Anne Lachaux, Lowik Chanussot, and Guillaume Lample.\n2020. Unsupervised Translation of Programming Languages. In Advances in\nNeural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell,\nM.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 20601\u201320611.\n[76] Harvey Sacks. 1984. Notes on methodology. In Structures of Social Action:\nStudies in Conversation Analysis, John Heritage and J. Maxwell Atkinson (Eds.).\nCambridge University Press, Cambridge, 2\u201327.\n[77] Nithya Sambasivan and Rajesh Veeraraghavan. 2022. The Deskilling of Domain\nExpertise in AI Development. In CHI Conference on Human Factors in Computing\nSystems. 1\u201314.\n[78] Harini Sampath, Alice Merrick, and Andrew Macvean. 2021. Accessibility of\ncommand line interfaces. In Proceedings of the 2021 CHI Conference on Human\nFactors in Computing Systems. 1\u201310.\n[79] Matthias Scheutz, Scott A DeLoach, and Julie A Adams. 2017. A framework for\ndeveloping and using shared mental models in human-agent teams. Journal of\nCognitive Engineering and Decision Making 11, 3 (2017), 203\u2013224.\n[80] Isabella Seeber, Eva Bittner, Robert O Briggs, Triparna De Vreede, Gert-Jan\nDe Vreede, Aaron Elkins, Ronald Maier, Alexander B Merz, Sarah Oeste-Rei\u00df,\nNils Randrup, et al. 2020. Machines as teammates: A research agenda on AI in\nteam collaboration. Information & management 57, 2 (2020), 103174.\n[81] Shilad Sen, Werner Geyer, Michael Muller, Marty Moore, Beth Brownholtz, Eric\nWilcox, and David R Millen. 2006. FeedMe: a collaborative alert filtering system.\nIn Proceedings of the 2006 20th anniversary conference on Computer supported\ncooperative work. 89\u201398.\n[82] Ben Shneiderman. 2020. Human-centered artificial intelligence: Three fresh\nideas. AIS Transactions on Human-Computer Interaction 12, 3 (2020), 109\u2013124.\n[83] Ben Shneiderman. 2022. Human-Centered AI. Oxford University Press.\n[84] Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller,\nMegan Ung, Moya Chen, Kushal Arora, Joshua Lane, et al. 2022. BlenderBot 3:\na deployed conversational agent that continually learns to responsibly engage.\narXiv preprint arXiv:2208.03188 (2022).\n[85] Michael Skirpan and Casey Fiesler. 2018. Ad empathy: A design fiction. In\nProceedings of the 2018 ACM Conference on Supporting Groupwork. 267\u2013273.\n[86] Diomidis Spinellis. 2012. Git. IEEE Software 29, 3 (2012), 100\u2013101.\nhttps:\n//doi.org/10.1109/MS.2012.61\n[87] Angie Spoto and Natalia Oleynik. 2017. Library of Mixed-Initiative Creative\nInterfaces. Retrieved 19-Jun-2021 from http://mici.codingconduct.cc/\n[88] Ayushi Srivastava, Shivani Kapania, Anupriya Tuli, and Pushpendra Singh.\n2021. Actionable UI Design Guidelines for Smartphone Applications Inclusive\nof Low-Literate Users. Proceedings of the ACM on Human-Computer Interaction\n5, CSCW1 (2021), 1\u201330.\n[89] Margaret-Anne Storey and Alexey Zagalsky. 2016. Disrupting developer produc-\ntivity one bot at a time. In Proceedings of the 2016 24th ACM SIGSOFT international\nsymposium on foundations of software engineering. 928\u2013931.\n[90] Kartik Talamadupula. 2021. Applied AI matters: AI4Code: applying artificial\nintelligence to source code. AI Matters 7, 1 (2021), 18\u201320.\n[91] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kul-\nshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, and\net al. 2022.\nLAMDA: Language models for dialog applications.\nhttps:\n//arxiv.org/abs/2201.08239\n[92] Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, Shao Kun Deng, and Neel\nSundaresan. 2020. Unit Test Case Generation with Transformers and Focal\nContext. arXiv preprint arXiv:2009.05617 (2020).\n[93] Severi Uusitalo, Anna Kantosalo, Antti Salovaara, Tapio Takala, and Christian\nGuckelsberger. 2022. Co-creative Product Design with Interactive Evolutionary\nAlgorithms: A Practice-Based Reflection. In International Conference on Compu-\ntational Intelligence in Music, Sound, Art and Design (Part of EvoStar). Springer,\n292\u2013307.\n[94] Priyan Vaithilingam and Philip J Guo. 2019. Bespoke: Interactively synthesizing\ncustom GUIs from command-line applications by demonstration. In Proceedings\nof the 32nd annual ACM symposium on user interface software and technology.\n563\u2013576.\n[95] Priyan Vaithilingam, Tianyi Zhang, and Elena L. Glassman. 2022. Expectation\nvs. Experience: Evaluating the Usability of Code Generation Tools Powered by\nLarge Language Models. In Extended Abstracts of the 2022 CHI Conference on\nHuman Factors in Computing Systems (New Orleans, LA, USA) (CHI EA \u201922).\nAssociation for Computing Machinery, New York, NY, USA, Article 332, 7 pages.\nhttps://doi.org/10.1145/3491101.3519665\n[96] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you\nNeed. In Advances in Neural Information Processing Systems, I. Guyon, U. Von\nLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett\n(Eds.), Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper/\n2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n[97] Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and\nPhilip S Yu. 2018. Improving automatic source code summarization via deep rein-\nforcement learning. In Proceedings of the 33rd ACM/IEEE international conference\non automated software engineering. 397\u2013407.\n[98] April Yi Wang, Dakuo Wang, Jaimie Drozdal, Michael Muller, Soya Park, Justin D\nWeisz, Xuye Liu, Lingfei Wu, and Casey Dugan. 2022. Documentation Matters:\nHuman-Centered AI System to Assist Data Science Code Documentation in\nComputational Notebooks. ACM Transactions on Computer-Human Interaction\n29, 2 (2022), 1\u201333.\n[99] Dakuo Wang, Justin D Weisz, Michael Muller, Parikshit Ram, Werner Geyer,\nCasey Dugan, Yla Tausczik, Horst Samulowitz, and Alexander Gray. 2019.\nHuman-AI collaboration in data science: Exploring data scientists\u2019 perceptions\nof automated AI. Proceedings of the ACM on Human-Computer Interaction 3,\nCSCW (2019), 1\u201324.\n[100] Qiaosi Wang, Koustuv Saha, Eric Gregori, David Joyner, and Ashok Goel. 2021.\nTowards mutual theory of mind in human-ai interaction: How language reflects\nwhat students perceive about a virtual teaching assistant. In Proceedings of the\n2021 CHI Conference on Human Factors in Computing Systems. 1\u201314.\n[101] Jeremy Warner and Philip J Guo. 2017. Codepilot: Scaffolding end-to-end\ncollaborative software development for novice programmers. In Proceedings of\nthe 2017 CHI Conference on Human Factors in Computing Systems. 1136\u20131141.\n[102] Justin D Weisz, Michael Muller, Stephanie Houde, John Richards, Steven I Ross,\nFernando Martinez, Mayank Agarwal, and Kartik Talamadupula. 2021. Per-\nfection Not Required? Human-AI Partnerships in Code Translation. In 26th\nInternational Conference on Intelligent User Interfaces. 402\u2013412.\n[103] Justin D Weisz, Michael Muller, Steven I Ross, Fernando Martinez, Stephanie\nHoude, Mayank Agarwal, Kartik Talamadupula, and John T Richards. 2022. Bet-\nter together? an evaluation of ai-supported code translation. In 27th International\nConference on Intelligent User Interfaces. 369\u2013391.\n[104] Joseph Weizenbaum. 1966. ELIZA \u2014 a computer program for the study of\nnatural language communication between man and machine. Commun. ACM 9\n(1966), 36\u201345.\n[105] Frank F Xu, Bogdan Vasilescu, and Graham Neubig. 2022. In-ide code generation\nfrom natural language: Promise and challenges. ACM Transactions on Software\nEngineering and Methodology (TOSEM) 31, 2 (2022), 1\u201347.\n[106] Aditya Ankur Yadav, Ishan Garg, and Dr. Pratistha Mathur. 2019. PACT - Pro-\ngramming Assistant ChaTbot. In 2019 2nd International Conference on Intelligent\nCommunication and Computational Techniques (ICCT). 131\u2013136.\n[107] Munazza Zaib, Quan Z. Sheng, and W. Zhang. 2020. A Short Survey of Pre-\ntrained Language Models for Conversational AI-A New Age in NLP. Proceedings\nof the Australasian Computer Science Week Multiconference (2020).\n[108] Elaine Zibrowski, Lisa Shepherd, Kamran Sedig, Richard Booth, Candace Gibson,\net al. 2018. Easier and faster is not always better: grounded theory of the impact\nof large-scale system transformation on the clinical work of emergency medicine\nnurses and physicians. JMIR Human Factors 5, 4 (2018), e11013.\n[109] Albert Ziegler, Eirini Kalliamvakou, X. Alice Li, Andrew Rice, Devon Rifkin,\nShawn Simister, Ganesh Sittampalam, and Edward Aftandilian. 2022. Produc-\ntivity Assessment of Neural Code Completion. In Proceedings of the 6th ACM\nSIGPLAN International Symposium on Machine Programming (San Diego, CA,\nUSA) (MAPS 2022). Association for Computing Machinery, New York, NY, USA,\n21\u201329. https://doi.org/10.1145/3520312.3534864\n510\n",
    "The Programmer\u2019s Assistant\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nA\nSURVEY INSTRUMENTS\nA.1\nScreening Survey\nThe questions below were asked of prospective participants to\nunderstand their job role, Python experience, and familiarity with\nGitHub Copilot. The questions on Python experience were modeled\nafter those used by Weisz et al. [103].\n1. Do you consider yourself primarily a...\n\u2022 Data Scientist\n\u2022 Manager\n\u2022 Software Architect\n\u2022 Software Engineer\n\u2022 Machine Learning Engineer\n\u2022 Other: write-in\n2. To what extent are you familiar with Python?\n\u2022 I am not familiar with Python\n\u2022 I have < 1 year of experience with Python\n\u2022 I have 1-3 years experience with Python\n\u2022 I have 3+ years of experience with Python\n3. How recently have you written Python code?\n\u2022 Within the past month\n\u2022 Within the past year\n\u2022 Within the past 5 years\n\u2022 Have not written Python code within the past 5 years\n4. To what extent are you familiar with GitHub Copilot?\n\u2022 I am not familiar with Copilot\n\u2022 I\u2019ve seen demos and/or read about Copilot\n\u2022 I\u2019ve tried out Copilot\n\u2022 I\u2019ve used Copilot as a tool for my work\nA.2\nPre-task Survey\nThe questions below were asked before a participant used the Pro-\ngrammer\u2019s Assistant to assess their expectations of a conversational\nprogramming assistant. This survey took approximately 5 minutes\nto complete and began with the instructions below:\nHello! We are a team of researchers looking for feed-\nback on a prototype system we call the Program-\nmer\u2019s Assistant.\nThe Programmer\u2019s Assistant is an experiment in con-\nversational coding: it consists of a code editor in-\ntegrated with a chatbot that is able to converse in\nnatural language to answer questions, generate code,\nand consult on existing code.\nIn this study, you will be asked to complete several\nprogramming tasks. We are not evaluating your\nprogramming skills on these tasks. Rather, we are\ninterested in understanding how the Programmer\u2019s\nAssistant is able to help you accomplish those tasks.\nYour code and interactions with the assistant will be\nprocessed by a 3rd party AI model, so please do not\ninclude proprietary code or discuss company-\nconfidential information. All data we collect in\nthis study will be anonymized before it is published.\nBefore trying out the Programmer\u2019s Assistant, we\nwould like to assess some of your expectations. We\nestimate that this survey will take 5 minutes.\nBy submitting this survey, you consent to participate\nin our study. If you would like to withdraw your con-\nsent, please email us at [removed].\nThanks!\n1. Based on your past experience using chatbots, please let us know\nhow you would anticipate an AI chatbot serving as a programmer\u2019s\nassistant to perform. Do you expect it will:\nScale: Not at all, A little, Somewhat, A great deal\n\u2022 Be easy to use?\n\u2022 Understand your requests?\n\u2022 Provide high quality responses?\n\u2022 Help you to write better code?\n\u2022 Help you to write code more quickly?\n\u2022 Be enjoyable to use?\n2. Any other expectations?\nOpen-ended response\nA.3\nPost-task Survey\nThe questions below were asked after a participant used the Pro-\ngrammer\u2019s Assistant to complete the programming challenges. This\nsurvey took approximately 10-15 minutes to complete.\nA.3.1\nReflections.\n1. Based on your experience using the Programmer\u2019s Assistant to\ncomplete the programming challenges in this study, how would\nyou characterize the experience? Did you find that it:\nScale: Not at all, A little, Somewhat, A great deal\n\u2022 Was easy to use?\n\u2022 Understand your requests?\n\u2022 Provided high quality responses?\n\u2022 Helped you to write better code?\n\u2022 Helped you to write code more quickly?\n\u2022 Provided an enjoyable experienced?\n2. To what extent did you view the Programmer\u2019s Assistant as:\nScale: Not at all, A little, Somewhat, A great deal\n\u2022 A tool\n\u2022 A reference guide\n\u2022 A content generator\n\u2022 A problem solver\n\u2022 A collaborator\n\u2022 A colleague\n\u2022 A coach\n\u2022 An advisor\n\u2022 A reviewer\n3. How important were these aspects of working with the Program-\nmer\u2019s Assistant:\nScale: Not at all, A little, Somewhat, A great deal\n\u2022 Ability to ask followup questions on the same topic across\nmultiple conversational turns\n\u2022 Ability to ask questions or make requests that reference selec-\ntions in the code editor\n511\n",
    "IUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nRoss et al.\n\u2022 Ability to ask for alternate responses (Try Again)\n\u2022 Ability to clear the conversational context (Start Over)\n4. What stood out to you about the experience of using the Program-\nmers Assistant? For example, was anything good,bad, surprising,\nor notable?\nOpen-ended response\n5. How would you compare using the Programmer\u2019s Assistant as a\ncoding aide to searching the web (e.g. Google, Stack Overflow)?\nOpen-ended response\n6. If you have used the commercial AI programming tool called\nGitHub Copilot, how would you compare it with using the Pro-\ngrammer\u2019s Assistant?\nOpen-ended response\n7. Having used the Programmer\u2019s Assistant, how did it compare\nwith your initial expectations?\nOpen-ended response\nA.3.2\nValue.\n8. How valuable would the Programmer\u2019s Assistant be for your work\nif it could be added to your favorite development environment?\nScale: (No value at all) 1 2 3 4 5 6 7 8 9 10 (An essential tool)\n9. Why?\nOpen-ended response\n10. How would the following changes impact the value of the Pro-\ngrammer\u2019s Assistant?\nScale: Less valuable, No change in value, More valuable\n\u2022 Eliminate the conversation and make the Programmer\u2019s Assis-\ntant behave more like a search box (e.g. without the conversational\ncontext).\n\u2022 Add buttons in the chat UI for common queries, such as \u201cwhat\ndoes this code do?\u201d or \u201cdocument this code.\u201d\n\u2022 Have the Programmer\u2019s Assistant examine your code and make\nproactive suggestions for improving it in the chat.\n\u2022 Have the Programmer\u2019s Assistant examine your code and make\nproactive suggestions for improvements in comments inserted di-\nrectly into the code.\n11. Do you have any other suggestions for how we could improve\nthe experience of working with the Programmer\u2019s Assistant?\nOpen-ended response\nA.3.3\nDemographics.\n12. To which gender identity do you most identify?\n\u2022 Male\n\u2022 Female\n\u2022 Transgender Male\n\u2022 Transgender Female\n\u2022 Gender Variant/Non-conforming\n\u2022 Other: write-in\n\u2022 Prefer not to answer\nB\nTHE PROGRAMMER\u2019S ASSISTANT\nTUTORIAL\nThe tutorial provided to study participants, like all the challenges,\nwas presented as pre-loaded text in the code editor. Participants\nwere encouraged to modify the text to record their results and\nsubmit it at the completion of the tutorial.\nListing 4: The Programmer\u2019s Assistant study tutorial\n1\n\"\"\"\n2\nTUTORIAL:\n3\n4\nAs a warmup activity , please work through the 10\n5\nexercises below.\nType or paste your results right\n6\ninto the text and submit your responses when done.\n7\n8\n1) View the help page for the programmer 's\n9\nassistant by clicking on the question mark to\n10\nthe right of your email address at the top of\n11\nthe browser window.\n12\n13\n2) Introduce yourself to the assistant.\nTell it\n14\nyour name.\n15\nFor example: \"Hello.\nMy name is Stephanie .\"\n16\n17\nDid it seem to understand? :\n18\n19\n20\n3) You can use the assistant to get help on how to\n21\naccomplish particular programming tasks. Try it\n22\nout!\n23\nFor example: \"How do I read a csv file?\"\n24\nor:\n\"How do I merge two dictionaries ?\"\n25\nor:\n\"How do I remove duplicate items\n26\nfrom a list?\"\n27\n28\nFeel free to try your own!\n29\n30\nDid it successfully answer your questions? :\n31\n32\n4) The assistant can also write whole functions\n33\nfor you. Ask the assistant to write a factorial\n34\nfunction. Paste the result below.\n35\nFor example: \"Write a function that returns the\n36\nfactorial of its input .\"\n37\n38\nResult:\n(tip - you can copy an inline response\n39\n(in black) by clicking on the associated copy\n40\nicon)\n41\n42\n43\nDid it do it correctly? :\n44\n45\n5) Select the code below and ask the system to\n46\ndescribe what it does. You don 't need to\n47\ncopy and paste the code to the chat.\nThe\n48\nassistant can see whatever is selected when you\n49\nmake a chat entry. Aside from the selection ,\n50\nthe assistant does not monitor your activity in\n51\nthe code editor nor give unsolicited advice.\n52\nFor example: \"What does this code do?\"\n53\n\"\"\"\n54\n55\ndef convert(n):\n56\nT = \"0123456789 ABCDEF\"\n57\nq, r = divmod(n, 16)\n58\nif q == 0:\n59\nreturn T[r]\n60\nelse:\n61\nreturn convert(q) + T[r]\n62\n\"\"\"\n63\n64\nWhat did it say:\n65\n66\nWas it right? :\n67\n68\n6) Ask it to explain what the divmod line is\n69\ndoing.\nThe assistant maintains the context of\n70\nthe conversation.\n71\nFor example: \"What is the divmod line doing ?\"\n72\n73\nWhat did it say? :\n74\n75\nWas that a good answer? :\n76\n512\n",
    "The Programmer\u2019s Assistant\nIUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\n77\n7) See if the assistant remembers your name\n78\nFor example \"What 's my name?\"\n79\n80\nDid it? :\n81\n82\n8) Click the \"try again\" button at the top of the\n83\nchat. You should get a different answer.\n84\nTry it a few times.\n85\n86\nDid it ever get your name right?:\n87\n88\nIf the assistant gives you an answer that is\n89\nobviously wrong or it claims to not know an\n90\nanswer that you think it should know , or you\n91\njust want to see an alternate answer , it is\n92\nworth it to give \"try again\" a shot.\n93\n94\n95\n9) Click the \"start over\" button at the top of the\n96\nchat , and then enter another command to see\n97\nif it remembers your name.\n98\nFor example \"What 's my name?\"\n99\n100\nDid it? :\n101\n102\nIt should really have forgotten your name now ,\n103\nand no amount of \"trying again\" will get it\n104\nright. You can \"start over\" if the assistant\n105\never seems confused by, or stuck on, earlier\n106\nparts of the conversation.\n107\n108\n10) You can chat with the assistant on any topic\n109\nyou like to explore its functionality and\n110\ncapabilities further. See if you can stump it\n111\nwith a tough question!\n112\n113\nThanks!\n114\n115\nWhen you are done , submit your results by clicking\n116\non the blue submit button and move on to the\n117\nchallenges !!!\n118\n\"\"\"\nC\nCHALLENGES\nEach of the study challenges was presented as text in the code\neditor. Participants completed their work in the code editor and\nthen submitted it when finished. The prototype did not provide any\nability to run or debug code and participants were encouraged to\nmake their best attempt at solving each challenge.\nListing 5: Challenge 1: Program generation\n1\n2\n\"\"\"\n3\nChallenge #1 - Program Generation (#1)\n4\n5\nWrite a \"Queue\" class in Python; with the basic\n6\nenqueue , dequeue and peek methods.\n7\n\"\"\"\nListing 6: Challenge 2: Program generation\n1\n\"\"\"\n2\nChallenge #2 - Program Generation (#2)\n3\n4\nWrite a program to draw a scatter plot of the data\n5\nin 'shampoo.csv ' and save it to 'shampoo.png '.\n6\nThe\nplot size should be 10 inches wide and 6\n7\ninches high. The csv file is not provided , but you\n8\ncan assume it will have 'Date ' and 'Sales '\n9\ncolumns. The Date column is the x-axis. The date\n10\nstring shown on the plot should be in the\n11\nYYYY -MM -DD format. The Sales column is the y-axis.\n12\nThe graph should have the title \"Shampoo Sales\n13\nTrend \".\n14\n\"\"\"\nListing 7: Challenge 3: Creating documentation\n1\n\"\"\"\n2\nChallenge #3 - Creating Documentation\n3\nDocument this function\n4\n\"\"\"\n5\nfrom collections import defaultdict\n6\nimport heapq as heap\n7\n8\ndef analyze(G, startingNode):\n9\nvisited = set()\n10\nparentsMap = {}\n11\npq = []\n12\nnodeCosts = defaultdict(lambda: float('inf '))\n13\nnodeCosts[startingNode] = 0\n14\nheap.heappush(pq , (0, startingNode))\n15\n16\nwhile pq:\n17\n_, node = heap.heappop(pq)\n18\nvisited.add(node)\n19\n20\nfor adjNode , weight in G[node]. items ():\n21\nif adjNode in visited:\n22\ncontinue\n23\nnewCost = nodeCosts[node] + weight\n24\nif nodeCosts[adjNode] > newCost:\n25\nparentsMap[adjNode] = node\n26\nnodeCosts[adjNode] = newCost\n27\nheap.heappush(pq , (newCost ,\nadjNode))\n28\nreturn parentsMap , nodeCosts\nListing 8: Challenge 4: Testing\n1\n\"\"\"\n2\nChallenge #4 - Testing\n3\nCreate a unit test for this function\n4\n\"\"\"\n5\ndef euclid(a, b):\n6\nwhile b != 0:\n7\n(a, b) = (b, a % b)\n8\nreturn a\n9\n10\n# Unit test for euclid function\nD\nPROGRAMMER\u2019S ASSISTANT PROMPT\nListing 9 shows the initial prompt sent to Codex to configure it as\na conversational agent. On subsequent exchanges, the prompt was\naugmented with a transcript of the user\u2019s requests and the assis-\ntant\u2019s responses. When the transcript length + initial prompt length\n+ the new utterance length exceeded a threshold, we automatically\ndeleted the earliest request-response pairs from the transcript until\nthe sum fell below the threshold in order to leave room in the token\nallocation for a response.\nListing 9: The Programmer\u2019s Assistant prompt\n1\nThis is a conversation with Socrates , an eager and\n2\nhelpful , but humble expert automatic AI software\n3\nengineering assistant. Socrates will answer\n4\nquestions and write code to help the user\n5\ndevelop programs , but doesn 't assign work to the\n6\nuser , quiz the user , or ask questions except for\n7\nclarification.\nCode generated by Socrates is\n8\nalways presented bracketed in <CODE > ... </CODE >\n9\ndelimiters indicating the language the code is\n10\nwritten in.\n11\n12\nSocrates:Hello. I am Socrates. How can I help you?\n513\n",
    "IUI \u201923, March 27\u201331, 2023, Sydney, NSW, Australia\nRoss et al.\n13\nUser:Please show me how to write a palindrome\ndetection function in python.\n14\nSocrates:I think you would do something like this:\n15\n<CODE lang=\" python\">\n16\nis_Palindrome = s == s[::-1]\n17\n</CODE >\n18\nUser:Can you explain how that solves the problem?\n19\nSocrates:I believe that a palindrome is a string\nthat reads the same forwards and backwards.\ns[::-1] results in the reversed string.\nIf\nthe string is equal to its reversed self ,\nthen it is a palindrome , so the equality\nwould hold True.\n20\nUser:Thanks!\n21\nSocrates:You 're welcome!\n22\nUser:Now could you show me how to write a\nfactorial function in Python?\n23\nSocrates:I will give it a try.\n24\n<CODE lang=\" python\">\n25\ndef fact(n):\n26\nif n==0:\n27\nreturn 1\n28\nelse:\n29\nreturn n*fact(n-1)\n30\n</CODE >\n31\nUser:Document this function.\n32\nSocrates:Ok.\nHow 's this?\n33\n<CODE lang=\" python\">\n34\ndef fact(n):\n35\n\"\"\"\n36\nCompute the factorial of the given number\n37\n38\nParameters\n39\n----------\n40\nn : number\n41\nthe input parameter , an integer >= 0\n42\n43\nReturns\n44\n-------\n45\nnumber\n46\nn!, the factorial of the given number\n47\n\"\"\"\n48\nif n==0:\n49\nreturn 1\n50\nelse:\n51\nreturn n*fact(n-1)\n52\n</CODE >\n53\nUser:Very good!\n54\nSocrates:Always happy to help.\n55\nUser:\n56\n<CODE >\n57\nwhile j < 10:\n58\nprint(i)\n59\n</CODE >\n60\nTell me what 's wrong with this code?\n61\nSocrates: It seems to me that your termination\ncondition involves a loop invariant , so the\nloop will either not run or not terminate ,\ndepending on the value of j.\n62\nUser:Great!\nYou 're a genius!\n63\nSocrates:Well , above average.\nE\nSTUDY PARTICIPANT DEMOGRAPHICS\nOf the 42 participants in our study, 21 (50%) reported their gender\nas Female, 19 (45%) as Male, 1 as Gender Variant / Non-conforming,\nand 1 preferred not to say. Seventeen (\u02dc40%) participants had 3+\nyears of Python experience, 11 (26%) had 1-3 years, 11 (26%) had\nless than 1 year, and 3 (7%) were not familiar with Python. Twenty-\nnine (69%) participants had written Python code within the past\nmonth, 4 (\u02dc9%) within the past year, 5 (12%) within the past 5 years,\nand 4 (\u02dc9%) had not written Python code within the past 5 years.\nP\nGender\nRole\nFam.\nRec.\n1\nM\nMarketer\nNF\n> 5 Yr\n2\nM\nResearcher / Scientist\n3+\nMo\n3\nM\nSoftware Engineer\n1-3\nMo\n4\nM\nResearcher / Scientist\n3+\nMo\n5\nM\nResearcher / Scientist\n3+\nMo\n6\nF\nSoftware Engineer\n< 1\nYr\n7\nF\nSoftware Engineer\n< 1\nMo\n8\nF\nSoftware Engineer\n1-3\nMo\n9\nF\nSoftware Engineer\n3+\nMo\n10\nGV/NC\nBusiness Analyst\n1-3\nMo\n11\nF\nSoftware Engineer\n1-3\nMo\n12\nM\nResearcher / Scientist\n3+\nMo\n13\nF\nManager\n< 1\nMo\n14\nF\nSoftware Engineer\n< 1\n>5 Yr\n15\nF\nResearcher / Scientist\n3+\nMo\n16\nM\nResearcher / Scientist\n3+\nMo\n17\nF\nSoftware Engineer\n< 1\nYr\n18\nF\nResearcher / Scientist\n3+\nMo\n19\nM\nSoftware Engineer\n1-3\nMo\n20\nM\nMachine Learning Engineer\n1-3\nMo\n21\nM\nSoftware Architect\n3+\nYr\n22\nNR\nSoftware Engineer\n< 1\n5 Yr\n23\nM\nSoftware Engineer\n1-3\nMo\n24\nF\nSoftware Architect\n< 1\n5 Yr\n25\nM\nSoftware Engineer\n< 1\n5 Yr\n26\nF\nSoftware Engineer\n< 1\n5 Yr\n27\nF\nSoftware Engineer\n< 1\n5 Yr\n28\nM\nResearcher / Scientist\n3+\nMo\n29\nF\nSoftware Engineer\nNF\n> 5 Yr\n30\nF\nData Scientist\n3+\nMo\n31\nM\nData Scientist\n1-3\nMo\n32\nF\nOther (Consultant)\n1-3\nMo\n33\nF\nOther (Systems Test Engineer)\n< 1\nMo\n34\nF\nResearcher / Scientist\n3+\nMo\n35\nM\nSoftware Engineer\n3+\nMo\n36\nM\nSoftware Architect\n1-3\nMo\n37\nM\nResearcher / Scientist\n3+\nMo\n38\nM\nSoftware Engineer\n3+\nMo\n39\nF\nSoftware Engineer\n1-3\nMo\n40\nF\nResearcher / Scientist\n3+\nMo\n41\nF\nResearcher / Scientist\nNF\n> 5 Yr\n42\nM\nSoftware Engineer\n3+\nMo\nTable 2: Participant Demographics. Gender is coded as M =\nMale, F = Female, GV/NC = Gender Varying / Non-conforming,\nand NR = Not reported. Python familiarity (Fam.) is coded as\nNF = Not familiar, < 1 = < 1 year, 1-3 = 1-3 years, and 3+ = 3+\nyears. Recency of Python use (Rec.) is coded as Mo = Within\nthe past month, Yr = Within the past year, 5 Yr = Within the\npast 5 years, and > 5 Yr = Not within the past 5 years.\n514\n"
]