[
    "181\n\nSemantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model Khushbu Doulani Vardhaman College of Engineering Hyderabad, India khushidoulani@gmail.com\n\nShivangi Sachan\u2217 Department of CSE IIIT Lucknow Lucknow, UP, India mcs21025@iiitl.ac.in\n\nMainak Adhikari Department of CSE IIIT Lucknow UP, India mainak.ism@gmail.com\n\nABSTRACT The emergence of novel types of communication, such as email, has been brought on by the development of the internet, which radically concentrated the way in that individuals communicate socially and with one another. It is now establishing itself as a crucial aspect of the communication network which has been adopted by a variety of commercial enterprises such as retail outlets. So in this research paper, we have built a unique spam-detection methodology based on email-body sentiment analysis. The proposed hybrid model is put into practice and preprocessing the data, extracting the proper- ties, and categorizing data are all steps in the process. To examine the emotive and sequential aspects of texts, we use word embed- ding and a bi-directional LSTM network. this model frequently shortens the training period, then utilizes the Convolution Layer to extract text features at a higher level for the BiLSTM network. Our model performs better than previous versions, with an accuracy rate of 97\u201398%. In addition, we show that our model beats not just some well-known machine learning classifiers but also cutting-edge methods for identifying spam communications, demonstrating its superiority on its own. Suggested Ensemble model\u2019s results are examined in terms of recall, accuracy, and precision\n\n1 INTRODUCTION Over the past few years, a clear surge of both the amount of spam- mers as well as spam emails. This is likely due to a fact that the investment necessary for engaging in the spamming industry is relatively low. As a result of this, we currently have a system that identifies every email as suspicious, which has caused major expen- ditures in the investment of defense systems [12]. Emails are used for online crimes like fraud, hacking, phishing, E-mail bombing, bul- lying, and spamming. [16]. Algorithms that are based on machine learning (ML) are now the most effective and often used approach to the recognition of spam. Phishing, which is defined as a fraudulent attempt to acquire private information by masquerading as a trust- worthy party in electronic communication, has rapidly advanced past use of simple techniques and the tactic of casting a wide net; instead, spear phishing uses a variety of sophisticated techniques to target a single high-value individual. Other researchers used NB, Decision Trees, and SVM to compare the performance of supervised ML algorithms for spam identification [6]. Spam emails clog up re- cipients\u2019 inboxes with unsolicited communications, which frustrate them and push them into the attacker\u2019s planned traps [7]. As a re- sult, spam messages unquestionably pose a risk to both email users and the Internet community. In addition, Users may occasionally read the entire text of an unsolicited message that is delivered to the target users\u2019 inboxes without realizing that the message is junk and then choosing to avoid it. Building a framework for email spam detection is the aim of this project. In this approach, we combine the Word-Embedding Network with the CNN layer, Bi-LSTM, and GRU (BiLSTM+GRU). CNN layers are used to speed up training time before the Bi-LSTM network, and more advanced textual character- istics are extracted with the use of this network in comparison to the straight LSTM network, in less time. Gated recurrent neural net- works (GRUs) are then added because they train more quickly and perform better for language modeling. To evaluate and investigate various machine learning algorithms for predicting email spam, and develop a hybrid classification algorithm to filter email spam before employing an ensemble classification algorithm to forecast it. To put an innovative technique into practice and compare it to the current method in terms of various metrics. Ensemble learn- ing, a successful machine learning paradigm, combines a group of learners rather than a single learner to forecast unknown target attributes. Bagging, boosting, voting, and stacking are the four main types of ensemble learning techniques. To increase performance, an integrated method and the combining of two or three algorithms are also suggested. Extraction of text-based features takes a long time. Furthermore, it can be challenging to extract all of the crucial information from a short text. Over the span associated with this\n\nCCS CONCEPTS \u2022 Computer systems organization \u2192 Embedded systems; Re- dundancy; Robotics; \u2022 Networks \u2192 Network reliability.\n\nKEYWORDS Dataset, KNN, Gaussian Naive Bayes, LSTM, SVM, Bidirectional LSTM, GRU, Word-Embeddings, CNN\n\nACM Reference Format: Shivangi Sachan, Khushbu Doulani, and Mainak Adhikari. 2023. Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model. In 2023 Fifteenth International Conference on Contemporary Computing (IC3-2023) (IC3 2023), August 03\u201305, 2023, Noida, India. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/3607947. 3607979\n\n\u2217Both authors contributed equally to this research.\n\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. IC3 2023, August 03\u201305, 2023, Noida, India \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-0022-4/23/08. . . $15.00 https://doi.org/10.1145/3607947.3607979\n\n",
    "182\n\nIC3 2023, August 03\u201305, 2023, Noida, India\n\nSachan et al.\n\nresearch, we utilize Bidirectional Large Short-Term Memories (Bi- LSTM) in conjunction with Convolutional Neural Networks (CNN) to come up with an innovative method to the detection of spam. Bagging and boosting approaches were widely preferred in this study. Contribution and paper organization is as follows: section 1.1 describes literature study, section 1.2 describe motivation for this research work, section 2 sketches procedure of details implemen- tation, Section 3 present experimental setup, dataset description and evaluation metrics, and section 4 summarizing outcomes of the experiment.\n\nRF (random forest) ensemble techniques were performed to forecast class labels, and the outputs were input into a classifying unit to increase accuracy. A method for content-based phishing detection was presented by the authors in [2], to classify phishing emails, they employed RF. They categorize spam and phishing emails. They enhanced phishing email classifiers with more accurate predictions by extracting features. They showed some effective Machine learn- ing spam filtering techniques. When the PCA method is used, it will lower the number of features in the dataset. The collected features go through the PCA algorithm to reduce the number of features. The PCA method is used to make a straightforward representation of the information which illustrates the amount of variability there is in the data. The authors of [20] presented the Fuzzy C-means method for classifying spam email. To stop spam, they implemented a membership threshold value. A methodology to identify unla- beled data was put forth by the authors of [1] and applied motive analysis to the Enron data collection. They divided the data into categories that were favorable, negative, and neutral. They grouped the data using k-means clustering, an unsupervised ML technique and then classified it using the supervised ML techniques SVM and NB. Hina, Maryam, and colleagues (2021) implemented Sefaced: Deep learning-based semantic analysis and categorization of e-mail data using a forensic technique. For multiclass email classification, SeFACED employs a Gated Recurrent Neural Network (GRU) based on Long Short-Term Memory (LSTM). Different random weight ini- tializations affect LSTMs [9]. Zhang, Yan, et al.(2019) Experiments on three-way game-theoretic rough set (GTRS) email spam filter- ing show that it is feasible to significantly boost coverage without decreasing accuracy [23]. According to Xia et al. [22], SMS spam has been identified using machine learning model such as naive bayes , vector-space modeling, support vector machines (SVM), long selective memory machines (LSTM), and convolutional neural networks including every instance of a method for categorizing data. Elshoush, Huwaida, et al. (2019) Using adaboost and stochastic gradient descent (sgd) algorithms for e-mail filtering with R and orange software spam [3]. Orange software was used to create the classifications, which included Adaboost and SGD. The majority of researchers focused on text-based email spam classification meth- ods because image-based spam can be filtered in the early stages of pre-processing. There are widely used word bag (BoW) model, which believes that documents are merely unordered collections of words, is the foundation for these techniques. Kumaresan [11] explains SVM with a cuckoo search algorithm was used to extract textual features for spam detection. Renuka and Visalakshi made use of svm [17] spam email identification, followed by selecting features using Latent Semantic Indexing (LSI). Here we have used labeled dataset to train the hybrid classifier. We used TF-IDF for feature extraction [20] and Textual features for spam detection were extracted using SVM and a cuckoo search algorithm. [4] for filtering out the spam email. Combining the integrated strategy to the pure SVM and NB methods, overall accuracy is really improved. Moreover, accurate detection for spam email has been proposed using the Negative Selection Algorithm (NSA) and Particle Swarm Optimization\u2019s (PSO) algorithm. PSO is used in this instance to improve the effectiveness of the classifier.\n\n1.1 Related Work Email is indeed the second most frequently utilized Internet appli- cation as well as the third most common method of cyberbullying, claims one study. Cybercriminals exploit it in a number of ways, including as sending obscene or abusive messages, adding viruses to emails, snatching the private information of victims, and ex- posing it to a broad audience. Spam letters made up 53.95% of all email traffic in March 2020. We examine three main types of un- lawful emails in our study. First are fake emails, which are sent to manipulate recipients to submit sensitive information. The sec- ond as being cyberbullying\u2019s use of harassing emails to threaten individuals. Suspicious emails that describe illegal activities belong to the third category. Many researchers have earlier contributed massively to this subject. The researcher claims there is some proof that suspicious emails were sent before to the events of 9/11. [14]. When it comes to data labeling, there are also convinced rule-based approaches and technologies ( like VADER) that are used, even though their efficiency of the are together is adversely affected. A hidden layer, which itself is essential for vectorization, is the top layer of the model. We use oversampling methods for this minority class because of the absence of data. Sampling techniques can help with multicollinearity, but they have an impact on simulation re- sults. Oversampling causes data to be randomly repeated, which affects test data because dividing data may result in duplicates. Un- dersampling may result in the loss of some strong information. In order to advance email research, it is crucial to provide datasets on criminal activity. P. Garg et al. (2021) [5], which revealed that spam in an email was detected in 70 percent of business emails, spam was established as an obstacle for email administrators. Recognizing spam and getting rid of it were the primary concerns, as spam can be offensive, may lead to other internet sites being tricked, which can offer harmful data, and can feature those who are not particu- lar with their content using NLP. To select the best-trained model, each mail transmission protocol requires precise and effective email classification, a machine learning comparison is done. Our study has suggested that innovative deep learning outperforms learning algorithms like SVM and RF. Current studies on the classification of emails use a variety of machine learning (ML) techniques, with a few of them focusing on the study of the sentiments consisted of within email databases. The lack of datasets is a significant obstacle to email classification. There are few publicly accessible E-mail datasets, thus researchers must use these datasets to test their hy- potheses or gather data on their own. Authors[15] describe supplied two-phased outlier detection models to enhance the IIOT network\u2019s dependability. Artificial Neural Network, SVM, Gaussian NB, and\n\n",
    "183\n\nIC3 2023, August 03\u201305, 2023, Noida, India\n\nSemantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model\n\n1.2 Motivation and Novelty Email is most common form of communication between people in this digital age. Many users have been victims of spam emails, and their personal information has been compromised. The email Classification technique is employed to identify and filter junk mail, junk, and virus-infected emails prior to reach a user\u2019s inbox. Existing email classification methods result in irrelevant emails and/or the loss of valuable information. Keeping these constraints in mind, the following contributions are made in this paper:\n\nlanguage processing (NLP). The major preprocessing steps are de- picted below.\n\n2.1 NLP Tokenization Tokenization of documents into words follows predefined rules. The tokenization step is carried out in Python with spacy library.\n\n2.2 Stop Words Removal Stop words appear infrequently or frequently in the document, but they are less significant in terms of importance. As a result, these are removed to improve data processing.\n\nText-based feature extraction is a lengthy process. Further- more, extracting every important feature from text is difficult. In this paper, we show how to employ GRU with Convo- lutional Neural Networks and Bidirectional-LSTM to find spam.\n\n2.3 Text Normalization A word\u2019s lexicon form or order may differ. Thus, they must all be changed to their root word to be correctly analyzed. Lemmatization and stemming are the two methods that can be used for normal- ization. When a word\u2019s final few characters are removed to create a shorter form, even if that form has no meaning, the procedure is known as stemming. lemmatization [21] is a mixture of corpus- based an rule-based methods, and it retains the context of a term while changing it back to its root.\n\nUsed Word-Embeddings, BiLSTM, and Gated Recurrent Neu- ral Networks to examine the relationships, sentimental con- tent, and sequential way of email contents.\n\nApplied CNN before the Bi-LSTM network, training time can be sped up. This network can also extract more advanced textual features faster than the Bi-LSTM network alone when combined with the GRU network.\n\nWe use Enorn Corpora datasets and compute precision, re- call, and f-score to assess how well the suggested technique performs. Our model outperforms several well-known ma- chine learning techniques as well as more contemporary methods for spam message detection.\n\n2.4 Feature Extraction feature extraction which transforms the initial text into its features so that it may be used for modeling after being cleaned up and normalized. Before predicting them, we use a specific way to give weights to specific terms in our document. While it is simple for a computer to process numbers, we choose to represent individual words numerically. In such cases, we choose word embeddings. IDF is the count of documents containing the term divided by the total number of documents, and occurrence is the amount of instances a word appears in a document. We derive characteristics based on equations. 1,2,3,4,5, and 6. We use equations to derive properties.\n\n2 PROPOSED SYSTEM ARCHITECTURE AND\n\nMODEL\n\nE-mail is a valuable tool for communicating with other users. Email allows the sender to efficiently forward millions of advertisements at no cost. Unfortunately, this scheme is now being used in a variety of organizations. As a result, a massive amount of redundant emails is known as spam or junk mail, many people are confused about the emails in their E- Mailboxes. Each learning sequence is given for- ward as well as backward to two different LSTM networks that are attached to the same outputs layer in order for bidirectional Lstms to function. This indicates that the Bi-LSTM has detailed sequential information about all points before and following each point in a specific sequence. In other words, we concatenate the outputs from both the forward and the backward LSTM at each time step rather than just encoding the sequence in the forward direction. Each word\u2019s encoded form now comprehends the words that come before and after it. This is a problem for the Internet community. The di- agram depicts various stages that aid in the prediction of email spam:\n\n(cid:19)\n\n(cid:18) 1 \ud835\udc51 \ud835\udc53\n\n\ud835\udc47 \ud835\udc53 \ud835\udc3c\ud835\udc51 \ud835\udc53 = \ud835\udc61 \ud835\udc53 \u2217\n\n(1)\n\n\ud835\udc47 \ud835\udc53 \ud835\udc3c\ud835\udc51 \ud835\udc53 = \ud835\udc61 \ud835\udc53 \u2217 Inverse(\ud835\udc51 \ud835\udc53 )\n\n(2)\n\n\ud835\udc47 \ud835\udc53 \ud835\udc3c\ud835\udc51 \ud835\udc53 (\ud835\udc61, \ud835\udc51, \ud835\udc37) = \ud835\udc47 \ud835\udc53 (\ud835\udc61, \ud835\udc51).\ud835\udc3c\ud835\udc51 \ud835\udc53 (\ud835\udc61, \ud835\udc37)\n\n(3)\n\n\ud835\udc41 |\ud835\udc51\ud835\udf16\ud835\udc37\ud835\udc61\ud835\udf16\ud835\udc37 |\n\n\ud835\udc47 \ud835\udc3c\ud835\udc51 \ud835\udc53 (\ud835\udc61, \ud835\udc51) = log\n\n(4)\n\nA word2vec neural network-based approach is the method that is utilized for this goal as the tool. The following equation, referred to as 5, shows how word2vec handles word context through the use of probability-accurate measurements. Here letter D stands for the paired-wise display of a set of words, while the letters w and c0 or c1 represent paired word context that originated from a larger collection of set D.\n\n1 1 + \ud835\udc52 \u2212 (\ud835\udc64\u00b7\ud835\udc5011+\ud835\udc64\u00b7\ud835\udc5012+...+\ud835\udc64\u00b7\ud835\udc501\ud835\udc58 )\n\n\ud835\udc43 (\ud835\udc37 = 1 | \ud835\udc64, \ud835\udc5011:\ud835\udc58 ) =\n\n(5)\n\nBecause real-world data is messy and contains unnecessary infor- mation and duplication, data preprocessing is critical in natural\n\n1 1 + \ud835\udc52 \u2212 (\ud835\udc64\u00b7\ud835\udc500)\n\n\ud835\udc43 (\ud835\udc37 = 1 | \ud835\udc64, \ud835\udc501:\ud835\udc58 ) =\n\n(6)\n\n",
    "184\n\nIC3 2023, August 03\u201305, 2023, Noida, India\n\nSachan et al.\n\n2.5 Word-Embeddings Word-Embedding helps to improve on the typical \"bag-of-words\" worldview, which requires a massive sparse feature vector to score every word individually to represent this same entire vocabulary. This perception is sparse because the vocabulary is large, and each word or document is defined by a massive vector. Using a word map-based dictionary, word embedding needs to be converted terms (words) into real value feature vectors. There are two basic issues with standard feature engineering techniques for deep learning. Data is represented using sparse vectors, and the second is that some of the meanings of words are not taken into consideration. Similar phrases will have values in embedding vectors that are almost real-valued. The Input length in our proposed study is set to 700 for our suggested model. If the texts seemed to be integer encoded with value systems between 10 and 20, the vocabulary distance would be 11. Our data is encoded as integers, and the input and output dimensions are both set to 50,000. The embedding layer outcome will be used in successive layers and for BiLSTM and GRU layers.\n\nlong-term dependencies[10]. LSTM was actually created to address the problem of long-term reliance. LSTM has the unique ability to recall. The cell state is the LSTM model\u2019s central concept. With only a small amount of linear interaction, the cell state follows the sequence essentially unmodified from beginning to end. gate of an LSTM is also significant. Under the command of these gates, information is safely inserted to or eliminated from the cell stated. The following equations are used by the LSTM model to update each cell:\n\n(cid:17)\n\n(cid:16) \ud835\udc4a\ud835\udc53 \u00b7 [\u210e\ud835\udc61 \u22121, \ud835\udc65\ud835\udc61 ] + \ud835\udc4f \ud835\udc53\n\n\ud835\udc53\ud835\udc61 = \ud835\udf0e\n\n(9)\n\nIn this case, Xt denotes input, and ht is the hidden state at the t time step. The following is the revised cell state Ct:\n\n\ud835\udc56t = \ud835\udf0e (\ud835\udc4a\ud835\udc56 [\u210e\ud835\udc61 \u22121, \ud835\udc65\ud835\udc61 ] + \ud835\udc4f\ud835\udc56 ) \ud835\udc36\ud835\udc47 = tanh (\ud835\udc4a\ud835\udc50 [\u210e\ud835\udc61 \u22121, \ud835\udc65\ud835\udc61 ] + \ud835\udc4f\ud835\udc50\ud835\udc61 ) \ud835\udc36\ud835\udc61 = \ud835\udc53\ud835\udc61 \u2217 \ud835\udc36\ud835\udc61 \u22121 + \ud835\udc56\ud835\udc61 \u2217 \ud835\udc36\ud835\udc47 Here, we may compute the output and hidden state at t time steps using the point-wise multiplication operator *. \ud835\udc5c\ud835\udc61 = \ud835\udf0e (\ud835\udc4a\ud835\udc5c \u00b7 [\u210e\ud835\udc61 \u22121, \ud835\udc65\ud835\udc61 ] + \ud835\udc4f\ud835\udc5c ) \u210e\ud835\udc61 = \ud835\udc5c\ud835\udc61 \u2217 tanh (\ud835\udc36\ud835\udc61 ) Due to the reality it only considers all prior contexts from the present one, LSTM does have a few drawbacks. As a result of this, it may accept data from preceding time steps through LSTM as well as RNN. Therefore, in order to avoid this issue, further improve- ments are carried out with the help of a bidirectional recurrent neural network(Bi-RNN). BiRNN [13] can handle two pieces of in- formation from both the front and the back. Bi-LSTM is created by combining the Bi-RNN and LSTM. As a result, operating LSTM has advantages such as cell state storage so that BiRNN have way to acknowledge from the context before and after. As a conse- quence of this, it provides the Bi-LSTM with the advantages of an LSTM with feedback for the next layer. Remembering long-term dependencies is a significant new benefit of Bi-LSTM. The output, which is a feature vector, will be based on the call state. Finally, we forecast the probability of email content as Normal, Fraudu- lent, Harassment, and Suspicious Emails using as an input to the softmax activation function, which is a weighted sum of the dense layer\u2019s outputs. To regulate the information flow, GRU employs the point-wise multiplying function and logistic sigmoid activation. The GRU has hidden states of storage memory and does not have distinct memory cells or units for state control. The W, U, and b vectors, which stand for weights, gates, and biases, respectively, are crucial variables that must be calculated during the creation of the GRU model. For training reasons, the pre-trained word embedding known as the Glove vector is used. They made it clear that GRU is the superior model when there is a large amount of training data for textual groups and word embedding is available. BiLSTM, CNN, and GRU is required so as to compensate for the deletion of the document\u2019s long-term and short-term connections. In our case, the embedding dimension, maximum sequence length, and lexicon size were used to start the LSTM embedding layer in three separate LSTM models. The input vector was modified to make it appropriate for such a Conv1D layer, prior situations\u2019 sequences are returned by LSTM layer. The \"return sequences\" of the LSTM layer must be set to False when the subsequent state is free of the gated\n\n(10)\n\n(11)\n\n(12)\n\n(13)\n\n2.6 Machine Learning Model Within the scope of the research, we are using the subsequent ma- chine learning techniques, to examine and compare the overall efficacy of our suggested Bi-LSTM strategy: Support Vector Ma- chine, Gaussian NB, Logistic Regression, K - nearest neighbors, and Random Forest (RF).\n\n(14)\n\n2.7 Convolution Network The popular RNN model generally performs well but takes too long to train the model incorporating the textual sequential data. When a layer is added after the RNN layer, the model\u2019s learning duration is considerably decreased. Higher-level feature extraction is another benefit. [19] additionally possible using the convolutional layer. In essence, the convolution layer looks for combinations of the various words or paragraphs in the document that involve the filters. We use features with 128 dimensions and a size 10 for each. For this task, the Relu activation function is utilized. After that, the one-dimensional largest pooling layers with a pooling size of 4 are put on the data in order to obtain higher-level features.\n\n2.8 BiLSTM Network with GRU Recurrent Neural Network (RNN) technique of text sentiment anal- ysis is particularly well-liked and frequently applied. Recurrent neural networks (RNN) surpass conventional neural networks. be- cause it can remember the information from earlier time steps thanks to its memory. A state vector is combined with an RNN\u2019s data to create a new state vector. The resulting state vector uses the present to recollect past knowledge. The RNN is straightforward and is based on the following equations:\n\n\u210e\ud835\udc61 = tanh (\ud835\udc4a\u210e\u210e\u210e\ud835\udc61 \u22121 + \ud835\udc4a\ud835\udf0b\u210e\ud835\udc65\ud835\udc61 ) \ud835\udc66\ud835\udc61 = \ud835\udc4a\u210e\ud835\udc66 The vanilla RNN[18]is not very good at remembering previous sequences. In addition to that, RNN struggles with diminishing gradient descent. A kind of RNN is a long short-term recall network (LSTM), solves a vanishing gradient descent problem and learns\n\n(7)\n\n\u210e\ud835\udc61\n\n(8)\n\n",
    "185\n\nIC3 2023, August 03\u201305, 2023, Noida, India\n\nSemantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model\n\narchitecture. Quantity of learning parameters must be taken into consideration. A 350-unit LSTM layer was set - up, and different LSTM unit combinations were tested. More importantly, because it has more parts, the model made with BiLSTM will take longer to train. Bidirectional LSTM is the name of a particular kind of recurrent neural network that is primarily used for the processing of natural languages. (BiLSTM). It is able to use data from both sides, and, in contrast to regular LSTM, it enables input flow in both directions. It is an effective instrument for demonstrating the logical relationships between words and phrases, and this involves both the forward and backward directions of the sequence. In con- clusion, BiLSTM works by adding one extra layer of LSTM, causing the information flow to travel in the other direction. It only denotes that the input sequence runs in reverse at the next LSTM layer. Mul- tiple operations, including averaging, summation, multiplication, and concatenation, are then applied to the results of the two LSTM layers. The gated design of Bi-LSTM and GRU networks solves the disappearing gradient and exploding problems. A good way to handle more long sequences is to use Bi-LSMT and GRU together. GRU works well with datasets that don\u2019t have text. In two to three rounds, the complicated CNN+BiLSTM+GRU model learns the long sequence of email text well. We have used word embedding, cnn, bidirectional lstm and gru networks as our three building blocks to separate email messages based on their sentiment and text\u2019s sequential features. Also, we succinctly demonstrate below why these blocks help identify email spam:\n\nincluding accuracy, precision, and recall, are used to examine the outcomes.\n\n3.3 Evaluation Metrics and Results Classifier performance is assessed Using metrics such as accuracy, precision, and recall. Four terms make up a confusion matrix that is used to calculate these metrics.\n\nTrue positives (TP) are positive values that have been accu- rately assigned the positive label.\n\nThe negative values that are accurately identified as negative are known as True Negatives (TN).\n\nTrue Negative values are those that can be accurately identi- fied as being negative (TN).\n\nPositive readings that have been mistakenly labeled as nega- tive are known as False Negatives (FN).\n\nAssess the efficacy of the suggested model is listed below:\n\n3.3.1 Accuracy. Accuracy reveals how frequently the ML model was overall correct.\n\n\ud835\udc47 \ud835\udc43 + \ud835\udc47 \ud835\udc41 \ud835\udc47 \ud835\udc43 + \ud835\udc47 \ud835\udc41 + \ud835\udc39 \ud835\udc43 + \ud835\udc39 \ud835\udc41\n\nAccuracy =\n\n(15)\n\n3.3.2 Precision. The accuracy of the model gauges how effectively it can predict a specific category.\n\n\ud835\udc47 \ud835\udc43 \ud835\udc47 \ud835\udc43 + \ud835\udc39 \ud835\udc43\n\nPrecision =\n\n(16)\n\nFirst, We have used the Sequence - to - sequence Lstm as the current block in the networks since it can retrieve both the previous and next sequences from the current. More so than a straightforward LSTM network, it can also recognize and extract text sentiment and sequential properties.\n\n3.3.3 Recall. Recall tells us how often the model was able to rec- ognize a specific category.\n\n\ud835\udc47 \ud835\udc43 \ud835\udc47 \ud835\udc43 + \ud835\udc39 \ud835\udc41\n\nRecall =\n\n(17)\n\nSecond, we extract the more complex and advanced charac- teristics for Bi-LSTM network using Convolutional Network block, which is the network\u2019s second block after the Bi-LSTM block. Bi-LSTM takes a long time to extract text-based fea- tures, hence one of the reasons for using this block is to reduce the network\u2019s overall training time.\n\nAccuracy 91.3 88.41 86.6 92.4 95.2 97.32\n\nModel Gaussian NB Random Forest KNN SVM LSTM Proposed Ensemble (CNN,BiLSTM+GRU)\n\nPrecision 90.1 90 89 91 95 95.6\n\nRecall 91.8 88 87 92 95.7 95.3\n\n3 EXPERIMENTAL EVALUATION 3.1 Experimental Setup We divided the information into training and testing groups of 80/20. We divided the remaining 20% of the 80 percent training data into test data for the model. Construct, compute, and evaluate the efficacy of the suggested method using the Pythonic packages Keras, as TensorFlow and Scikit learn.\n\nTable 1: Differet Model\u2019s Score on Test Data\n\nAccuracy, Precision, and Recall metrics are computed. In the given Table 1 where six different classifiers are Gaussian NB, Ran- dom Forest, KNN, SVM, LSTM, and Propose Ensemble Hybrid Model (CNN+BiLSTM+GRU) have been used in this work. In the CNN, Bi-LSTM, and GRU architectures which enable sequence pre- diction, CNN strands for feature extraction on data input which are combined with LSTM. It requires less time training and a higher expandable model. Any bottlenecks are created by predictions and the increasing number of distinct units of information. This model is useful for dealing with issue-related classifications that consist of two or more than two classes. So suggested Ensemble model, out of these six classifiers, produces more accurate findings.\n\n3.2 Dataset Description Email spam detection is the foundation of this research project. The dataset includes normal emails from the Enron corpora, deceptive emails from phished email corpora, harassment emails chosen from hate speech, and the offensive dataset. Only the content of the email body is used for analysis; all header information, including sender, topic, CC, and BCC, are eliminated. Word2vector, TF-IDF, and Word Embedding are used to extract characteristics from the email mes- sage and classify them. This dataset[8] is publicly available. The presented model is implemented using Python, and several metrics,\n\n",
    "186\n\nIC3 2023, August 03\u201305, 2023, Noida, India\n\nSachan et al.\n\nFigure 3: LSTM Model Training and Validation Loss\n\nFigure 1: Performance Analysis\n\n3.4 Comparative Analysis A model\u2019s ability to fit new data is measured by the validation loss, whereas its ability to fit training data is determined by the training loss. The two main variables that decide whether in which learning is efficient or not are validation loss and training loss. LSTM and Suggested Ensemble hybrid Models have equivalent loss and accuracy. In this context, we are contrasting the LSTM with the proposed model (CNN, Bilstm, and GRU) in terms of their respective validation accuracies and losses. The model\u2019s accuracy was at its highest after 14 epochs of operation when it achieved an accuracy of roughly 97-98% while minimizing model loss.\n\nFigure 4: Ensemble Model (CNN,BiLSTM+GRU) Training and Validation Accuracy\n\nFigure 5: Ensemble Model (CNN,BiLSTM+GRU)Training and Validation Loss\n\nFigure 2: LSTM Model Training and Validation Accuracy\n\n",
    "187\n\nIC3 2023, August 03\u201305, 2023, Noida, India\n\nSemantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model\n\nIn this Proposed ensemble hybrid model\u2019s train accuracy is 98.7% Validation accuracy is 97.32% and LSTM has train accuracy of 97.41% and validation accuracy is 95.2%. So based on figures 3 and 5 indicate the validation loss for LSTM and the proposed ensemble hybrid model to be 0.93 and 0.84, respectively, and figures 2 and 4 show the validation accuracy to be 95.2% and 97.3%, respectively. LSTM and the proposed hybrid model used ensemble artificial intelligence, with the proposed hybrid model outperforming the LSTM. We decide on dense architecture as the final model for identifying the text messages as spam or nonspam based on loss, accuracy, and the aforementioned charts. The loss and accuracy over epochs are more stable than LSTM, and the Proposed classifier has a straightforward structure.\n\nEgyptian Informatics Journal 15, 3 (2014), 169\u2013174.\n\n[15] V Priya, I Sumaiya Thaseen, Thippa Reddy Gadekallu, Mohamed K Aboudaif, and Emad Abouel Nasr. 2021. Robust attack detection approach for IIoT using ensemble classifier. arXiv preprint arXiv:2102.01515 (2021).\n\n[16] Justinas Rastenis, Simona Ramanauskait\u02d9e, Justinas Janulevi\u010dius, Antanas \u010cenys, Asta Slotkien\u02d9e, and K\u0119stutis Pakrijauskas. 2020. E-mail-based phishing attack taxonomy. Applied Sciences 10, 7 (2020), 2363.\n\n[17] Karthika D Renuka and P Visalakshi. 2014. Latent semantic indexing based SVM\n\nmodel for email spam classification. (2014).\n\n[18] Shuvendu Roy, Sk Imran Hossain, MAH Akhand, and N Siddique. 2018. Sequence modeling for intelligent typing assistant with Bangla and English keyboard. In 2018 International Conference on Innovation in Engineering and Technology (ICIET). IEEE, 1\u20136.\n\n[19] Tara N Sainath, Oriol Vinyals, Andrew Senior, and Ha\u015fim Sak. 2015. Convolu- tional, long short-term memory, fully connected deep neural networks. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP). Ieee, 4580\u20134584.\n\n[20] Anuj Kumar Singh, Shashi Bhushan, and Sonakshi Vij. 2019. Filtering spam messages and mails using fuzzy C means algorithm. In 2019 4th International Conference on Internet of Things: Smart Innovation and Usages (IoT-SIU). IEEE, 1\u20135.\n\n4 CONCLUSION The model is composed of four networks Word-Embeddings, CNN, Bi-LSTM, and GRU. We may train the model more quickly by using the convolutional layer first, followed by the word-embedding layer, and then the BiLSTM network. The Bidirectional LSTM network also has higher-level properties that we can extract. We have used a bidirectional LSTM(BiLSTM)and GRU network to memorize a sentence\u2019s contextual meaning and sequential structure, which im- proves the model\u2019s performance accuracy to roughly 97.32 percent.\n\n[21] Kristina Toutanova and Colin Cherry. 2009. A global model for joint lemmati- zation and part-of-speech prediction. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP. 486\u2013494.\n\n[22] Tian Xia. 2020. A constant time complexity spam detection algorithm for boosting throughput on rule-based filtering systems. IEEE Access 8 (2020), 82653\u201382661. [23] Yan Zhang, PengFei Liu, and JingTao Yao. 2019. Three-way email spam filtering with game-theoretic rough sets. In 2019 International conference on computing, networking and communications (ICNC). IEEE, 552\u2013556.\n\nReceived 15 April 2023\n\nREFERENCES [1] Rayan Salah Hag Ali and Neamat El Gayar. 2019. Sentiment analysis using unla- beled email data. In 2019 International Conference on Computational Intelligence and Knowledge Economy (ICCIKE). IEEE, 328\u2013333.\n\n[2] Ali Shafigh Aski and Navid Khalilzadeh Sourati. 2016. Proposed efficient algo- rithm to filter spam using machine learning techniques. Pacific Science Review A: Natural Science and Engineering 18, 2 (2016), 145\u2013149.\n\n[3] Huwaida T Elshoush and Esraa A Dinar. 2019. Using adaboost and stochastic gradient descent (sgd) algorithms with R and orange software for filtering e-mail spam. In 2019 11th Computer Science and Electronic Engineering (CEEC). IEEE, 41\u201346.\n\n[4] Weimiao Feng, Jianguo Sun, Liguo Zhang, Cuiling Cao, and Qing Yang. 2016. A support vector machine based naive Bayes algorithm for spam filtering. In 2016 IEEE 35th International Performance Computing and Communications Conference (IPCCC). IEEE, 1\u20138.\n\n[5] Pranjul Garg and Nancy Girdhar. 2021. A Systematic Review on Spam Filtering Techniques based on Natural Language Processing Framework. In 2021 11th Inter- national Conference on Cloud Computing, Data Science & Engineering (Confluence). IEEE, 30\u201335.\n\n[6] Adam Kavon Ghazi-Tehrani and Henry N Pontell. 2021. Phishing evolves: Ana- lyzing the enduring cybercrime. Victims & Offenders 16, 3 (2021), 316\u2013342. [7] Radicati Group et al. 2015. Email Statistics Report 2015\u20132019. Radicati Group.\n\nAccessed August 13 (2015), 2019.\n\n[8] Maryam Hina, Mohsin Ali, and Javed. 2021. Sefaced: Semantic-based forensic IEEE Access 9\n\nanalysis and classification of e-mail data using deep learning. (2021), 98398\u201398411.\n\n[9] Maryam Hina, Mohsin Ali, Abdul Rehman Javed, Fahad Ghabban, Liaqat Ali Khan, and Zunera Jalil. 2021. Sefaced: Semantic-based forensic analysis and classification of e-mail data using deep learning. IEEE Access 9 (2021), 98398\u2013 98411.\n\n[10] Weicong Kong, Zhao Yang Dong, Youwei Jia, David J Hill, Yan Xu, and Yuan Zhang. 2017. Short-term residential load forecasting based on LSTM recurrent neural network. IEEE transactions on smart grid 10, 1 (2017), 841\u2013851.\n\n[11] T Kumaresan and C Palanisamy. 2017. E-mail spam classification using S-cuckoo search and support vector machine. International Journal of Bio-Inspired Compu- tation 9, 3 (2017), 142\u2013156.\n\n[12] Nuha H Marza, Mehdi E Manaa, and Hussein A Lafta. 2021. Classification of spam emails using deep learning. In 2021 1st Babylon International Conference on Information Technology and Science (BICITS). IEEE, 63\u201368.\n\n[13] Tomas Mikolov and Geoffrey Zweig. 2012. Context dependent recurrent neural network language model. In 2012 IEEE Spoken Language Technology Workshop (SLT). IEEE, 234\u2013239.\n\n[14] Sarwat Nizamani, Nasrullah Memon, Mathies Glasdam, and Dong Duong Nguyen. 2014. Detection of fraudulent emails by employing advanced feature abundance.\n\n"
]