[
    "Availableonline12December20230965-9978/\u00a92023PublishedbyElsevierLtd.\n\nAdvancesinEngineeringSoftware188(2024)103568\n\nContents lists available at ScienceDirect\n\nAdvances in Engineering Software\n\njournal homepage: www.elsevier.com/locate/advengsoft\n\nImproved stochastic subset optimization method for structural design optimization\n\nMohd Aman Khalid , Sahil Bansal *\n\nDepartment of Civil Engineering, Indian Institute of Technology Delhi, New Delhi 110016, India\n\nA R T I C L E I N F O\n\nA B S T R A C T\n\nKeywords: Stochastic subset optimization Voronoi tessellation Stochastic simulation Stochastic optimization Optimization under uncertainty\n\nThe Stochastic Subset Optimization (SSO) algorithm was proposed for optimal reliability problems that mini- mizes the probability of system failure over the admissible space for the design parameters. It is based on the simulation of samples of the design parameters from an auxiliary Probability Density Function (PDF) and exploiting the information contained in these samples to identify subregions for the optimal design parameters within the original design space. This paper presents an improved version of SSO, named iSSO to overcome the shortcomings in the SSO. In the improved version, the Voronoi tessellation is implemented to partition the design space into non-overlapping subregions using the pool of samples distributed according to the auxiliary PDF. A double-sort approach is then used to identify the subregions for the optimal design. The iSSO is presented as a generalized design optimization approach primarily tailored for the stochastic structural systems but also adaptable to deterministic systems. Several optimization problems are considered to illustrate the effectiveness and efficiency of the proposed iSSO.\n\napproaches available in the literature, however, but it\u2019s worth noting that no one-size-fits-all optimization approach is ideal for all sorts of problems [4\u20137]. The choice of optimization method is often determined by the specific characteristics of the problem, such as its complexity, dimensionality, constraints, and the nature of the objective function. As a result, there is always a scope for new approaches to be developed or the adaptation of existing methods to better suit specific problem clas- ses. A detailed discussion of deterministic optimization approaches can be found in the literature [8,9].\n\n1. Introduction\n\nStructural optimization may be defined as the rational establishment of an economical structural design with the available resources while satisfying specific performance criteria. In general terms, the economy may be characterized by minimum weight, minimum cost, maximum utility, or even minimum probability of failure. Broadly, structural optimization can be categorized into deterministic and stochastic opti- mization [1,2]. The classical statement of unconstraint deterministic optimization is mathematically expressed as:\n\nIn any practical situation, several parameters, such as loadings, structural parameters, geometric parameters, operation conditions, etc., are either not known at the design stage or are subjected to random fluctuations that give rise to performance variability and affect the performance of a system [10]. These parameters are characterized as uncertain parameters. Deterministic structural optimization discards the impact of uncertainty and can result in improper design. Therefore, it is desirable to account for the uncertainty in the parameters during opti- mization by using the rational methods of probabilistic structural analysis [11]. Such structural optimization that accounts for un- certainties is called stochastic optimization [12]. Although stochastic optimization refers to any method that employs randomness within some communities, in this paper, we will only consider settings where\n\nminimize : \u03c6\u2208\u03a6\n\ng(\u03c6)\n\n(1)\n\n]T \u2208 \u03a6\u2282Rn\u03c6 is a set of deterministic adjustable pa- where, \u03c6 = [\u03c61\u22ef\u03c6n\u03c6 rameters that define the structural design, referred to herein as design parameters, g(\u03c6) : Rn\u03c6 \u2192R is the objective function to be minimized, and \u03a6 denotes the bounded admissible design space. The deterministic constraints can be considered by the appropriate definition of the ad- missible design space \u03a6 for deterministic design parameters \u03c6, as mentioned in [3]. In the deterministic structural optimization problem, the uncertainties in parameters are ignored, and fixed values are assumed for all the parameters. There are numerous optimizations\n\nCorresponding author.\n\nE-mail addresses: mohdamankhalid@gmail.com (M.A. Khalid), sahil@iitd.ac.in (S. Bansal).\n\nhttps://doi.org/10.1016/j.advengsoft.2023.103568 Received 5 June 2023; Received in revised form 2 October 2023; Accepted 24 November 2023\n\n",
    "AdvancesinEngineeringSoftware188(2024)1035682\n\nM.A. Khalid and S. Bansal\n\nthe objective function is random. Stochastic optimization or optimal design under uncertainty has been widely applied in many practical engineering fields, including civil engineering structures [13\u201315], composite structures [16,17], and vehicles [18,19].\n\nvarious challenging optimization problems, it has two shortcomings. First, the effectiveness of SSO is dependent on the correct selection of the geometrical shape of the admissible subsets. Here, it is pertinent to mention that choosing a geometrical shape that effectively investigates the sensitivity of the objective function to each design variable is essential. The shapes, such as hyper-rectangle and hyper-ellipse are suggested in the literature for the admissible subsets. However, as shown later via the illustrative example, these shapes fail to include the optimal solution in cases with complex design spaces or problems with multiple optimal solutions. And second, identifying the optimal subset that con- tains the smallest volume density involves a non-smooth optimization problem which is quite challenging.\n\nConsider an engineering system that involves deterministic design parameters \u03c6, and uncertain variables \u03b8 = [\u03b81\u22ef\u03b8n\u03b8 ]T \u2208 \u0398\u2282Rn\u03b8 following a joint PDF p(\u03b8|\u03c6), where \u0398 denotes the parameter space of the uncer- tain variables. The classical statement of stochastic optimization is mathematically expressed as:\n\nminimize : \u03c6\u2208\u03a6\n\nE\u03b8[h(\u03c6, \u03b8)]\n\n(2)\n\nwhere, h(\u03c6, \u03b8) : Rn\u03b8 +n\u03c6 \u2192R is the structural performance function, and E\u03b8[ \u22c5 ] denotes expectation with respect to the PDF for \u03b8. Note that the objective function in the optimization problem in (2) is the expectation E\u03b8[h(\u03c6, \u03b8)] which is a deterministic function. It\u2019s worth mentioning that stochastic optimization may also involve other stochastic measures such as variance or quantile values. However, these stochastic measures can rarely be evaluated analytically; therefore, several methods have been proposed for solving stochastic optimization problems. These special- ized methods include, for example, sample average approximation, stochastic approximation, stochastic subset optimization, and ap- proaches based on the use of Taylor series expansion [15,20,21], response surface, and metamodels [22\u201325]. Specific to structural engi- neering, there are two broad categories of problems involving design optimization under uncertainty [26\u201335]: Reliability-Based Design Optimization (RBDO) and Robust Design Optimization (RDO). The objective of RBDO is to find an optimal solution that minimizes some deterministic, objective function under observance of probabilistic constraints instead of conventional deterministic constraints [36,37]. On the other hand, RDO aims to find an optimal solution that is insen- sitive (or less sensitive) to input variations. It improves the design quality by minimizing performance variation without eliminating un- certainty [29,38].\n\nIn this paper, an improved version of SSO is developed to overcome the shortcomings of the original SSO. This new version of the algorithm, as mentioned earlier, is named iSSO (improved SSO). Voronoi tessella- tion is implemented to partition the design space into non-overlapping subregions (a set of Voronoi cells) using the pool of samples distrib- uted according to the auxiliary PDF. The admissible set (a set of all admissible subregions) is then defined as a set containing all subsets of the set of Voronoi cells. This approach is able to capture the regions with lower objective function values even if they are disjointed or when the design space is complex. The details of the Voronoi tessellation are presented in Appendix A. A double-sort algorithm is then implemented to identify the optimal subset containing the smallest volume density.\n\nIn the next section, the original SSO is reviewed. Section 3 presents the general theoretical and computational framework for the iSSO al- gorithm. Section 4 considers several optimization problems to illustrate the effectiveness and efficiency of the proposed iSSO algorithm.\n\n2. Original stochastic subset optimization\n\nIn SSO, say at the i + 1th iteration, the design space is represented by a subset I(i), where I(i) \u2208 I(i (cid:0) 1)\u22c5\u22c5\u22c5 \u2208 I(0) \u2208 \u03a6. Following the augmented formulation concept initially discussed in [44] for RBDO, the design parameters \u03c6, are artificially considered uncertain variables with a prescribed PDF p(\u03c6|I(i)) over the design space I(i) [45]. For convenience, p(\u03c6|I(i)) = 1/V(i) is considered, where V(i) is the volume of I(i). In this setting of the augmented stochastic design problem, the auxiliary PDF is defined as:\n\nTaflanidis and Beck [39] introduced a novel algorithm for optimal reliability problem, the so-called SSO. SSO involves formulating an augmented problem where the design parameters are artificially considered uncertain and defining an auxiliary PDF that includes the structural performance function and the PDF of the uncertain variables. Next, SSO involves generating a pool of samples distributed according to this auxiliary PDF and identifying a subregion in the original design space, which, on average, improves the value of the objective function. By repeating this procedure several times, it is possible to determine at each step a smaller subregion in the design space, which in turn im- proves the value of the objective function. Ultimately, this subregion will be sufficiently small to directly identify the optimal solution or provide sufficient information to launch another optimization algo- rithm, such as the sample average approximation or stochastic approx- imation. The implementation of the SSO method closely resembles the Subset Simulation (SS) algorithm [40] for reliability analysis. Since SSO is based on simulation, it can deal with linear or nonlinear problems and, at least theoretically, an unbounded number of design parameters. The numerical effort for solving a given optimization problem is indepen- dent of the number of uncertain variables, and it grows linearly with the number of design parameters.\n\n\u20d2 \u20d2I(i) E\u03c6,\u03b8[hs(\u03c6, \u03b8)]\n\n)\n\n(cid:0)\n\n\u20d2 \u20d2I(i) \u03c6, \u03b8\n\n\u20d2 \u20d2I(i)\n\n(cid:0)\n\n)\n\n)\n\n(cid:0) \u221dh(\u03c6, \u03b8)p\n\nh(\u03c6, \u03b8)p\n\n\u03c6, \u03b8\n\n\u03c6, \u03b8\n\n=\n\n\u03c0\n\n(3)\n\nwhere, p(\u03c6, \u03b8|I(i)) = p(\u03b8|\u03c6)p(\u03c6|I(i)). Note that if h(\u03c6, \u03b8)\u2264 0, it must be suitably transformed to ensure that \u03c0(\u03c6, \u03b8|I(i)) \u2265 0. One way to do this is to define hs(\u03c6,\u03b8) = h(\u03c6, \u03b8) (cid:0) s, that is, the two expected values differ only by a constant, and the optimization of the expected value of h( \u22c5 ) is equivalent, in terms of the optimal design choice, to optimization for the expected value for hs( \u22c5 ). In the above equation, the denominator is a normalizing constant given by: \u20d2 \u20d2I(i)\n\ns, since E\u03b8[hs(\u03c6,\u03b8)] = E\u03b8[h(\u03c6,\u03b8)] (cid:0)\n\n\u222b\n\n\u222b\n\n) d\u03b8d\u03c6.\n\n(cid:0) h(\u03c6, \u03b8)p\n\n\u03c6, \u03b8\n\nE\u03c6,\u03b8[h(\u03c6, \u03b8)] =\n\n(4)\n\n\u03a6\n\n\u0398\n\nAlthough this expected value is not explicitly needed, it can be determined using any state-of-the-art stochastic simulation method. The objective function E\u03b8[hs(\u03c6, \u03b8)] in this context of the auxiliary PDF is expressed as:\n\nSince the introduction of SSO, several extensions of SSO have been proposed. An extension of SSO termed Non-Parametric SSO, which adopts kernel density estimation to approximate the objective function, is presented in [41]. In [42], efficient integration of the Moving Least Squares approximation within SSO is introduced to reduce the compu- tational effort in SSO. In [3], an augmented formulation is presented for the RDO of structures using SSO. SSO or its variants have also been applied to solve structural optimization problems. SSO has been used for reliability optimization and sensitivity analysis in system design in [39]. A framework for RDO of Tuned Mass Dampers (TMD) by SSO is dis- cussed in [43]. Even though SSO has proved to be efficient for meeting\n\n\u20d2 \u20d2I(i) \u03c6 \u20d2 \u20d2I(i) \u03c6\n\n(cid:0)\n\n) )E\u03c6,\u03b8[h(\u03c6, \u03b8)],\n\n\u03c0 (cid:0) p\n\nE\u03b8[h(\u03c6, \u03b8)] =\n\n(5)\n\nwhere, the marginal \u03c0(\u03c6|I(i)) is given by: (cid:0)\n\n\u222b\n\n\u20d2 \u20d2I(i) \u03c6\n\n)\n\n=\n\n\u03c0(\u03c6, \u03b8)d\u03b8.\n\n\u03c0\n\n(6)\n\nI(i)\n\nIn (5), since E\u03c6,\u03b8[h(\u03c6, \u03b8)] is a normalizing constant, minimization of\n\n",
    "AdvancesinEngineeringSoftware188(2024)1035683\n\nM.A. Khalid and S. Bansal\n\nE\u03b8[h(\u03c6, \u03b8)] is equivalent to minimization of J(\u03c6), which is equal to: ) ).\n\n/\n\n\u20d2 \u20d2I(i) \u03c6 \u20d2 \u20d2I(i) \u03c6\n\n(cid:0)\n\nI(i+1) = argmin I\u2208A\u03c1\n\nH(I) = arg min\n\nVI\n\nNI\n\n\u20d2 \u20d2I(i) \u03c6\n\n(cid:0)\n\n)\n\n\u03c0 p\n\nE\u03b8[hs(\u03c6, \u03b8)] E\u03c6,\u03b8[hs(\u03c6, \u03b8)]\n\n.\n\n(i+1) I\u2208A \u03c1 } / N(i)\n\n(cid:0)\n\n=\n\n=\n\n(10)\n\nJ\n\n(7)\n\n{\n\nA(i+1) \u03c1 =\n\nI\u2282I(i) : \u03c1 = NI\n\nThe estimation of the marginal \u03c0(\u03c6|I(i)) in (7) is necessary to mini- mize J(\u03c6|I(i)). Analytical approximations of \u03c0(\u03c6|I(i)) based on kernel density approaches or the maximum entropy method might be arduous in case of complex problems, such as when design parameters n\u03c6 are large, or the sensitivity for some design parameters is complex [44]. In the SSO framework, such approximation of \u03c0(\u03c6|I(i)) is avoided. In SSO, samples distributed as \u03c0(\u03c6|I(i)) are obtained, and the information in these samples is exploited to identify a smaller subset of the design space with a high likelihood of containing the optimal design parameters. Samples distributed as \u03c0(\u03c6, \u03b8|I(i)) are obtained using any appropriate stochastic sampling algorithm, such as Markov Chain Monte Carlo (MCMC) sampling [46]. The \u03c6 component of these samples then cor- responds to samples from the marginal distribution \u03c0(\u03c6|I(i)).\n\nThe effectiveness of SSO is dependent on the correct selection of the geometrical shape and size of the admissible subsets. Choosing a geometrical shape that effectively investigates the sensitivity of the objective function to each design variable is essential. The optimization in (10) determines the subset with the smallest average value of J(\u03c6|I(i)) . I(i + 1) is a (or equivalently E\u03b8[hs(\u03c6, \u03b8)]) within the admissible set A subset of the design space I(i) with a high likelihood of containing the optimal design parameters. The above steps are repeated until the stopping criterion is met. This way, SSO adaptively converges to a relatively small subregion within the original design space. The imple- mentation of SSO is demonstrated in Fig. 1. The reader may refer to the original publication for a detailed explanation of SSO [39].\n\n(i+1) \u03c1\n\n(i)) expresses the average relative sensitivity of E\u03b8[h(\u03c6,\u03b8)] to \u03c6. A (i)) indicates that E\u03b8[h(\u03c6,\u03b8)] is more sensitive to \u03c6, and low value of H(I (i)), close to 1 corresponds to a sample vice versa. A high value of H(I density in design space I(i) that approximates a uniform distribution and suggests that the identified subset I(i) has a low likelihood of containing (i)) exceeds a threshold \u03c6* [39]. Therefore, the SSO is stopped when H(I value. A threshold value of 0.75\u20130.80 has been found to give satisfactory results [39].\n\nH(I\n\nThe sensitivity of objective function E\u03b8[hs(\u03c6, \u03b8)] to \u03c6 is determined by evaluating the average value (or equivalently volume density) of J(\u03c6| I(i)) over any subset I in I(i), which is denoted by H(I) and defined as:\n\n(cid:0)\n\n) ) d\u03c6 =\n\n\u222b\n\n\u222b\n\n\u222b\n\n)\n\n(cid:0)\n\n) d\u03c6\n\n(cid:0)\n\n\u03c6|I(i) \u03c6|I(i)\n\n\u03c0 p\n\n1 VI\n\n1 VI\n\nVI(i) VI\n\n\u03c6|I(i)\n\n\u03c6|I(i)\n\n(cid:0)\n\nH(I) =\n\nd\u03c6 =\n\n\u03c0\n\nJ\n\n(8)\n\nI\n\nI\n\nI\n\nwhere, VI is the volume of subset I. Based on the samples distributed according to \u03c0(\u03c6|I(i)) belonging to I(i), an estimate of H(I) is provided by:\n\nNI/VI NI(i) /VI(i)\n\n3. Proposed approach\n\nH(I) =\n\n,\n\n(9)\n\nIn the proposed approach, the Voronoi tessellation is implemented to partition the design space into non-overlapping subregions (a set of Voronoi cells) using the pool of samples distributed according to this auxiliary PDF. Conceptually, Voronoi tessellation involves partitioning a space into convex polygons, called Voronoi cells, such that each cell contains exactly one sample, called a cell-generating sample. Every sample in a given polygon is closer to its generating sample compared to any other. In the proposed approach, the admissible set (a set of all admissible subspaces) is defined as a set containing all subsets of the set of Voronoi cells. An alternative approach to identify the optimal subset without performing any non-smooth deterministic optimization is also presented. The general theoretical and computational framework for the iSSO algorithm is presented in the following subsections, and the\n\nwhere, NI(i) is the number of samples distributed as \u03c0(\u03c6|I(i)) belonging to I(i), and NI denotes the number of samples from \u03c0(\u03c6|I(i)) belonging to the I (NI < NI(i(cid:0) 1) since I\u2282I(k (cid:0) 1)). Say NI = p0NI(i(cid:0) 1) . A smaller value of \u03c1 re- sults in a faster decrease in the size of the identified subsets but with poorer accuracy. The use of \u03c1 equal to 0.1 - 0.2 is suggested in the literature [39].\n\nA deterministic optimization, based on the estimate H(I) of H(I), is (i+1) next performed to identify the subset I \u2208 A is a set of \u03c1 admissible subsets in I(i), that contains the smallest volume density NI/ VI, that is,\n\n(i+1) \u03c1\n\n, where A\n\nFig. 1. Illustration of the original SSO algorithm.\n\n",
    "AdvancesinEngineeringSoftware188(2024)1035684\n\nM.A. Khalid and S. Bansal\n\nalgorithm is demonstrated in Fig. 2.\n\n3.2. Identification of an optimal subset\n\n3.1. Partitioning of design space\n\nA deterministic optimization needs to be performed to identify a subset I that contains the smallest volume density NI/VI. In the case of unique samples, since \u03b7(i) (\u22c5) = 1, the solution to the minimization problem in (10) is a set of \u03c1N(i) Voronoi cells with the largest volume. For the case with repeated samples, the optimization can be performed using methods appropriate for non-smooth optimization problems, such as sub-gradient methods, bundle methods, gradient sampling methods, etc. In this study, we propose an alternative approach to identify the optimal subset without performing any non-smooth deterministic opti- mization. A double-sort algorithm is proposed, which involves sorting the Voronoi cells in ascending order of the sample counts and then in groups of cells with the same sample count in descending order of cell volume. Finally, the top cells containing \u03c1N(i)samples are selected as an approximate optimal solution from the sorted list.\n\nIn the proposed approach, at the i + 1th iteration, say N(i) is the number of samples distributed as \u03c0(\u03c6|I(i)) belonging to the design space (i) /(1 + \u03b3), \u03b3 \u2265 0 be the number of unique samples. If I(i). Let nv = N sampling techniques such as accept rejection, importance sampling, etc., are used, then \u03b3 = 0, and each sample in the design space will be unique. However, if MCMC sampling techniques are used, the resulting samples will be correlated, that is \u03b3 > 0, and we will have repeated samples. (i) Assume that the design space I(i) is divided into v k , k = 1\u22c5\u22c5\u22c5nv, Voronoi k contains \u03b7(i) (i)\n\ncells using nv unique samples, and say the Voronoi cell v repeated samples, then, an estimate of \u03c0(\u03c6|I(i)) is provided by:\n\nk\n\n\u03b7(i) k N(i)V (i) k\n\n(cid:0)\n\n)\n\n\u2200 \u03c6 \u2208 v(i) k ,\n\n\u03c6|I(i)\n\n=\n\n\u2265 0,\n\n\u03c0\n\n(11)\n\nOne may argue that the optimal subset can be obtained by first sorting the Voronoi cells in ascending order of the cell density, defined as \u03b7(i) (i) k , and then by selecting the top cells containing \u03c1N(i) samples from the sorted list. However, this argument is erroneous because the \u2211 (i) S (s)). The s=1V effectiveness of the proposed double-sort algorithm is demonstrated in Section 4 with the help of examples.\n\n\u222b\n\n(i) k is the volume of the kth Voronoi cell. Obviously,\n\n(i))\n\nI(i) \u03c0(\u03c6|I\n\nwhere, V d\u03c6 = 1.\n\n/V\n\nk\n\nSimilar to the original SSO, the sensitivity of the objective function J (\u03c6|I(i)) to \u03c6 is determined by evaluating the average value of J(\u03c6|I(i)) over any subspace I of the design space I(i). Subset I is any subset of nvVoronoi cells (these cells may be disjointed). Since the design space is partitioned into nv subspaces or Voronoi cells, the number of admissible subsets (proper subsets) is given by 2n\u03bd (cid:0) 1. Based on the estimate \u03c0(\u03c6|I\n\n\u2211\n\n\u2211\n\ns=1\u03b7(i) (s)/\n\n(i) (s) and not\n\n(\u03b7(i)\n\nS\n\nS s=1\n\n(s) /V\n\nobjective is to minimize\n\n3.3. Simulation of conditional samples\n\n(i)) provided in (11), an estimate of H(I) is provided as:\n\nAt the i + 1th iteration, \u03c1N(i) samples distributed as \u03c0(\u03c6|I(i + 1)) are available from the previous iteration. Using these samples as seeds, \u03c1)N(i + 1) are simulated. The proposed method to additional (1 (cid:0) simulate additional samples involves two steps: (a) randomly selecting a Voronoi cell within the subset I(i + 1) based on the estimate \u03c0(\u03c6|I (i)) and (b) applying the Metropolis-Hastings algorithm within the selected Voronoi cell.\n\n\u2211\n\n\u222b\n\nI\u03b7(i) i N(i)\n\n(cid:0)\n\n) d\u03c6 =\n\nV (i) VI\n\nV (i) VI\n\nV (i) VI\n\nNI N(i)\n\n\u03c6|I(i)\n\nH(I) =\n\n=\n\n\u03c0\n\n(12)\n\nI\n\nwhere, VI is the volume of the subset I and NI is the number of samples (i) belonging to it. Let I = {v (S)}, where S is the number of Vor- onoi cells defining the subset I. Note that the parentheses are used in the subscript to differentiate between the Voronoi cell number defined in the previous section from the Voronoi cell index describing the subset I. An estimate of H(I) is then provided as:\n\n(i) (1), v\n\n(i) (2)\u22efv\n\nA Voronoi cell is selected according to the following weights in the\n\nfirst step:\n\n/\n\n\u03b7(i) k \u2211 \u03b7(i) k\n\nV (i) k / V (i) k\n\n[\n\n]\n\nw(i)\n\n\u03b7(i) (1) + \u03b7(i) (1) + V (i) V (i)\n\n(2) + \u22ef + \u03b7(i) (2) + \u22ef + V (i)\n\nk =\n\n.\n\nV (i) N(i)\n\n(14)\n\n(S)\n\n.\n\nH(I) =\n\n(13)\n\nk\n\n(S)\n\nFig. 2. Illustration of the proposed iSSO algorithm.\n\n",
    "AdvancesinEngineeringSoftware188(2024)1035685\n\nM.A. Khalid and S. Bansal\n\nTo simulate a new sample within a selected Voronoi cell, the sample that generated the selected Voronoi cell or the last simulated sample in the selected Voronoi cell is used as the seed sample, and the Metropolis- Hastings algorithm is implemented. A candidate sample [\u03c6c,\u03b8c] is simulated using the proposal q(\u03c6c,\u03b8c|\u03c6,\u03b8) and is accepted with the probability min(1, a0), where, a0 is given as:\n\n3.5. Implementation issues\n\nAn important issue for the effective implementation of the iSSO is the creation of the Voronoi cells at the current iteration bounded within the Voronoi cell created at the previous iterations. Although it is possible to create such bounded Voronoi cells, due to the geometrical complexities, it is usually unfeasible for the higher dimensional problems (n\u03c6>2). An alternative approach is proposed in the present study for creating the Voronoi cells at any iteration of the iSSO. The proposed approach in- volves creating Voronoi cells using the samples generated at the current and all previous iterations and then by considering Voronoi cells cor- responding to the samples from the current iteration. This is shown in Fig. 3, where Fig. 3(a) shows the N samples at the first iteration and the corresponding Voronoi cells. Fig. 3(b) shows the \u03c1N selected Voronoi cells leading to the smallest volume density and the additional (1 (cid:0) \u03c1)N samples being generated using these \u03c1N samples as seeds. Fig. 3(c) shows that the Voronoi cells are generated using all N + (1 (cid:0) \u03c1)N samples that are generated in the two iterations. The Voronoi cells corresponding to the N samples for consideration at the second iteration are also highlighted in Fig. 3(c). Fig. 3(d) shows a zoomed-in version of Fig. 3(c) where it can be observed that the area covered by the N Voronoi cells considered in the second iteration is not the same as the area covered by the \u03c1N Voronoi cells selected in the first iteration. On the contrary, the area covered by the Voronoi cells in the second iteration is more than the area covered by the Voronoi cells corresponding to the seed samples from the first iteration. This is because a new sample within the Voronoi cell between an existing sample and the existing Voronoi cell edge results in the relocation of the Voronoi cell edge in a\n\nh(\u03c6c, \u03b8c)p(\u03c6c, \u03b8c)q(\u03c6, \u03b8|\u03c6c, \u03b8c) . h(\u03c6, \u03b8)p(\u03c6, \u03b8)q(\u03c6c, \u03b8c|\u03c6, \u03b8)\n\na0 =\n\n(15)\n\nIn the present study, the proposed PDF is equal to the uniform PDF for design parameters and the initial PDF for uncertain variables, i.e., q (\u03c6, \u03b8|\u03c6c,\u03b8c) = p(\u03c6, \u03b8). Therefore, on simplifying (15), a0 is given as:\n\nh(\u03c6c, \u03b8c) . h(\u03c6, \u03b8)\n\na0 =\n\n(16)\n\n3.4. Stopping criteria\n\nA new stopping criterion is proposed in this study. The convergence of the expected value of the performance measure h(\u03c6, \u03b8) with respect to the PDF for \u03c6 and \u03b8 in consecutive iterations is used as the stopping criterion. Mathematically the proposed stopping criterion is represented by: \u20d2 \u20d2E\u03c6,\u03b8[h(\u03c6, \u03b8)]i (cid:0) E\u03c6,\u03b8[h(\u03c6, \u03b8)]i(cid:0) 1\n\n\u20d2 \u20d2 \u2264 \u03b5\n\n(17)\n\nwhere, \u03b5 is a user-specified tolerance limit. Other stopping criteria, as indicated in [39,47], can also be chosen.\n\nFig. 3. Implementation of Voronoi tessellation in iSSO.\n\n",
    "AdvancesinEngineeringSoftware188(2024)1035686\n\nM.A. Khalid and S. Bansal\n\ndirection away from the new sample. The increase at each iteration in- troduces a bias in the estimate of \u03c0(\u03c6|I(i)) in (11). However, this does not affect the performance of the proposed approach as the objective is not to simulate the samples distributed as \u03c0(\u03c6|I(i)) but to identify the subsets for an optimal solution. In addition, the increase is not substantial, as seen later in the illustrative examples in Section 4.\n\nperformed. In the third example, the mean minimization of 120 bars truss problems is explored to demonstrate the applicability of the pro- posed approach to a high-dimensional stochastic design problem. Finally, the fourth example investigates the reliability-based optimiza- tion of a base isolation system for a 10-story building.\n\nIn this study, after implementing iSSO, the optimal design solution is identified as follows. Let \u03b8j, j = 1\u22c5\u22c5\u22c5n be a set of independent, identically distributed realizations of \u03b8, and let h(\u03c6, \u03b8j) be the structural perfor- mance function realization for \u03b8j. The expected structural performance function is approximated by the average of the realizations as:\n\n3.6. Special case: deterministic optimization\n\nIn the iSSO framework, a deterministic optimization problems can also be handled with the vector of uncertain variables \u03b8 set equal to a null vector (n\u03b8 = 0). Since the determination of the subset at each iSSO iteration is solely dependent on the samples distributed as \u03c0(\u03c6), no modification to the iSSO algorithm is required to solve a deterministic optimization problem, and the entire formulation remains valid.\n\n\u2211n\n\n) .\n\n(cid:0) h\n\n1 n\n\nE\u03b8[h(\u03c6, \u03b8)] \u2248\n\n\u03c6, \u03b8j\n\n(18)\n\nj=1\n\nE\u03b8[h(\u03c6, \u03b8)] is evaluated for all unique \u03c6 samples obtained at the last iteration of the iSSO, and the \u03c6 sample resulting in the smallest value of E\u03b8[h(\u03c6, \u03b8)] is taken as the optimal solution. Alternatively, as the right- hand side of (18) is deterministic, any deterministic optimization method can also be used to solve the optimization problem with the approximate expectation.\n\n4. Illustrative examples\n\nIn this section, typical optimization problems are considered to demonstrate the effectiveness and efficiency of the proposed approach. First, deterministic optimization problems are considered. These prob- lems include several local and global minima. Next, stochastic optimi- zation problems are illustrated. The second example presents an RDO problem of the TMD. In this example, the variance minimization of the protected structure\u2019s displacement (TMD attached to the structure) is\n\nIn the following examples, both iSSO and SSO are implemented with N = 1000n\u03c6, \u03c1 = 0.20 and the stopping criteria as stated in (17). Here, a value of \u03b5 = 10\n\n(cid:0) 3 is adopted.\n\nFig. 4. Results for the Griewank function.\n\n",
    "AdvancesinEngineeringSoftware188(2024)1035687\n\nM.A. Khalid and S. Bansal\n\nFig. 5. Results for the Cross-in-Tray function.\n\n\u221a\n\n\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305 \u20d2 \u20d2 1 + \u03c62 \u03c62 \u20d2 \u20d2 \u20d2 \u03c0\n\n4.1. Multimodal deterministic optimization problems\n\n)\u20d2 \u20d2 \u20d2 \u20d2 \u20d2\n\n(\u20d2 \u20d2 \u20d2 \u20d2 \u20d2\n\n\u20d2 \u20d2 \u20d2 \u20d2 \u20d2\n\n2\n\n,\n\n1 (cid:0)\n\nminh(\u03c6) = (cid:0)\n\nsin(\u03c61)cos(\u03c62)exp\n\nIn this section, three two-dimensional benchmark deterministic optimization problems are considered. Results are also compared with the SSO. The test functions are:\n\n(21))\n\ns.t.\u03c6 = [ (cid:0) 10, 10]\n\na) Griewank function:\n\n(\n\n)\n\n\u2211d\n\n\u220fd\n\n\u03c62 i 4000\n\n\u03c6i\u0305\u0305 \u221a i\n\nThe results for the Griewank function are presented in Fig. 4. Fig. 4(a, b) shows that the function has multiple closely spaced local minima with a single global minimum. Fig. 4(c, d) shows the SSO optimization using hyper-rectangle and hyper-ellipse as shapes of admissible subsets. It is seen that these shapes fail to capture the region containing the optimal design due to the presence of multiple local minima. Next, the iSSO is implemented, where the Voronoi cells selected at the first and last iteration are shown in Fig. 4(e, f). It is observed that at the first iteration, the selected Voronoi cells effectively capture both the local and global minima and in the subsequent iterations, the selected cells are more concentrated near the global minimum. The region selected at the last iteration captures the optimal global solution.\n\nminh(\u03c6) =\n\n+ 1,\n\n(cid:0)\n\ncos\n\n(19)\n\ni=1\n\ni=1\n\ns.t.\u03c6 = [ (cid:0) 10, 10]\n\nb) Cross-in-Tray function:\n\n(\u20d2 \u20d2 \u20d2 \u20d2 \u20d2\n\n\u20d2 \u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305 \u20d2 1 + \u03c62 \u03c62 \u20d2 2 \u20d2 \u20d2 \u03c0\n\n)\u20d2 \u20d2 \u20d2 \u20d2 \u20d2\n\n(\u20d2 \u20d2 \u20d2 100 (cid:0) \u20d2 \u20d2\n\n)0.1\n\n\u221a\n\nminh(\u03c6) = (cid:0) 0.0001\n\nsin(\u03c61)sin(\u03c62)exp\n\n+ 1\n\n,\n\ns.t.\u03c6 = [ (cid:0) 10, 10]\n\n(20)\n\nThe Cross-in-Tray function has a relatively complex design space compared to the Griewank function. Fig. 5(a, b) shows multiple local and global minima. Minimization by using SSO is demonstrated in Fig. 5 (c, d). It is found that both the hyper-rectangle and hyper-ellipse are trapped around any one of the global minima. At the same time, the iSSO is able to capture the regions that include all of the global minima, as\n\nc) Holder Table function:\n\n",
    "AdvancesinEngineeringSoftware188(2024)1035688\n\nM.A. Khalid and S. Bansal\n\nFig. 6. Results for the Holder Table function.\n\n(1)) for the 50 indepen- three functions. Fig. 7 shows the value of H(I dent simulation runs, which is estimated by implementing the proposed double sort algorithm and by using the Genetic algorithm. It can be (1)) values obtained using the proposed noted that for each run, the H(I double sort algorithm and Genetic algorithm are well matched, thereby confirming the adequacy of the proposed double sort algorithm.\n\nseen in Fig. 5(e, f).\n\nThe Holder Table function has multiple local and global minima; the global minima are placed at the boundary of the design space, as shown in Fig. 6(a, b). Once again, it is seen that both the hyper-rectangle and hyper-ellipse are trapped around any of one of the global minima, and on the other hand, the iSSO is able to capture the regions that include all of the global minima, as seen in Fig. 6(e, f).\n\nAt any iteration of iSSO, new samples are simulated using the seed samples. In the proposed approach, the volume of the Voronoi cells corresponding to the seed and new samples is greater than the volume of the Voronoi cells corresponding only to the seed samples. Fig. 8 shows this change in volume V due to the creation of Voronoi cells at any generation of iSSO using the procedure mentioned in Section 3.4. The increase is observed to be small which further reduces with an in- crease in the iteration number. It is also observed that the increase in volume decreases with an increase in sample size at each iteration and increases with an increase in the dimension of the problem.\n\nThe results from the three examples demonstrate that the proposed iSSO is able to capture the regions containing the optimal solution effectively.\n\nNext, the statistics of the results of 50 independent runs, both for SSO and iSSO are presented in Table 1. It also includes the results obtained by using state-of-the-art approaches, such as the Genetic algorithm, particle swarm optimization, and the gradient based optimization approach (interior-point algorithm). The proposed iSSO outperforms all other approaches as more successes in determining the optimal solution are observed in all three optimization problems. It is also seen that both SSO and iSSO result in a similar value of volume reduction for the same stopping criterion; however, with SSO, the number of iterations required to achieve this volume reduction are relatively higher. The proposed approach outperformed the state-of-the-art approaches, as indicated by the number of successes. These examples demonstrate that the main advantage of implementing Voronoi tessellation is an effective explo- ration of the design space.\n\n(seeds+new)(cid:0) V V(seeds)\n\n(seeds)\n\n4.2. Robust design optimization of the tuned mass damper\n\nThis example considers a stochastic design problem involving a Tuned Mass Damper (TMD) attached to a Single Degree of Freedom (SDOF) system. The problem is taken from [48] and is shown in Fig. 9. In this problem, the system is excited by a white noise signal with a mean zero and unit variance. The performance measure is the variance of the displacement of the system \u03c32 xs . The mass mS, stiffness kS, and\n\nNext, the performance of the proposed \"double sort algorithm\" for selecting the optimal subset is studied by using the above-mentioned\n\n",
    "AdvancesinEngineeringSoftware188(2024)1035689\n\nM.A. Khalid and S. Bansal\n\nTable 1 Statistics of optimization results for multimodal deterministic optimization problems.\n\nExample\n\nSSO\n\niSSO\n\nGA*\n\nPSO*\n\nGBA*\n\nHyper-Rectangle\n\nHyper-Ellipse\n\nGriewank\n\nNF NS BV WV AV c.o.v FE Gen VR NF NS BV WV AV c.o.v FE Gen VR NF NS BV WV AV c.o.v FE Gen VR\n\n30 20 0 0.1028 0.0190 1.1663 22,702 7 94.025 50 0 (cid:0) 2.0576 (cid:0) 2.0472 (cid:0) 2.0527 0.0133 27,563 9 99.89 50 0 (cid:0) 19.2085 (cid:0) 18.8916 (cid:0) 19.0916 0.0025 40,700 13 99.59\n\n32 18 0 0.1161 0.0292 1.0168 15,223 5 84.79 50 0 (cid:0) 2.0626 (cid:0) 2.0481 (cid:0) 2.0621 0.001 21,595 7 98.95 50 0 (cid:0) 17.5025 (cid:0) 1.1419 (cid:0) 8.432 0.3898 24,684 8 99.42\n\n5 45 0 0.0270 0.0110 0.5137 12,426 4 93.56 1 49 (cid:0) 2.0624 (cid:0) 2.0260 (cid:0) 2.0522 0.0042 9982 3 88.71 3 47 (cid:0) 19.2085 (cid:0) 17.3030 (cid:0) 18.8798 0.0182 18,142 6 94.37\n\n38 12 0 0.0296 0.0094 0.9603 3385\n\n35 15 0 0.0232 0.0057 0.8763 1432\n\n47 3 0 0.0296 0.0173 0.05971 33\n\nN/A\n\nCross-In-Tray\n\n50 0 (cid:0) 2.0626 (cid:0) 2.0626 (cid:0) 2.0626 0 3178\n\n50 0 (cid:0) 2.0626 (cid:0) 2.0626 (cid:0) 2.0626 0 933\n\n50 0 (cid:0) 2.0626 (cid:0) 1.3853 (cid:0) 1.7360 0.0977 32\n\nN/A\n\nHolder-Table\n\n50 0 (cid:0) 19.2085 (cid:0) 9.5047 (cid:0) 19.0144 0.0722 3413 N/A\n\n50 0 (cid:0) 19.2085 (cid:0) 15.1402 (cid:0) 18.9745 0.0443 988\n\n50 0 (cid:0) 19.2085 (cid:0) 1.1831 (cid:0) 6.5493 0.8358 30\n\nGA = genetic algorithm, PSO = particle swarm optimization, GBA = gradient-based optimization approach, NF = no. of. failure, NS = no. of. success, BV = best value, WV = worst value, AV = average value, c.o.v = coefficient of variation, FE = no. of. function evaluations, Gen = generations, VR = volume reduction percentage, * = efficiently applicable only for deterministic problems.\n\nFig. 7. Comparison of double sort algorithm and Genetic algorithm results.\n\nFig. 8. Percentage change in volume at each iSSO iteration.\n\nparameters mT,\u03c9T,and\u03c9S are, in order, the mass of the TMD, the natural ), and the natural frequency of the frequency of the TMD ( \u221a\n\ndamping cS of the system are taken as uncertain parameters, following independent Gaussian distribution. The mean value of these variables is taken to be 105 kg, 107 N/m, and 4 \u00d7 104 Ns/m respectively. To account for uncertainty, the c.o.v value for each variable taken is 0.05. The frequency ratio \u03b2 = \u03c9T /\u03c9S and damping \u03beT of the TMD are considered design parameters. The TMD has a mass ratio, mT/ms, of 0.10. The\n\n\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305 kT/mT\n\n\u221a\n\n\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305 ks/ms\n\n). The optimization problem is written as:\n\nstructure (\n\n[(\n\n)]2\n\n\u03c32 xs\n\nminimize : \u03c6\u2208\u03a6,\u03c6\u2208\u03a6\n\nE\u03b8[h(\u03b8, \u03c6, \u03c6)] = E\u03b8\n\n,\n\n(\u03b8, \u03c6) (cid:0) \u03c6\n\n(22)\n\n",
    "AdvancesinEngineeringSoftware188(2024)10356810\n\nM.A. Khalid and S. Bansal\n\niSSO are shown. SAA is applied with a sample size of 103, as mentioned in [43]. The results demonstrate that iSSO is effective in locating the optimal solution. SSO implemented with hyper-ellipse gives an optimal solution but has a higher computational cost.\n\n4.3. 120-bars truss structure\n\nThe third example involves minimizing the mean of the compliance of a 120-bar linear elastic truss structure shown in Fig. 10 under the weight constraint W \u2264 15, 000kg. Because of structural symmetry, design parameters corresponding to the cross-sectional areas of elements (cid:0) 4 m2. are divided into seven groups, each with a minimum area of 10 The Young\u2019s modulus for the bar groups are assumed as uncorrelated normal random variables with mean values equal to 210 GPa and the c. o.v equal to 0.10 respectively. The density of the material is 7971.89 kg/ m3. The dome is subjected to concentrated vertical loads acting down- ward at the top node, normally distributed with a mean equal to 60 kN and c.o.v equal to 0.20. In addition, the mass of bars is concentrated at the nodes. The problem is taken from [48].\n\nTable 3 presents the best of 10 independent run results obtained with SSO and iSSO. Once again, the SSO and iSSO solutions agree well, thereby demonstrating the effectiveness of the proposed approach. At the same time, the number of function evaluations is substantially less in the case of iSSO, indicating the efficiency of the proposed approach.\n\nFig. 9. TMD attached to a SDOF system [48].\n\nwhere,\n\n0.01 \u2264 \u03b2 \u2264 1.5, 0.01 \u2264 \u03beT \u2264 1.0, 0 \u2264 \u03c6 \u2264 1000.\n\n(23)\n\nTable 2 presents the optimal design parameter values as well as the objective function value that solve the optimization problem in (22). Results obtained using SSO, Sample Average Approximation (SAA), and\n\n4.4. Reliability-based design of a base isolated structure\n\nThis example, adapted from [49], involves the reliability-based\n\nTable 2 Variance minimization of TMD-structure.\n\nE\u03b8[h(\u03b8, \u03c6, \u03c6)] (\u00d7 10\n\nMethod\n\nAdmissible Subset shape\n\nDesign parameters \u03b2\n\nFE\n\nNS\n\nNF\n\n(cid:0) 16 mm4)\n\n\u03beT\n\nSSO [48]\n\nHyper-Rectangle Hyper-Ellipse N/A Voronoi tessellation\n\n0.551 0.749 0.749 0.749\n\n0.623 0.221 0.221 0.221\n\n41.324 1.7586 1.7587 1.7586\n\n7433 8245 3 \u00d7 106 6198\n\n0 34 4 50\n\n50 16 46 0\n\nSAA iSSO\n\nFig. 10. 120-bar dome truss structure [48].\n\nTable 3 Results for the 120 bars truss structure.\n\n\u03bcg(\u0303\u03c6\u2217) (Nm)\n\n(\u0303\u03c6\u2217) \u03c32 g (Nm)2\n\nMethod\n\nDesign parameters A1 A2 (cm2) (cm2)\n\nFE\n\nA3 (cm2)\n\nA4 (cm2)\n\nA5 (cm2)\n\nA6 (cm2)\n\nA7 (cm2)\n\nSSO [48] SAA iSSO\n\n47.2 47.3 48.1\n\n67.5 68.3 66.4\n\n42.7 40.7 42.6\n\n9.4 10.5 8.5\n\n30.1 30.3 31.4\n\n55.6 49.5 56.5\n\n13.1 14.8 12.5\n\n242.3 243.5 242.8\n\n5294.2 5407.4 5317.2\n\n54,700 3 \u00d7 106 14,937\n\n",
    "AdvancesinEngineeringSoftware188(2024)10356811\n\nM.A. Khalid and S. Bansal\n\nFig. 11. (left) 10-story base isolated shear model, and (right) force-deformation of bilinear isolator [49].\n\n/\n\n2\u03b6g 4\u03b62\n\nTable 4 Base isolation structure system optimization results (best of 10 independent runs).\n\nS0 = \u03c32 \u03c9\n\n)m2\n\ns3,\n\n(\n\n(27)\n\ng + 1\n\n\u03c0\u03c9g\n\nDesign parameters (\u03c6*) Kpr (MN/ m)\n\nMethod\n\nFailure probability PF(\u03c6*)\n\nwhere, \u03c9g, \u03b6gand \u03c3\u03c9 are the resonant frequency, damping, and RMS of the acceleration input of the filter, respectively. These are also considered uncertain variables with mean values of [2\u03c0rad/s, 0.5, 0.2g] and a c.o.v equal to 0.20. The non-stationarity of the excitation is modeled by multiplying the filter output with the envelope function as:\n\nKp (MN/ m)\n\nFy (MN)\n\ncd (MNs/ m)\n\nSSO\n\n425.33\n\n1.20\n\n15.52\n\n6.54\n\n0.0340\n\n[39]\n\nSAA iSSO\n\n414.68 418.34\n\n1.16 1.11\n\n16.15 15.88\n\n6.26 7.08\n\n0.0324 0.0366\n\ne(t) = \u03bb3t\u03bb1 exp( (cid:0) \u03bb2t),\n\n(28)\n\nwith parameters \u03bb1 = 1.25, \u03bb2 = 0.2 and \u03bb3 = 0.353 chosen to simulate strong earthquake excitation for a duration of 40 s with a sampling time of 0.02 s. The base-isolation system considered is a lead\u2013rubber bilinear isolator with an additional viscous damper. The base has a 247-ton mass. The design parameters \u03c6 for the base isolation structure system are the stiffness before yielding Kprand after yielding Kp, the yield force is Fy, and the damping coefficient cd. The reader may refer to [39,50] for additional details regarding the base isolation structure system adopted in this study.\n\noptimization of a base-isolation system attached to a 10-story building as shown in Fig. 11. This optimization problem includes maximizing the reliability of the base-isolated structure which is performed by the minimization of its failure probability and mathematically expressed as: \u222b\n\nminimize : \u03c6\u2208\u03a6\n\nP(F|\u03c6) = E\u03b8[IF(\u03c6, \u03b8)] =\n\nIF(\u03c6, \u03b8)p(\u03c6, \u03b8)d\u03b8,\n\n(24)\n\n\u0398\n\nwhere, IF(\u03c6,\u03b8) is the function that indicates failure, and it equals 1 when the system fails, i.e., when unacceptable performance occurs. Notably, in this problem h(\u03c6, \u03b8) = IF(\u03c6,\u03b8).\n\nFailure is indicated when any of the normalized base displacements or inter-story drifts exceeds unity. The normalization constants are 0.5 m and 0.033 m respectively. The design interval for each variable is specified as Kpr = [50, 600] MN/m, Fy = [1, 8] MN, Kp = [5, 60] MN/m, and cd = [0.1, 10]MNs/m. In this example, iSSO and SSO are imple- mented with six number of iterations.\n\nThe 10-story building is considered as a shear structure with un- certain inter-story stiffness and damping. Each story has a total mass of 207 ton. The inter-story stiffness ki of all stories are parameterized by ki = \u0302 ki\u03b8i, i = 1, \u2026, 10 where the most probable values of the inter-story stiffness are [\u0302 ki] = [687.1, 613.1, 540.1, 481.1, 421.7, 353.7, 286.6, 225.6, 184.5, 104.5] MN/m. The entity \u03b8i is a set of non-dimensional uncertain variables that are considered to be correlated Gaussian vari- \u0302\u03b8i = 1, \u2200i and a covariance matrix defined\n\nTable 4 shows the optimization results for the best 10 independent simulation runs. The comparison of the results obtained using SSO, SAA (with a sample size of 103), and iSSO shows that the optimal design obtained using the proposed approach iSSO is in good agreement. The failure probability of the structure is reduced from 0.95 (without the base isolation system) to 0.0326 after installing the optimally designed base isolation system.\n\nables with a unit mean value as: [\n\n)]\n\ni)2 /\n\n] .\n\n(cid:0) (\u03b8i (cid:0) \u0302\u03b8i)\n\n[ = (0.2)2exp\n\n\u03b8j (cid:0) \u0302\u03b8j\n\n22\n\n(cid:0) (j (cid:0)\n\nE\n\n(25)\n\nThe damping ratios are considered independent Gaussian variables with mean values of 0.025 and c.o.v of 0.10 for all modes. The Kanai- Tajimi model is used to simulate the ground excitation modelled as a filtered white noise process, with the power spectral density function given as:\n\n5. Conclusion\n\nThis study attempts to provide an optimization approach called \"iSSO\", which is an improved version of SSO, primarily for stochastic optimization problems while it retains utility for deterministic optimi- zation problems as well. Two novel ideas are introduced in this study: first, a better characterization of the design space is offered by parti- tioning the design space into non-overlapping subregions using Voronoi\n\ng + 4\u03b62 )2\n\n\u03c94\n\ng\u03c92\n\ng\u03c92\n\nS(\u03c9) = S0\n\n,\n\n(\n\n(26)\n\n+ 4\u03b62\n\ng (cid:0) \u03c92\n\n\u03c92\n\ng\u03c92\n\ng\u03c92\n\n",
    "AdvancesinEngineeringSoftware188(2024)10356812\n\nM.A. Khalid and S. Bansal\n\ntessellation which improves the effectiveness and efficiency of the pro- posed iSSO considerably in comparison to SSO. Second, a novel \"double sort\" approach is proposed, eliminating the need for optimization to identify the subregions for the optimal design at each iSSO iteration. Several mathematical and engineering design examples, including TMD, 120 bars truss structure, and base-isolated structure, are included in this study to demonstrate the efficacy of the proposed iSSO. The results show that the proposed iSSO effectively identifies the reduced design space for complex design problems with multiple global and local minima. This is attributable to the Voronoi tessellation, which eliminates the require- ment of the presumed admissible design space form to resemble the contour of the original design. Voronoi tessellation enabled better design space exploration, allowing multiple global minima scattered throughout the design pace to be effectively identified. Due to the dis- cretization of the design space via Voronoi tessellation, computation demand is significantly reduced as the number of function evaluations for all examples is lower vis-a-vis the original SSO. Moreover, the novel idea of the double sort approach achieves the requisite precision in identifying the subregions for optimal solutions and makes iSSO implementation simple and effective.\n\nVoronoi cells. At present the methods available in the literation for creating the Voronoi tessellation are computationally demanding when considering problems of very high dimension. Future work will focus on developing a method for creating the Voronoi tessellation in higher di- mensions, particularly those greater than ten.\n\nCRediT authorship contribution statement\n\nMohd Aman Khalid: Investigation, Methodology, Formal analysis, Software, Visualization, Writing \u2013 original draft. Sahil Bansal: Conceptualization, Methodology, Supervision.\n\nDeclaration of Competing Interest\n\nThe authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\n\nData availability\n\nThe applicability of the approach is dependent on the creation of the\n\nNo data was used for the research described in the article.\n\nAppendix-A: Voronoi Tessellation\n\nVoronoi tessellation is a mathematical concept named after the Russian mathematician Georgy Voronoi. It is also known as the Voronoi diagram or Dirichlet tessellation. A Voronoi tessellation of a set of points P in a plane is a partition of the plane into a set of non-overlapping convex polygons, with each polygon including precisely one point of P and each point in a polygon being closer to its associated point in P than to any other point in P. Each polygon is referred to as a Voronoi cell or a Dirichlet region. The boundary of each cell is constituted of points that are equidistant to two or more points in P. Fig. 12 shows the Voronoi diagram in a two-dimensional design space.\n\nFig. 12. Voronoi diagram in 2-dimensional space.\n\nThere are several efficient algorithms for creating Voronoi diagrams. One such basic algorithm is to start with a set of points and then compute the Voronoi cells by dividing the space into regions based on the distance to the nearest point. The Bowyer-Watson algorithm [51], which generates a Delaunay triangulation in any number of dimensions, can be applied while creating a Voronoi diagram. The Delaunay triangulation is a triangulation of the point in which no point falls within the circumcircle of any triangle. The polygon generated by the intersection of the half-planes defined by the edges of the Delaunay triangles enclosing the point is therefore obtained as the Voronoi cell of a point.\n\nIt can be summarized that Voronoi tessellation is a powerful mathematical concept that aids in dividing space into regions based on the distance to a set of points. Voronoi tessellation finds widespread applications in areas such as image processing [52], spatial topology analysis [53], and microstructure study [52]. The MATLAB command \"Voronoin\" from the \"Parallel Computing Toolbox\" [54] has been used in this study to create the Voronoi cells.\n\n[4] Meng Z, Li G, Wang X, Sait SM, R\u0131za A. A comparative study of metaheuristic algorithms for reliability \u2011 based design optimization problems. Arch Comput Methods Eng 2021;28:1853\u201369. https://doi.org/10.1007/s11831-020-09443-z.\n\nReferences\n\n[1] Marti K. Stochastic optimization methods. Berlin: Springer; 2008. [2] Tsompanakis Y, Lagaros ND, Papadrakakis M. Structural design optimization\n\n[5] Abualigah L, Elaziz MA, Khasawneh AM, Alshinwan M. Meta-heuristic\n\noptimization algorithms for solving real-world mechanical engineering design problems : a comprehensive survey, applications, comparative analysis, and results. Neural Comput Appl 2022;34:4081\u2013110. https://doi.org/10.1007/s00521- 021-06747-4.\n\nconsidering uncertainties. CRC Press; 2008.\n\n[3] Khalid MA, Bansal S, Ramamohan V. An augmented formulation for robust design optimization of structures using stochastic simulation method. Res Eng Des 2023; 34:179\u2013200. https://doi.org/10.1007/s00163-022-00405-z.\n\n[6] Katebi J, Shoaei M, Nguyen S, Trung T, Khorami M. Developed comparative\n\nanalysis of metaheuristic optimization algorithms for optimal active control of\n\n",
    "AdvancesinEngineeringSoftware188(2024)10356813\n\nM.A. Khalid and S. Bansal\n\nstructures. Eng Comput 2020;36:1539\u201358. https://doi.org/10.1007/s00366-019- 00780-7.\n\n[31] Yildiz AR. Comparison of evolutionary-based optimization algorithms for\n\nstructural design optimization. Eng Appl Artif Intell 2013;26:327\u201333. https://doi. org/10.1016/j.engappai.2012.05.014.\n\n[7] Alorf A. Engineering applications of artificial intelligence a survey of recently developed metaheuristics and their comparative analysis. Eng Appl Artif Intell 2023;117:105622. https://doi.org/10.1016/j.engappai.2022.105622.\n\n[32] Beck AT, Gomes WJDS. A comparison of deterministic, reliability-based and risk- based structural optimization under uncertainty. Probab Eng Mech 2012;28:18\u201329. https://doi.org/10.1016/j.probengmech.2011.08.007.\n\n[8] Kirsch U. Structural optimization: fundamentals and applications. Springer-Verlag;\n\n2012.\n\n[33] Acar, E., Bayrak, G., Jung, Y., Lee, I., Ramu, P., Ravichandran, S.S.: Modeling,\n\n[9] Floudas CA, Pardalos PA. Encyclopedia of optimization. Springer; 2008.\n\nanalysis, and optimization under uncertainties: a review, (2021). 10.1007/s001 58-021-03026-7.\n\n[10] Kiureghian AD, Ditlevsen O. Aleatory or epistemic? Does it matter? Struct Saf\n\n2009;31:105\u201312. https://doi.org/10.1016/j.strusafe.2008.06.020.\n\n[34] Georghiou A, Kuhn D, Wiesemann W. The decision rule approach to optimization under uncertainty: methodology and applications. Comput Manag Sci 2019. https://doi.org/10.1007/s10287-018-0338-5.\n\n[11] Schu\u00a8eller GI, Jensen HA. Computational methods in optimization considering uncertainties - an overview. Comput Methods Appl Mech Eng 2008;198:2\u201313. https://doi.org/10.1016/j.cma.2008.05.004.\n\n[35] Braydi O, Lafon P, Younes R. Study of uncertainties and objective function\n\n[12] Schneider J, Kirkpatrick S. Stochastic optimization. Springer; 2007. [13] Do B, Ohsaki M. A random search for discrete robust design optimization of linear-\n\nmodeling effects on probabilistic optimization results. ASCE ASME J Risk Uncertain Eng Syst Part B Mech Eng 2019. https://doi.org/10.1115/1.4044152.\n\nelastic steel frames under interval parametric uncertainty. Comput Struct 2021; 249:106506. https://doi.org/10.1016/j.compstruc.2021.106506.\n\n[36] Liu WS, Cheung SH. Reliability based design optimization with approximate failure\n\nprobability function in partitioned design space. Reliab Eng Syst Saf 2017;167: 602\u201311. https://doi.org/10.1016/j.ress.2017.07.007.\n\n[14] Asadpoure A, Tootkaboni M, Guest JK. Robust topology optimization of structures with uncertainties in stiffness - application to truss structures. Comput Struct 2011. https://doi.org/10.1016/j.compstruc.2010.11.004.\n\n[37] Chiralaksanakul A, Mahadevan S. First-order approximation methods in reliability- based design optimization. J Mech Des Trans ASME 2005. https://doi.org/ 10.1115/1.1899691.\n\n[15] Doltsinis I, Kang Z. Robust design of structures using optimization methods.\n\nComput Methods Appl Mech Eng 2004;193:2221\u201337. https://doi.org/10.1016/j. cma.2003.12.055.\n\n[38] Doltsinis I, Kang Z, Cheng G. Robust design of non-linear structures using\n\noptimization methods. Comput Methods Appl Mech Eng 2005;194:1779\u201395. https://doi.org/10.1016/j.cma.2004.02.027.\n\n[16] Carneiro G, das N, Ant\u00b4onio CC. Dimensional reduction applied to the reliability- based robust design optimization of composite structures. Compos Struct 2021; 255. https://doi.org/10.1016/j.compstruct.2020.112937.\n\n[39] Taflanidis AA, Beck JL. Stochastic Subset Optimization for optimal reliability\n\nproblems. Probab Eng Mech 2008. https://doi.org/10.1016/j. probengmech.2007.12.011.\n\n[17] An H, Youn BD, Kim HS. Reliability-based design optimization of laminated\n\ncomposite structures under delamination and material property uncertainties. Int J Mech Sci 2021. https://doi.org/10.1016/j.ijmecsci.2021.106561.\n\n[40] Au SK, Beck JL. Estimation of small failure probabilities in high dimensions by subset simulation. Probab Eng Mech 2001;16:263\u201377. https://doi.org/10.1016/ S0266-8920(01)00019-4.\n\n[18] Li Z, Duan LB, Cheng AG, Yao ZP, Chen T, Yao W. Lightweight and crashworthiness design of an electric vehicle using a six-sigma robust design optimization method. Eng Optim 2019. https://doi.org/10.1080/0305215X.2018.1521396.\n\n[41] Jia GF, Taflanidis AA. Non-parametric stochastic subset optimization for optimal- reliability design problems. Comput Struct 2013;126:86\u201399. https://doi.org/ 10.1016/j.compstruc.2012.12.009.\n\n[19] Gholinezhad H, Torabi SH. Reliability-based multidisciplinary design optimization of an underwater vehicle including cost analysis. J Mar Sci Technol 2021. https:// doi.org/10.1007/s00773-021-00804-2.\n\n[42] Taflanidis AA. Stochastic subset optimization incorporating moving least squares response surface methodologies for stochastic sampling. Adv Eng Softw 2012;44: 3\u201314. https://doi.org/10.1016/j.advengsoft.2011.07.009.\n\n[20] Lee KH, Park GJ. Robust optimization considering tolerances of design variables. Comput Struct 2001;79:77\u201386. https://doi.org/10.1016/S0045-7949(00)00117-6. [21] Anderson TV, Mattson CA. Propagating skewness and kurtosis through engineering models for low-cost, meaningful, nondeterministic design. J Mech Des Trans ASME. 2012. https://doi.org/10.1115/1.4007389.\n\n[43] Khalid MA, Bansal S. Framework for robust design optimization of tuned mass\n\ndampers by stochastic subset optimization. Int J Struct Stab Dyn 2023;23. https:// doi.org/10.1142/S0219455423501559.\n\n[44] Au SK. Reliability-based design sensitivity by efficient simulation. Comput Struct\n\n2005;83:1048\u201361.\n\n[22] Zhou Q, Wang Y, Choi SK, Jiang P, Shao X, Hu J, Shu L. A robust optimization\n\napproach based on multi-fidelity metamodel. Struct Multidiscip Optim 2018. https://doi.org/10.1007/s00158-017-1783-4.\n\n[45] Taflanidis AA, Beck JL. An efficient framework for optimal robust stochastic system design using stochastic simulation. Comput Methods Appl Mech Eng 2008. https:// doi.org/10.1016/j.cma.2008.03.029.\n\n[23] Wang GG, Shan S. Review of metamodeling techniques in support of engineering design optimization. J Mech Des Trans ASME. 2007;129:370\u201380. https://doi.org/ 10.1115/1.2429697.\n\n[46] Robert CP, Casella G. Monte carlo statistical methods. New York, NY: Springer;\n\n2004.\n\n[24] Chatterjee T, Chakraborty S, Chowdhury R. A critical review of surrogate assisted robust design optimization. Arch Comput Methods Eng 2019;26:245\u201374. https:// doi.org/10.1007/s11831-017-9240-5.\n\n[47] Li HS. Subset simulation for unconstrained global optimization. Appl Math Model\n\n2011;35:5108\u201320. https://doi.org/10.1016/j.apm.2011.04.023.\n\n[48] Khalid MA, Bansal S, Ramamohan V. An augmented formulation for robust design optimization of structures using stochastic simulation method. Res Eng Des 2022. https://doi.org/10.1007/s00163-022-00405-z.\n\n[25] Chatterjee T, Friswell MI, Adhikari S, Chowdhury R. A global two-layer meta-\n\nmodel for response statistics in robust design optimization. Eng Optim 2021. https://doi.org/10.1080/0305215X.2020.1861262.\n\n[49] Taflanidis AA, Beck JL. An efficient framework for optimal robust stochastic system\n\n[26] Guo X, Zhao X, Zhang W, Yan J, Sun G. Multi-scale robust design and optimization considering load uncertainties. Comput Methods Appl Mech Eng 2015;283: 994\u20131009. https://doi.org/10.1016/j.cma.2014.10.014.\n\ndesign using stochastic simulation. Comput Methods Appl Mech Eng 2008;198: 88\u2013101. https://doi.org/10.1016/j.cma.2008.03.029.\n\n[50] Kandemir EC, Mortazavi A. Optimization of seismic base isolation system using a fuzzy reinforced swarm intelligence. Adv Eng Softw 2022;174:103323. https://doi. org/10.1016/j.advengsoft.2022.103323.\n\n[27] Jerez DJ, Jensen HA, Beer M. Reliability-based design optimization of structural systems under stochastic excitation: an overview. Mech Syst Signal Process 2022. https://doi.org/10.1016/j.ymssp.2021.108397.\n\n[51] Rebay S. Efficient unstructured mesh generation by means of delaunay\n\ntriangulation and Bowyer-Watson algorithm. J Comput Phys 1993;106:125\u201338. [52] Wade N, Graham-Brady L. Estimating microstructural feature distributions from image data using a Bayesian framework. J Microsc 2023:1\u201316. https://doi.org/ 10.1111/jmi.13184.\n\n[28] Li W, Gao L, Xiao M. Multidisciplinary robust design optimization under parameter\n\nand model uncertainties. Eng Optim 2020;52:426\u201345. https://doi.org/10.1080/ 0305215X.2019.1590564.\n\n[29] Beyer HG, Sendhoff B. Robust optimization - a comprehensive survey. Comput Methods Appl Mech Eng 2007;196:3190\u2013218. https://doi.org/10.1016/j. cma.2007.03.003.\n\n[53] Duan X, Li L, Ge Y, Liu B. Exact Voronoi diagram for topographic spatial analysis.\n\nGIScience Remote Sens 2023;60. https://doi.org/10.1080/ 15481603.2023.2171703.\n\n[30] Motta R, de S, Afonso SMB. An efficient procedure for structural reliability-based robust design optimization. Struct Multidiscip Optim 2016;54:511\u201330. https://doi. org/10.1007/s00158-016-1418-1.\n\n[54] MATLAB and parallel computing toolbox release. Natick, Massachusetts, United\n\nStates: The Mathworks, Inc.; 2021.\n\n"
]