[
    "Advances in Engineering Software 188 (2024) 103568\nAvailable online 12 December 2023\n0965-9978/\u00a9 2023 Published by Elsevier Ltd.Improved stochastic subset optimization method for structural \ndesign optimization \nMohd Aman Khalid , Sahil Bansal* \nDepartment of Civil Engineering, Indian Institute of Technology Delhi, New Delhi 110016, India   \nARTICLE INFO  \nKeywords: \nStochastic subset optimization \nVoronoi tessellation \nStochastic simulation \nStochastic optimization \nOptimization under uncertainty ABSTRACT  \nThe Stochastic Subset Optimization (SSO) algorithm was proposed for optimal reliability problems that mini-\nmizes the probability of system failure over the admissible space for the design parameters. It is based on the \nsimulation of samples of the design parameters from an auxiliary Probability Density Function (PDF) and \nexploiting the information contained in these samples to identify subregions for the optimal design parameters \nwithin the original design space. This paper presents an improved version of SSO, named iSSO to overcome the \nshortcomings in the SSO. In the improved version, the Voronoi tessellation is implemented to partition the design \nspace into non-overlapping subregions using the pool of samples distributed according to the auxiliary PDF. A \ndouble-sort approach is then used to identify the subregions for the optimal design. The iSSO is presented as a \ngeneralized design optimization approach primarily tailored for the stochastic structural systems but also \nadaptable to deterministic systems. Several optimization problems are considered to illustrate the effectiveness \nand efficiency of the proposed iSSO.   \n1.Introduction \nStructural optimization may be defined as the rational establishment \nof an economical structural design with the available resources while \nsatisfying specific performance criteria. In general terms, the economy \nmay be characterized by minimum weight, minimum cost, maximum \nutility, or even minimum probability of failure. Broadly, structural \noptimization can be categorized into deterministic and stochastic opti-\nmization [1,2]. The classical statement of unconstraint deterministic \noptimization is mathematically expressed as: \nminimize :\n\u03c6\u2208\u03a6g(\u03c6) (1)  \nwhere, \u03c6=[\u03c61\u22ef\u03c6n\u03c6]T\u2208\u03a6\u2282Rn\u03c6 is a set of deterministic adjustable pa-\nrameters that define the structural design, referred to herein as design \nparameters, g(\u03c6):Rn\u03c6\u2192R is the objective function to be minimized, and \n\u03a6 denotes the bounded admissible design space. The deterministic \nconstraints can be considered by the appropriate definition of the ad-\nmissible design space \u03a6 for deterministic design parameters \u03c6, as \nmentioned in [3]. In the deterministic structural optimization problem, \nthe uncertainties in parameters are ignored, and fixed values are \nassumed for all the parameters. There are numerous optimizations approaches available in the literature, however, but it\u2019s worth noting \nthat no one-size-fits-all optimization approach is ideal for all sorts of \nproblems [4\u20137]. The choice of optimization method is often determined \nby the specific characteristics of the problem, such as its complexity, \ndimensionality, constraints, and the nature of the objective function. As \na result, there is always a scope for new approaches to be developed or \nthe adaptation of existing methods to better suit specific problem clas-\nses. A detailed discussion of deterministic optimization approaches can \nbe found in the literature [8,9]. \nIn any practical situation, several parameters, such as loadings, \nstructural parameters, geometric parameters, operation conditions, etc., \nare either not known at the design stage or are subjected to random \nfluctuations that give rise to performance variability and affect the \nperformance of a system [10]. These parameters are characterized as \nuncertain parameters. Deterministic structural optimization discards the \nimpact of uncertainty and can result in improper design. Therefore, it is \ndesirable to account for the uncertainty in the parameters during opti-\nmization by using the rational methods of probabilistic structural \nanalysis [11]. Such structural optimization that accounts for un-\ncertainties is called stochastic optimization [12]. Although stochastic \noptimization refers to any method that employs randomness within \nsome communities, in this paper, we will only consider settings where \n*Corresponding author. \nE-mail addresses: mohdamankhalid@gmail.com (M.A. Khalid), sahil@iitd.ac.in (S. Bansal).  \nContents lists available at ScienceDirect \nAdvances in Engineering Software \njournal homepag e: www.else vier.com/loc ate/adven gsoft \nhttps://doi.org/10.1016/j.advengsoft.2023.103568 \nReceived 5 June 2023; Received in revised form 2 October 2023; Accepted 24 November 2023   ",
    "Advances in Engineering Software 188 (2024) 103568\n2the objective function is random. Stochastic optimization or optimal \ndesign under uncertainty has been widely applied in many practical \nengineering fields, including civil engineering structures [13\u201315], \ncomposite structures [16,17], and vehicles [18,19]. \nConsider an engineering system that involves deterministic design \nparameters \u03c6, and uncertain variables \u03b8=[\u03b81\u22ef\u03b8n\u03b8]T\u2208\u0398\u2282Rn\u03b8 following \na joint PDF p(\u03b8|\u03c6), where \u0398 denotes the parameter space of the uncer -\ntain variables. The classical statement of stochastic optimization is \nmathematically expressed as: \nminimize :\n\u03c6\u2208\u03a6E\u03b8[h(\u03c6,\u03b8)] (2)  \nwhere, h(\u03c6,\u03b8):Rn\u03b8+n\u03c6\u2192R is the structural performance function, and \nE\u03b8[ \u22c5 ] denotes expectation with respect to the PDF for \u03b8. Note that the \nobjective function in the optimization problem in (2) is the expectation \nE\u03b8[h(\u03c6,\u03b8)]which is a deterministic function. It\u2019s worth mentioning that \nstochastic optimization may also involve other stochastic measures such \nas variance or quantile values. However, these stochastic measures can \nrarely be evaluated analytically; therefore, several methods have been \nproposed for solving stochastic optimization problems. These special -\nized methods include, for example, sample average approximation, \nstochastic approximation, stochastic subset optimization, and ap-\nproaches based on the use of Taylor series expansion [15,20,21], \nresponse surface, and metamodels [22\u201325]. Specific to structural engi-\nneering, there are two broad categories of problems involving design \noptimization under uncertainty [26\u201335]: Reliability-Based Design \nOptimization (RBDO) and Robust Design Optimization (RDO). The \nobjective of RBDO is to find an optimal solution that minimizes some \ndeterministic, objective function under observance of probabilistic \nconstraints instead of conventional deterministic constraints [36,37]. \nOn the other hand, RDO aims to find an optimal solution that is insen -\nsitive (or less sensitive) to input variations. It improves the design \nquality by minimizing performance variation without eliminating un-\ncertainty [29,38]. \nTaflanidis and Beck [39] introduced a novel algorithm for optimal \nreliability problem, the so-called SSO. SSO involves formulating an \naugmented problem where the design parameters are artificially \nconsidered uncertain and defining an auxiliary PDF that includes the \nstructural performance function and the PDF of the uncertain variables. \nNext, SSO involves generating a pool of samples distributed according to \nthis auxiliary PDF and identifying a subregion in the original design \nspace, which, on average, improves the value of the objective function. \nBy repeating this procedure several times, it is possible to determine at \neach step a smaller subregion in the design space, which in turn im-\nproves the value of the objective function. Ultimately, this subregion \nwill be sufficiently small to directly identify the optimal solution or \nprovide sufficient information to launch another optimization algo-\nrithm, such as the sample average approximation or stochastic approx -\nimation. The implementation of the SSO method closely resembles the \nSubset Simulation (SS) algorithm [40] for reliability analysis. Since SSO \nis based on simulation, it can deal with linear or nonlinear problems and, \nat least theoretically, an unbounded number of design parameters. The \nnumerical effort for solving a given optimization problem is indepen -\ndent of the number of uncertain variables, and it grows linearly with the \nnumber of design parameters. \nSince the introduction of SSO, several extensions of SSO have been \nproposed. An extension of SSO termed Non-Parametric SSO, which \nadopts kernel density estimation to approximate the objective function, \nis presented in [41]. In [42], efficient integration of the Moving Least \nSquares approximation within SSO is introduced to reduce the compu -\ntational effort in SSO. In [3], an augmented formulation is presented for \nthe RDO of structures using SSO. SSO or its variants have also been \napplied to solve structural optimization problems. SSO has been used for \nreliability optimization and sensitivity analysis in system design in [39]. \nA framework for RDO of Tuned Mass Dampers (TMD) by SSO is dis-\ncussed in [43]. Even though SSO has proved to be efficient for meeting various challenging optimization problems, it has two shortcomings. \nFirst, the effectiveness of SSO is dependent on the correct selection of \nthe geometrical shape of the admissible subsets. Here, it is pertinent to \nmention that choosing a geometrical shape that effectively investigates \nthe sensitivity of the objective function to each design variable is \nessential. The shapes, such as hyper-rectangle and hyper-ellipse are \nsuggested in the literature for the admissible subsets. However, as shown \nlater via the illustrative example, these shapes fail to include the optimal \nsolution in cases with complex design spaces or problems with multiple \noptimal solutions. And second, identifying the optimal subset that con-\ntains the smallest volume density involves a non-smooth optimization \nproblem which is quite challenging. \nIn this paper, an improved version of SSO is developed to overcome \nthe shortcomings of the original SSO. This new version of the algorithm, \nas mentioned earlier, is named iSSO (improved SSO). Voronoi tessella -\ntion is implemented to partition the design space into non-overlapping \nsubregions (a set of Voronoi cells) using the pool of samples distrib -\nuted according to the auxiliary PDF. The admissible set (a set of all \nadmissible subregions) is then defined as a set containing all subsets of \nthe set of Voronoi cells. This approach is able to capture the regions with \nlower objective function values even if they are disjointed or when the \ndesign space is complex. The details of the Voronoi tessellation are \npresented in Appendix A. A double-sort algorithm is then implemented \nto identify the optimal subset containing the smallest volume density. \nIn the next section, the original SSO is reviewed. Section 3 presents \nthe general theoretical and computational framework for the iSSO al-\ngorithm. Section 4 considers several optimization problems to illustrate \nthe effectiveness and efficiency of the proposed iSSO algorithm. \n2.Original stochastic subset optimization \nIn SSO, say at the i +1th iteration, the design space is represented by \na subset I(i), where I(i) \u2208I(i \u00001)\u22c5\u22c5\u22c5 \u2208I(0) \u2208\u03a6. Following the augmented \nformulation concept initially discussed in [44] for RBDO, the design \nparameters \u03c6, are artificially considered uncertain variables with a \nprescribed PDF p(\u03c6|I(i)) over the design space I(i) [45]. For convenience, \np(\u03c6|I(i)) =1/V(i) is considered, where V(i) is the volume of I(i). In this \nsetting of the augmented stochastic design problem, the auxiliary PDF is \ndefined as: \n\u03c0\u0000\n\u03c6,\u03b8\u20d2\u20d2I(i))\n=h(\u03c6,\u03b8)p\u0000\n\u03c6,\u03b8\u20d2\u20d2I(i))\nE\u03c6,\u03b8[hs(\u03c6,\u03b8)]\u221dh(\u03c6,\u03b8)p\u0000\n\u03c6,\u03b8\u20d2\u20d2I(i))\n(3)  \nwhere, p(\u03c6, \u03b8|I(i)) =p(\u03b8|\u03c6)p(\u03c6|I(i)). Note that if h(\u03c6, \u03b8)\u22640, it must be \nsuitably transformed to ensure that \u03c0(\u03c6, \u03b8|I(i)) \u22650. One way to do this is \nto define hs(\u03c6,\u03b8) =h(\u03c6, \u03b8) \u0000s, since E\u03b8[hs(\u03c6,\u03b8)] =E\u03b8[h(\u03c6,\u03b8)] \u0000s, that is, \nthe two expected values differ only by a constant, and the optimization \nof the expected value of h( \u22c5 ) is equivalent, in terms of the optimal design \nchoice, to optimization for the expected value for hs( \u22c5 ). In the above \nequation, the denominator is a normalizing constant given by: \nE\u03c6,\u03b8[h(\u03c6,\u03b8)]=\u222b\n\u03a6\u222b\n\u0398h(\u03c6,\u03b8)p\u0000\n\u03c6,\u03b8\u20d2\u20d2I(i))\nd\u03b8d\u03c6. (4) \nAlthough this expected value is not explicitly needed, it can be \ndetermined using any state-of-the-art stochastic simulation method. The \nobjective function E\u03b8[hs(\u03c6,\u03b8)]in this context of the auxiliary PDF is \nexpressed as: \nE\u03b8[h(\u03c6,\u03b8)]=\u03c0\u0000\n\u03c6\u20d2\u20d2I(i))\np\u0000\n\u03c6\u20d2\u20d2I(i))E\u03c6,\u03b8[h(\u03c6,\u03b8)], (5)  \nwhere, the marginal \u03c0(\u03c6|I(i)) is given by: \n\u03c0\u0000\n\u03c6\u20d2\u20d2I(i))\n=\u222b\nI(i)\u03c0(\u03c6,\u03b8)d\u03b8. (6) \nIn (5), since E\u03c6,\u03b8[h(\u03c6,\u03b8)]is a normalizing constant, minimization of M.A. Khalid and S. Bansal                                                                                                                                                                                                                   ",
    "Advances in Engineering Software 188 (2024) 103568\n3E\u03b8[h(\u03c6,\u03b8)]is equivalent to minimization of J(\u03c6), which is equal to: \nJ\u0000\n\u03c6\u20d2\u20d2I(i))\n=E\u03b8[hs(\u03c6,\u03b8)]\nE\u03c6,\u03b8[hs(\u03c6,\u03b8)]=\u03c0\u0000\n\u03c6\u20d2\u20d2I(i))\np\u0000\n\u03c6\u20d2\u20d2I(i)). (7) \nThe estimation of the marginal \u03c0(\u03c6|I(i)) in (7) is necessary to mini-\nmize J(\u03c6|I(i)). Analytical approximations of \u03c0(\u03c6|I(i)) based on kernel \ndensity approaches or the maximum entropy method might be arduous \nin case of complex problems, such as when design parameters n\u03c6 are \nlarge, or the sensitivity for some design parameters is complex [44]. In \nthe SSO framework, such approximation of \u03c0(\u03c6|I(i)) is avoided. In SSO, \nsamples distributed as \u03c0(\u03c6|I(i)) are obtained, and the information in \nthese samples is exploited to identify a smaller subset of the design space \nwith a high likelihood of containing the optimal design parameters. \nSamples distributed as \u03c0(\u03c6, \u03b8|I(i)) are obtained using any appropriate \nstochastic sampling algorithm, such as Markov Chain Monte Carlo \n(MCMC) sampling [46]. The \u03c6 component of these samples then cor-\nresponds to samples from the marginal distribution \u03c0(\u03c6|I(i)). \nThe sensitivity of objective function E\u03b8[hs(\u03c6,\u03b8)]to \u03c6 is determined by \nevaluating the average value (or equivalently volume density) of J(\u03c6| \nI(i)) over any subset I in I(i), which is denoted by H(I) and defined as: \nH(I)=1\nVI\u222b\nIJ\u0000\n\u03c6|I(i))\nd\u03c6=1\nVI\u222b\nI\u03c0\u0000\n\u03c6|I(i))\np\u0000\n\u03c6|I(i))d\u03c6=VI(i)\nVI\u222b\nI\u03c0\u0000\n\u03c6|I(i))\nd\u03c6 (8)  \nwhere, VI is the volume of subset I. Based on the samples distributed \naccording to \u03c0(\u03c6|I(i)) belonging to I(i), an estimate of H(I) is provided by: \nH(I)=NI/VI\nNI(i)/VI(i), (9)  \nwhere, NI(i)is the number of samples distributed as \u03c0(\u03c6|I(i)) belonging to \nI(i), and NI denotes the number of samples from \u03c0(\u03c6|I(i)) belonging to the \nI (NI<NI(i\u00001)since I\u2282I(k \u00001)). Say NI=p0NI(i\u00001). A smaller value of \u03c1 re-\nsults in a faster decrease in the size of the identified subsets but with \npoorer accuracy. The use of \u03c1 equal to 0.1 - 0.2 is suggested in the \nliterature [39]. \nA deterministic optimization, based on the estimate H(I)of H(I), is \nnext performed to identify the subset I\u2208A(i+1)\n\u03c1, where A(i+1)\n\u03c1 is a set of \nadmissible subsets in I(i), that contains the smallest volume density NI/ \nVI, that is, I(i+1)=argmin\nI\u2208A\u03c1H(I)=argmin\nI\u2208A(i+1)\n\u03c1NI/\nVI\nA(i+1)\n\u03c1={\nI\u2282I(i):\u03c1=NI/\nN(i)}. (10) \nThe effectiveness of SSO is dependent on the correct selection of the \ngeometrical shape and size of the admissible subsets. Choosing a \ngeometrical shape that effectively investigates the sensitivity of the \nobjective function to each design variable is essential. The optimization \nin (10) determines the subset with the smallest average value of J(\u03c6|I(i)) \n(or equivalently E\u03b8[hs(\u03c6,\u03b8)]) within the admissible set A(i+1)\n\u03c1. I(i +1) is a \nsubset of the design space I(i) with a high likelihood of containing the \noptimal design parameters. The above steps are repeated until the \nstopping criterion is met. This way, SSO adaptively converges to a \nrelatively small subregion within the original design space. The imple -\nmentation of SSO is demonstrated in Fig. 1. The reader may refer to the \noriginal publication for a detailed explanation of SSO [39]. \nH(I(i))expresses the average relative sensitivity of E\u03b8[h(\u03c6,\u03b8)] to \u03c6. A \nlow value of H(I(i))indicates that E\u03b8[h(\u03c6,\u03b8)] is more sensitive to \u03c6, and \nvice versa. A high value of H(I(i)), close to 1 corresponds to a sample \ndensity in design space I(i) that approximates a uniform distribution and \nsuggests that the identified subset I(i) has a low likelihood of containing \n\u03c6* [39]. Therefore, the SSO is stopped when H(I(i))exceeds a threshold \nvalue. A threshold value of 0.75 \u20130.80 has been found to give satisfactory \nresults [39]. \n3.Proposed approach \nIn the proposed approach, the Voronoi tessellation is implemented to \npartition the design space into non-overlapping subregions (a set of \nVoronoi cells) using the pool of samples distributed according to this \nauxiliary PDF. Conceptually, Voronoi tessellation involves partitioning a \nspace into convex polygons, called Voronoi cells, such that each cell \ncontains exactly one sample, called a cell-generating sample. Every \nsample in a given polygon is closer to its generating sample compared to \nany other. In the proposed approach, the admissible set (a set of all \nadmissible subspaces) is defined as a set containing all subsets of the set \nof Voronoi cells. An alternative approach to identify the optimal subset \nwithout performing any non-smooth deterministic optimization is also \npresented. The general theoretical and computational framework for the \niSSO algorithm is presented in the following subsections, and the \nFig. 1.Illustration of the original SSO algorithm.  M.A. Khalid and S. Bansal                                                                                                                                                                                                                   ",
    "Advances in Engineering Software 188 (2024) 103568\n4algorithm is demonstrated in Fig. 2. \n3.1. Partitioning of design space \nIn the proposed approach, at the i +1th iteration, say N(i) is the \nnumber of samples distributed as \u03c0(\u03c6|I(i)) belonging to the design space \nI(i). Let nv=N(i)/(1+\u03b3), \u03b3 \u22650 be the number of unique samples. If \nsampling techniques such as accept rejection, importance sampling, etc., \nare used, then \u03b3 =0, and each sample in the design space will be unique. \nHowever, if MCMC sampling techniques are used, the resulting samples \nwill be correlated, that is \u03b3 >0, and we will have repeated samples. \nAssume that the design space I(i) is divided into v(i)\nk, k =1\u22c5\u22c5\u22c5nv, Voronoi \ncells using nv unique samples, and say the Voronoi cell v(i)\nk contains \u03b7(i)\nk \nrepeated samples, then, an estimate of \u03c0(\u03c6|I(i)) is provided by: \n\u03c0\u0000\n\u03c6|I(i))\n=\u03b7(i)\nk\nN(i)V(i)\nk\u22650,\u2200\u03c6\u2208v(i)\nk, (11)  \nwhere, V(i)\nk is the volume of the kth Voronoi cell. Obviously, \u222b\nI(i)\u03c0(\u03c6|I(i))\nd\u03c6=1. \nSimilar to the original SSO, the sensitivity of the objective function J \n(\u03c6|I(i)) to \u03c6 is determined by evaluating the average value of J(\u03c6|I(i)) \nover any subspace I of the design space I(i). Subset I is any subset of \nnvVoronoi cells (these cells may be disjointed). Since the design space is \npartitioned into nv subspaces or Voronoi cells, the number of admissible \nsubsets (proper subsets) is given by 2n\u03bd\u00001. Based on the estimate \n\u03c0(\u03c6|I(i))provided in (11), an estimate of H(I) is provided as: \nH(I)=V(i)\nVI\u222b\nI\u03c0\u0000\n\u03c6|I(i))\nd\u03c6=V(i)\nVI\u2211\nI\u03b7(i)\ni\nN(i)=V(i)\nVINI\nN(i)(12)  \nwhere, VI is the volume of the subset I and NI is the number of samples \nbelonging to it. Let I={v(i)\n(1),v(i)\n(2)\u22efv(i)\n(S)}, where S is the number of Vor-\nonoi cells defining the subset I. Note that the parentheses are used in the \nsubscript to differentiate between the Voronoi cell number defined in \nthe previous section from the Voronoi cell index describing the subset I. \nAn estimate of H(I) is then provided as: \nH(I)=V(i)\nN(i)[\n\u03b7(i)\n(1)+\u03b7(i)\n(2)+\u22ef+\u03b7(i)\n(S)\nV(i)\n(1)+V(i)\n(2)+\u22ef+V(i)\n(S)]\n. (13)  3.2. Identification of an optimal subset \nA deterministic optimization needs to be performed to identify a \nsubset I that contains the smallest volume density NI/VI. In the case of \nunique samples, since \u03b7(i)\n(\u22c5)=1, the solution to the minimization problem \nin (10) is a set of \u03c1N(i) Voronoi cells with the largest volume. For the case \nwith repeated samples, the optimization can be performed using \nmethods appropriate for non-smooth optimization problems, such as \nsub-gradient methods, bundle methods, gradient sampling methods, etc. \nIn this study, we propose an alternative approach to identify the \noptimal subset without performing any non-smooth deterministic opti-\nmization. A double-sort algorithm is proposed, which involves sorting \nthe Voronoi cells in ascending order of the sample counts and then in \ngroups of cells with the same sample count in descending order of cell \nvolume. Finally, the top cells containing \u03c1N(i)samples are selected as an \napproximate optimal solution from the sorted list. \nOne may argue that the optimal subset can be obtained by first \nsorting the Voronoi cells in ascending order of the cell density, defined \nas \u03b7(i)\nk/V(i)\nk, and then by selecting the top cells containing \u03c1N(i) samples \nfrom the sorted list. However, this argument is erroneous because the \nobjective is to minimize \u2211S\ns=1\u03b7(i)\n(s)/\u2211S\ns=1V(i)\n(s)and not \u2211S\ns=1(\u03b7(i)\n(s)/V(i)\n(s)). The \neffectiveness of the proposed double-sort algorithm is demonstrated in \nSection 4 with the help of examples. \n3.3. Simulation of conditional samples \nAt the i +1th iteration, \u03c1N(i) samples distributed as \u03c0(\u03c6|I(i +1)) are \navailable from the previous iteration. Using these samples as seeds, \nadditional (1 \u0000\u03c1)N(i +1) are simulated. The proposed method to \nsimulate additional samples involves two steps: (a) randomly selecting a \nVoronoi cell within the subset I(i +1) based on the estimate \u03c0(\u03c6|I(i))and \n(b) applying the Metropolis-Hastings algorithm within the selected \nVoronoi cell. \nA Voronoi cell is selected according to the following weights in the \nfirst step: \nw(i)\nk=\u03b7(i)\nk/\nV(i)\nk\n\u2211\nk\u03b7(i)\nk/\nV(i)\nk. (14) \nFig. 2.Illustration of the proposed iSSO algorithm.  M.A. Khalid and S. Bansal                                                                                                                                                                                                                   ",
    "Advances in Engineering Software 188 (2024) 103568\n5To simulate a new sample within a selected Voronoi cell, the sample \nthat generated the selected Voronoi cell or the last simulated sample in \nthe selected Voronoi cell is used as the seed sample, and the Metropolis- \nHastings algorithm is implemented. A candidate sample [\u03c6c,\u03b8c] is \nsimulated using the proposal q(\u03c6c,\u03b8c|\u03c6,\u03b8) and is accepted with the \nprobability min(1, a0), where, a0 is given as: \na0=h(\u03c6c,\u03b8c)p(\u03c6c,\u03b8c)q(\u03c6,\u03b8|\u03c6c,\u03b8c)\nh(\u03c6,\u03b8)p(\u03c6,\u03b8)q(\u03c6c,\u03b8c|\u03c6,\u03b8). (15) \nIn the present study, the proposed PDF is equal to the uniform PDF \nfor design parameters and the initial PDF for uncertain variables, i.e., q \n(\u03c6, \u03b8|\u03c6c,\u03b8c) =p(\u03c6, \u03b8). Therefore, on simplifying (15), a0 is given as: \na0=h(\u03c6c,\u03b8c)\nh(\u03c6,\u03b8). (16)  \n3.4. Stopping criteria \nA new stopping criterion is proposed in this study. The convergence \nof the expected value of the performance measure h(\u03c6, \u03b8) with respect to \nthe PDF for \u03c6 and \u03b8 in consecutive iterations is used as the stopping \ncriterion. Mathematically the proposed stopping criterion is represented \nby: \n\u20d2\u20d2E\u03c6,\u03b8[h(\u03c6,\u03b8)]i\u0000E\u03c6,\u03b8[h(\u03c6,\u03b8)]i\u00001\u20d2\u20d2\u2264\u03b5 (17)  \nwhere, \u03b5 is a user-specified tolerance limit. Other stopping criteria, as \nindicated in [39,47], can also be chosen. 3.5. Implementation issues \nAn important issue for the effective implementation of the iSSO is the \ncreation of the Voronoi cells at the current iteration bounded within the \nVoronoi cell created at the previous iterations. Although it is possible to \ncreate such bounded Voronoi cells, due to the geometrical complexities, \nit is usually unfeasible for the higher dimensional problems (n\u03c6>2). An \nalternative approach is proposed in the present study for creating the \nVoronoi cells at any iteration of the iSSO. The proposed approach in-\nvolves creating Voronoi cells using the samples generated at the current \nand all previous iterations and then by considering Voronoi cells cor-\nresponding to the samples from the current iteration. This is shown in \nFig. 3, where Fig. 3(a) shows the N samples at the first iteration and the \ncorresponding Voronoi cells. Fig. 3(b) shows the \u03c1N selected Voronoi \ncells leading to the smallest volume density and the additional (1 \u0000\u03c1)N \nsamples being generated using these \u03c1N samples as seeds. Fig. 3(c) \nshows that the Voronoi cells are generated using all N +(1 \u0000\u03c1)N \nsamples that are generated in the two iterations. The Voronoi cells \ncorresponding to the N samples for consideration at the second iteration \nare also highlighted in Fig. 3(c). Fig. 3(d) shows a zoomed-in version of \nFig. 3(c) where it can be observed that the area covered by the N Voronoi \ncells considered in the second iteration is not the same as the area \ncovered by the \u03c1N Voronoi cells selected in the first iteration. On the \ncontrary, the area covered by the Voronoi cells in the second iteration is \nmore than the area covered by the Voronoi cells corresponding to the \nseed samples from the first iteration. This is because a new sample \nwithin the Voronoi cell between an existing sample and the existing \nVoronoi cell edge results in the relocation of the Voronoi cell edge in a \nFig. 3.Implementation of Voronoi tessellation in iSSO.  M.A. Khalid and S. Bansal                                                                                                                                                                                                                   ",
    "Advances in Engineering Software 188 (2024) 103568\n6direction away from the new sample. The increase at each iteration in-\ntroduces a bias in the estimate of \u03c0(\u03c6|I(i)) in (11). However, this does not \naffect the performance of the proposed approach as the objective is not \nto simulate the samples distributed as \u03c0(\u03c6|I(i)) but to identify the subsets \nfor an optimal solution. In addition, the increase is not substantial, as \nseen later in the illustrative examples in Section 4. \n3.6. Special case: deterministic optimization \nIn the iSSO framework, a deterministic optimization problems can \nalso be handled with the vector of uncertain variables \u03b8 set equal to a \nnull vector (n\u03b8 =0). Since the determination of the subset at each iSSO \niteration is solely dependent on the samples distributed as \u03c0(\u03c6), no \nmodification to the iSSO algorithm is required to solve a deterministic \noptimization problem, and the entire formulation remains valid. \n4.Illustrative examples \nIn this section, typical optimization problems are considered to \ndemonstrate the effectiveness and efficiency of the proposed approach. \nFirst, deterministic optimization problems are considered. These prob-\nlems include several local and global minima. Next, stochastic optimi -\nzation problems are illustrated. The second example presents an RDO \nproblem of the TMD. In this example, the variance minimization of the \nprotected structure \u2019s displacement (TMD attached to the structure) is performed. In the third example, the mean minimization of 120 bars \ntruss problems is explored to demonstrate the applicability of the pro-\nposed approach to a high-dimensional stochastic design problem. \nFinally, the fourth example investigates the reliability-based optimiza -\ntion of a base isolation system for a 10-story building. \nIn this study, after implementing iSSO, the optimal design solution is \nidentified as follows. Let \u03b8j, j =1\u22c5\u22c5\u22c5n be a set of independent, identically \ndistributed realizations of \u03b8, and let h(\u03c6, \u03b8j) be the structural perfor -\nmance function realization for \u03b8j. The expected structural performance \nfunction is approximated by the average of the realizations as: \nE\u03b8[h(\u03c6,\u03b8)]\u22481\nn\u2211n\nj=1h\u0000\n\u03c6,\u03b8j)\n. (18) \nE\u03b8[h(\u03c6,\u03b8)]is evaluated for all unique \u03c6 samples obtained at the last \niteration of the iSSO, and the \u03c6 sample resulting in the smallest value of \nE\u03b8[h(\u03c6,\u03b8)]is taken as the optimal solution. Alternatively, as the right- \nhand side of (18) is deterministic, any deterministic optimization \nmethod can also be used to solve the optimization problem with the \napproximate expectation. \nIn the following examples, both iSSO and SSO are implemented with \nN =1000 n\u03c6, \u03c1 =0.20 and the stopping criteria as stated in (17). Here, a \nvalue of \u03b5 =10\u00003 is adopted. \nFig. 4.Results for the Griewank function.  M.A. Khalid and S. Bansal                                                                                                                                                                                                                   ",
    "Advances in Engineering Software 188 (2024) 103568\n74.1. Multimodal deterministic optimization problems \nIn this section, three two-dimensional benchmark deterministic \noptimization problems are considered. Results are also compared with \nthe SSO. The test functions are:  \na) Griewank function: \nminh(\u03c6)=\u2211d\ni=1\u03c62\ni\n4000\u0000\u220fd\ni=1cos(\u03c6i\u0305\u0305\ni\u221a)\n+1,\ns.t.\u03c6=[\u000010,10](19)    \nb) Cross-in-Tray function: \nminh(\u03c6)=\u00000.0001(\u20d2\u20d2\u20d2\u20d2\u20d2sin(\u03c61)sin(\u03c62)exp(\u20d2\u20d2\u20d2\u20d2\u20d2100\u0000\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\n\u03c62\n1+\u03c62\n2\u221a\n\u03c0\u20d2\u20d2\u20d2\u20d2\u20d2)\u20d2\u20d2\u20d2\u20d2\u20d2+1)0.1\n,\ns.t.\u03c6=[\u000010,10]\n(20)    \nc) Holder Table function: minh(\u03c6)=\u0000\u20d2\u20d2\u20d2\u20d2\u20d2sin(\u03c61)cos(\u03c62)exp(\u20d2\u20d2\u20d2\u20d2\u20d21\u0000\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\n\u03c62\n1+\u03c62\n2\u221a\n\u03c0\u20d2\u20d2\u20d2\u20d2\u20d2)\u20d2\u20d2\u20d2\u20d2\u20d2,\ns.t.\u03c6=[\u000010,10](21))   \nThe results for the Griewank function are presented in Fig. 4. Fig. 4(a, \nb) shows that the function has multiple closely spaced local minima with \na single global minimum. Fig. 4(c, d) shows the SSO optimization using \nhyper-rectangle and hyper-ellipse as shapes of admissible subsets. It is \nseen that these shapes fail to capture the region containing the optimal \ndesign due to the presence of multiple local minima. Next, the iSSO is \nimplemented, where the Voronoi cells selected at the first and last \niteration are shown in Fig. 4(e, f). It is observed that at the first iteration, \nthe selected Voronoi cells effectively capture both the local and global \nminima and in the subsequent iterations, the selected cells are more \nconcentrated near the global minimum. The region selected at the last \niteration captures the optimal global solution. \nThe Cross-in-Tray function has a relatively complex design space \ncompared to the Griewank function. Fig. 5(a, b) shows multiple local \nand global minima. Minimization by using SSO is demonstrated in Fig. 5 \n(c, d). It is found that both the hyper-rectangle and hyper-ellipse are \ntrapped around any one of the global minima. At the same time, the iSSO \nis able to capture the regions that include all of the global minima, as \nFig. 5.Results for the Cross-in-Tray function.  M.A. Khalid and S. Bansal                                                                                                                                                                                                                   ",
    "Advances in Engineering Software 188 (2024) 103568\n8seen in Fig. 5(e, f). \nThe Holder Table function has multiple local and global minima; the \nglobal minima are placed at the boundary of the design space, as shown \nin Fig. 6(a, b). Once again, it is seen that both the hyper-rectangle and \nhyper-ellipse are trapped around any of one of the global minima, and \non the other hand, the iSSO is able to capture the regions that include all \nof the global minima, as seen in Fig. 6(e, f). \nThe results from the three examples demonstrate that the proposed \niSSO is able to capture the regions containing the optimal solution \neffectively. \nNext, the statistics of the results of 50 independent runs, both for SSO \nand iSSO are presented in Table 1. It also includes the results obtained by \nusing state-of-the-art approaches, such as the Genetic algorithm, particle \nswarm optimization, and the gradient based optimization approach \n(interior-point algorithm). The proposed iSSO outperforms all other \napproaches as more successes in determining the optimal solution are \nobserved in all three optimization problems. It is also seen that both SSO \nand iSSO result in a similar value of volume reduction for the same \nstopping criterion; however, with SSO, the number of iterations required \nto achieve this volume reduction are relatively higher. The proposed \napproach outperformed the state-of-the-art approaches, as indicated by \nthe number of successes. These examples demonstrate that the main \nadvantage of implementing Voronoi tessellation is an effective explo -\nration of the design space. \nNext, the performance of the proposed \"double sort algorithm\" for \nselecting the optimal subset is studied by using the above-mentioned three functions. Fig. 7 shows the value of H(I(1))for the 50 indepen -\ndent simulation runs, which is estimated by implementing the proposed \ndouble sort algorithm and by using the Genetic algorithm. It can be \nnoted that for each run, the H(I(1))values obtained using the proposed \ndouble sort algorithm and Genetic algorithm are well matched, thereby \nconfirming the adequacy of the proposed double sort algorithm. \nAt any iteration of iSSO, new samples are simulated using the seed \nsamples. In the proposed approach, the volume of the Voronoi cells \ncorresponding to the seed and new samples is greater than the volume of \nthe Voronoi cells corresponding only to the seed samples. Fig. 8 shows \nthis change in volume V(seeds+new)\u0000V(seeds)\nV(seeds) due to the creation of Voronoi cells \nat any generation of iSSO using the procedure mentioned in Section 3.4. \nThe increase is observed to be small which further reduces with an in-\ncrease in the iteration number. It is also observed that the increase in \nvolume decreases with an increase in sample size at each iteration and \nincreases with an increase in the dimension of the problem. \n4.2. Robust design optimization of the tuned mass damper \nThis example considers a stochastic design problem involving a \nTuned Mass Damper (TMD) attached to a Single Degree of Freedom \n(SDOF) system. The problem is taken from [48] and is shown in Fig. 9. \nIn this problem, the system is excited by a white noise signal with a \nmean zero and unit variance. The performance measure is the variance \nof the displacement of the system \u03c32\nxs. The mass mS, stiffness kS, and \nFig. 6.Results for the Holder Table function.  M.A. Khalid and S. Bansal                                                                                                                                                                                                                   ",
    "Advances in Engineering Software 188 (2024) 103568\n9damping cS of the system are taken as uncertain parameters, following \nindependent Gaussian distribution. The mean value of these variables is \ntaken to be 105 kg, 107 N/m, and 4 \u00d7104 Ns/m respectively. To account \nfor uncertainty, the c.o.v value for each variable taken is 0.05. The \nfrequency ratio \u03b2=\u03c9T/\u03c9S and damping \u03beT of the TMD are considered \ndesign parameters. The TMD has a mass ratio, mT/ms, of 0.10. The parameters mT,\u03c9T,and\u03c9S are, in order, the mass of the TMD, the natural \nfrequency of the TMD (\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\nkT/mT\u221a\n), and the natural frequency of the \nstructure (\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\u0305\nks/ms\u221a\n). The optimization problem is written as: \nminimize :\n\u03c6\u2208\u03a6,\u03c6\u2208\u03a6E\u03b8[h(\u03b8,\u03c6,\u03c6)]=E\u03b8[(\n\u03c32\nxs(\u03b8,\u03c6)\u0000\u03c6)]2\n, (22) Table 1 \nStatistics of optimization results for multimodal deterministic optimization problems.  \nExample SSO iSSO GA* PSO* GBA*   \nHyper-Rectangle Hyper-Ellipse     \nGriewank NF 30 32 5 38 35 47  \nNS 20 18 45 12 15 3  \nBV 0 0 0 0 0 0  \nWV 0.1028 0.1161 0.0270 0.0296 0.0232 0.0296  \nAV 0.0190 0.0292 0.0110 0.0094 0.0057 0.0173  \nc.o.v 1.1663 1.0168 0.5137 0.9603 0.8763 0.05971  \nFE 22,702 15,223 12,426 3385 1432 33  \nGen 7 5 4 N/A  \nVR 94.025 84.79 93.56    \nCross-In-Tray NF 50 50 1 50 50 50  \nNS 0 0 49 0 0 0  \nBV \u00002.0576 \u00002.0626 \u00002.0624 \u00002.0626 \u00002.0626 \u00002.0626  \nWV \u00002.0472 \u00002.0481 \u00002.0260 \u00002.0626 \u00002.0626 \u00001.3853  \nAV \u00002.0527 \u00002.0621 \u00002.0522 \u00002.0626 \u00002.0626 \u00001.7360  \nc.o.v 0.0133 0.001 0.0042 0 0 0.0977  \nFE 27,563 21,595 9982 3178 933 32  \nGen 9 7 3 N/A  \nVR 99.89 98.95 88.71    \nHolder-Table NF 50 50 3 50 50 50  \nNS 0 0 47 0 0 0  \nBV \u000019.2085 \u000017.5025 \u000019.2085 \u000019.2085 \u000019.2085 \u000019.2085  \nWV \u000018.8916 \u00001.1419 \u000017.3030 \u00009.5047 \u000015.1402 \u00001.1831  \nAV \u000019.0916 \u00008.432 \u000018.8798 \u000019.0144 \u000018.9745 \u00006.5493  \nc.o.v 0.0025 0.3898 0.0182 0.0722 0.0443 0.8358  \nFE 40,700 24,684 18,142 3413 988 30  \nGen 13 8 6 N/A  \nVR 99.59 99.42 94.37    \nGA =genetic algorithm, PSO =particle swarm optimization, GBA =gradient-based optimization approach, NF =no. of. failure, NS =no. of. success, BV =best value, \nWV =worst value, AV =average value, c.o.v =coefficient of variation, FE =no. of. function evaluations, Gen =generations, VR =volume reduction percentage, * =\nefficiently applicable only for deterministic problems. \nFig. 7.Comparison of double sort algorithm and Genetic algorithm results.  \nFig. 8.Percentage change in volume at each iSSO iteration.  M.A. Khalid and S. Bansal                                                                                                                                                                                                                   ",
    "Advances in Engineering Software 188 (2024) 103568\n10where, \n0.01\u2264\u03b2\u22641.5,0.01\u2264\u03beT\u22641.0,0\u2264\u03c6\u22641000. (23) \nTable 2 presents the optimal design parameter values as well as the \nobjective function value that solve the optimization problem in (22). \nResults obtained using SSO, Sample Average Approximation (SAA), and iSSO are shown. SAA is applied with a sample size of 103, as mentioned \nin [43]. The results demonstrate that iSSO is effective in locating the \noptimal solution. SSO implemented with hyper-ellipse gives an optimal \nsolution but has a higher computational cost. \n4.3. 120-bars truss structure \nThe third example involves minimizing the mean of the compliance \nof a 120-bar linear elastic truss structure shown in Fig. 10 under the \nweight constraint W \u226415, 000kg. Because of structural symmetry, \ndesign parameters corresponding to the cross-sectional areas of elements \nare divided into seven groups, each with a minimum area of 10\u00004 m2. \nThe Young \u2019s modulus for the bar groups are assumed as uncorrelated \nnormal random variables with mean values equal to 210 GPa and the c. \no.v equal to 0.10 respectively. The density of the material is 7971.89 kg/ \nm3. The dome is subjected to concentrated vertical loads acting down -\nward at the top node, normally distributed with a mean equal to 60 kN \nand c.o.v equal to 0.20. In addition, the mass of bars is concentrated at \nthe nodes. The problem is taken from [48]. \nTable 3 presents the best of 10 independent run results obtained with \nSSO and iSSO. Once again, the SSO and iSSO solutions agree well, \nthereby demonstrating the effectiveness of the proposed approach. At \nthe same time, the number of function evaluations is substantially less in \nthe case of iSSO, indicating the efficiency of the proposed approach. \n4.4. Reliability-based design of a base isolated structure \nThis example, adapted from [49], involves the reliability-based \nFig. 9.TMD attached to a SDOF system [48].  \nTable 2 \nVariance minimization of TMD-structure.  \nMethod Admissible Subset shape Design parameters E\u03b8[h(\u03b8,\u03c6,\u03c6)]\n(\u00d710\u000016 mm4) FE NS NF \n\u03b2 \u03beT \nSSO [48] Hyper-Rectangle 0.551 0.623 41.324 7433 0 50 \nHyper-Ellipse 0.749 0.221 1.7586 8245 34 16 \nSAA N/A 0.749 0.221 1.7587 3 \u00d7106 4 46 \niSSO Voronoi tessellation 0.749 0.221 1.7586 6198 50 0  \nFig. 10.120-bar dome truss structure [48].  \nTable 3 \nResults for the 120 bars truss structure.  \nMethod Design parameters \u03bcg(\u0303\u03c6\u2217)\n(Nm) \u03c32\ng(\u0303\u03c6\u2217)\n(Nm)2 FE \nA1 \n(cm2) A2 \n(cm2) A3 \n(cm2) A4 \n(cm2) A5 \n(cm2) A6 \n(cm2) A7 \n(cm2) \nSSO [48] 47.2 67.5 42.7 9.4 30.1 55.6 13.1 242.3 5294.2 54,700 \nSAA 47.3 68.3 40.7 10.5 30.3 49.5 14.8 243.5 5407.4 3 \u00d7106 \niSSO 48.1 66.4 42.6 8.5 31.4 56.5 12.5 242.8 5317.2 14,937  M.A. Khalid and S. Bansal                                                                                                                                                                                                                   ",
    "Advances in Engineering Software 188 (2024) 103568\n11optimization of a base-isolation system attached to a 10-story building \nas shown in Fig. 11. This optimization problem includes maximizing the \nreliability of the base-isolated structure which is performed by the \nminimization of its failure probability and mathematically expressed as: \nminimize :\n\u03c6\u2208\u03a6P(F|\u03c6)=E\u03b8[IF(\u03c6,\u03b8)]=\u222b\n\u0398IF(\u03c6,\u03b8)p(\u03c6,\u03b8)d\u03b8, (24)  \nwhere, IF(\u03c6,\u03b8) is the function that indicates failure, and it equals 1 when \nthe system fails, i.e., when unacceptable performance occurs. Notably, \nin this problem h(\u03c6, \u03b8) =IF(\u03c6,\u03b8). \nThe 10-story building is considered as a shear structure with un-\ncertain inter-story stiffness and damping. Each story has a total mass of \n207 ton. The inter-story stiffness ki of all stories are parameterized by ki \n=\u0302ki\u03b8i,i=1,\u2026,10 where the most probable values of the inter-story \nstiffness are [\u0302ki]=[687.1, 613.1, 540.1, 481.1, 421.7, 353.7, 286.6, \n225.6, 184.5, 104.5] MN/m. The entity \u03b8i is a set of non-dimensional \nuncertain variables that are considered to be correlated Gaussian vari-\nables with a unit mean value \u0302\u03b8i=1,\u2200i and a covariance matrix defined \nas: \nE[\n(\u03b8i\u0000\u0302\u03b8i)\u0000\n\u03b8j\u0000\u0302\u03b8j)]\n=(0.2)2exp[\n\u0000(j\u0000i)2/\n22]\n. (25) \nThe damping ratios are considered independent Gaussian variables \nwith mean values of 0.025 and c.o.v of 0.10 for all modes. The Kanai- \nTajimi model is used to simulate the ground excitation modelled as a \nfiltered white noise process, with the power spectral density function \ngiven as: \nS(\u03c9)=S0\u03c94\ng+4\u03b62\ng\u03c92\ng\u03c92\n(\n\u03c92\ng\u0000\u03c92)2\n+4\u03b62\ng\u03c92\ng\u03c92, (26)  S0=\u03c32\n\u03c92\u03b6g\n\u03c0\u03c9g(\n4\u03b62\ng+1)m2/\ns3, (27)  \nwhere, \u03c9g, \u03b6gand \u03c3\u03c9 are the resonant frequency, damping, and RMS of the \nacceleration input of the filter, respectively. These are also considered \nuncertain variables with mean values of [2\u03c0rad/s, 0.5, 0.2g] and a c.o.v \nequal to 0.20. The non-stationarity of the excitation is modeled by \nmultiplying the filter output with the envelope function as: \ne(t)=\u03bb3t\u03bb1exp(\u0000\u03bb2t), (28)  \nwith parameters \u03bb1 =1.25, \u03bb2 =0.2 and \u03bb3 =0.353 chosen to simulate \nstrong earthquake excitation for a duration of 40 s with a sampling time \nof 0.02 s. The base-isolation system considered is a lead\u2013rubber bilinear \nisolator with an additional viscous damper. The base has a 247-ton mass. \nThe design parameters \u03c6 for the base isolation structure system are the \nstiffness before yielding Kprand after yielding Kp, the yield force is Fy, \nand the damping coefficient cd. The reader may refer to [39,50] for \nadditional details regarding the base isolation structure system adopted \nin this study. \nFailure is indicated when any of the normalized base displacements \nor inter-story drifts exceeds unity. The normalization constants are 0.5 m \nand 0.033 m respectively. The design interval for each variable is \nspecified as Kpr =[50, 600] MN/m, Fy =[1, 8] MN, Kp =[5, 60] MN/m, \nand cd =[0.1, 10]MNs/m. In this example, iSSO and SSO are imple -\nmented with six number of iterations. \nTable 4 shows the optimization results for the best 10 independent \nsimulation runs. The comparison of the results obtained using SSO, SAA \n(with a sample size of 103), and iSSO shows that the optimal design \nobtained using the proposed approach iSSO is in good agreement. The \nfailure probability of the structure is reduced from 0.95 (without the \nbase isolation system) to 0.0326 after installing the optimally designed \nbase isolation system. \n5.Conclusion \nThis study attempts to provide an optimization approach called \n\"iSSO\", which is an improved version of SSO, primarily for stochastic \noptimization problems while it retains utility for deterministic optimi -\nzation problems as well. Two novel ideas are introduced in this study: \nfirst, a better characterization of the design space is offered by parti-\ntioning the design space into non-overlapping subregions using Voronoi \nFig. 11.(left) 10-story base isolated shear model, and (right) force-deformation of bilinear isolator [49].  \nTable 4 \nBase isolation structure system optimization results (best of 10 independent \nruns).  \nMethod Design parameters (\u03c6*) Failure \nprobability \nPF(\u03c6*) Kpr (MN/ \nm) Fy \n(MN) Kp (MN/ \nm) cd (MNs/ \nm) \nSSO  \n[39] 425.33 1.20 15.52 6.54 0.0340 \nSAA 414.68 1.16 16.15 6.26 0.0324 \niSSO 418.34 1.11 15.88 7.08 0.0366  M.A. Khalid and S. Bansal                                                                                                                                                                                                                   ",
    "Advances in Engineering Software 188 (2024) 103568\n12tessellation which improves the effectiveness and efficiency of the pro-\nposed iSSO considerably in comparison to SSO. Second, a novel \"double \nsort\" approach is proposed, eliminating the need for optimization to \nidentify the subregions for the optimal design at each iSSO iteration. \nSeveral mathematical and engineering design examples, including TMD, \n120 bars truss structure, and base-isolated structure, are included in this \nstudy to demonstrate the efficacy of the proposed iSSO. The results show \nthat the proposed iSSO effectively identifies the reduced design space for \ncomplex design problems with multiple global and local minima. This is \nattributable to the Voronoi tessellation, which eliminates the require -\nment of the presumed admissible design space form to resemble the \ncontour of the original design. Voronoi tessellation enabled better \ndesign space exploration, allowing multiple global minima scattered \nthroughout the design pace to be effectively identified. Due to the dis-\ncretization of the design space via Voronoi tessellation, computation \ndemand is significantly reduced as the number of function evaluations \nfor all examples is lower vis-a-vis the original SSO. Moreover, the novel \nidea of the double sort approach achieves the requisite precision in \nidentifying the subregions for optimal solutions and makes iSSO \nimplementation simple and effective. \nThe applicability of the approach is dependent on the creation of the Voronoi cells. At present the methods available in the literation for \ncreating the Voronoi tessellation are computationally demanding when \nconsidering problems of very high dimension. Future work will focus on \ndeveloping a method for creating the Voronoi tessellation in higher di-\nmensions, particularly those greater than ten. \nCRediT authorship contribution statement \nMohd Aman Khalid: Investigation, Methodology, Formal analysis, \nSoftware, Visualization, Writing \u2013 original draft. Sahil Bansal: \nConceptualization, Methodology, Supervision. \nDeclaration of Competing Interest \nThe authors declare that they have no known competing financial \ninterests or personal relationships that could have appeared to influence \nthe work reported in this paper. \nData availability \nNo data was used for the research described in the article.  \nAppendix-A: Voronoi Tessellation \nVoronoi tessellation is a mathematical concept named after the Russian mathematician Georgy Voronoi. It is also known as the Voronoi diagram or \nDirichlet tessellation. A Voronoi tessellation of a set of points P in a plane is a partition of the plane into a set of non-overlapping convex polygons, with \neach polygon including precisely one point of P and each point in a polygon being closer to its associated point in P than to any other point in P. Each \npolygon is referred to as a Voronoi cell or a Dirichlet region. The boundary of each cell is constituted of points that are equidistant to two or more \npoints in P. Fig. 12 shows the Voronoi diagram in a two-dimensional design space.\nFig. 12.Voronoi diagram in 2-dimensional space.  \nThere are several efficient algorithms for creating Voronoi diagrams. One such basic algorithm is to start with a set of points and then compute the \nVoronoi cells by dividing the space into regions based on the distance to the nearest point. The Bowyer-Watson algorithm [51], which generates a \nDelaunay triangulation in any number of dimensions, can be applied while creating a Voronoi diagram. The Delaunay triangulation is a triangulation \nof the point in which no point falls within the circumcircle of any triangle. The polygon generated by the intersection of the half-planes defined by the \nedges of the Delaunay triangles enclosing the point is therefore obtained as the Voronoi cell of a point. \nIt can be summarized that Voronoi tessellation is a powerful mathematical concept that aids in dividing space into regions based on the distance to \na set of points. Voronoi tessellation finds widespread applications in areas such as image processing [52], spatial topology analysis [53], and \nmicrostructure study [52]. The MATLAB command \"Voronoin \" from the \"Parallel Computing Toolbox \" [54] has been used in this study to create the \nVoronoi cells. \nReferences \n[1]Marti K. Stochastic optimization methods. Berlin: Springer; 2008 . \n[2]Tsompanakis Y, Lagaros ND, Papadrakakis M. Structural design optimization \nconsidering uncertainties. CRC Press; 2008 . \n[3]Khalid MA, Bansal S, Ramamohan V. An augmented formulation for robust design \noptimization of structures using stochastic simulation method. Res Eng Des 2023; \n34:179 \u2013200. https://doi.org/10.1007/s00163-022-00405-z . [4]Meng Z, Li G, Wang X, Sait SM, R\u0131za A. A comparative study of metaheuristic \nalgorithms for reliability \u2011 based design optimization problems. Arch Comput \nMethods Eng 2021;28:1853 \u201369. https://doi.org/10.1007/s11831-020-09443-z . \n[5]Abualigah L, Elaziz MA, Khasawneh AM, Alshinwan M. Meta-heuristic \noptimization algorithms for solving real-world mechanical engineering design \nproblems : a comprehensive survey, applications, comparative analysis, and \nresults. Neural Comput Appl 2022;34:4081 \u2013110. https://doi.org/10.1007/s00521- \n021-06747-4 . \n[6]Katebi J, Shoaei M, Nguyen S, Trung T, Khorami M. Developed comparative \nanalysis of metaheuristic optimization algorithms for optimal active control of M.A. Khalid and S. Bansal                                                                                                                                                                                                                   ",
    "Advances in Engineering Software 188 (2024) 103568\n13structures. Eng Comput 2020;36:1539 \u201358. https://doi.org/10.1007/s00366-019- \n00780-7 . \n[7]Alorf A. Engineering applications of artificial intelligence a survey of recently \ndeveloped metaheuristics and their comparative analysis. Eng Appl Artif Intell \n2023;117:105622. https://doi.org/10.1016/j.engappai.2022.105622 . \n[8]Kirsch U. Structural optimization: fundamentals and applications. Springer-Verlag; \n2012 . \n[9]Floudas CA, Pardalos PA. Encyclopedia of optimization. Springer; 2008 . \n[10] Kiureghian AD, Ditlevsen O. Aleatory or epistemic? Does it matter? Struct Saf \n2009;31:105 \u201312. https://doi.org/10.1016/j.strusafe.2008.06.020 . \n[11] Schu \u00a8eller GI, Jensen HA. Computational methods in optimization considering \nuncertainties - an overview. Comput Methods Appl Mech Eng 2008;198:2 \u201313. \nhttps://doi.org/10.1016/j.cma.2008.05.004 . \n[12] Schneider J, Kirkpatrick S. Stochastic optimization. Springer; 2007 . \n[13] Do B, Ohsaki M. A random search for discrete robust design optimization of linear- \nelastic steel frames under interval parametric uncertainty. Comput Struct 2021; \n249:106506. https://doi.org/10.1016/j.compstruc.2021.106506 . \n[14] Asadpoure A, Tootkaboni M, Guest JK. Robust topology optimization of structures \nwith uncertainties in stiffness - application to truss structures. Comput Struct 2011. \nhttps://doi.org/10.1016/j.compstruc.2010.11.004 . \n[15] Doltsinis I, Kang Z. Robust design of structures using optimization methods. \nComput Methods Appl Mech Eng 2004;193:2221 \u201337. https://doi.org/10.1016/j. \ncma.2003.12.055 . \n[16] Carneiro G, das N, Ant\u00b4onio CC. Dimensional reduction applied to the reliability- \nbased robust design optimization of composite structures. Compos Struct 2021; \n255. https://doi.org/10.1016/j.compstruct.2020.112937 . \n[17] An H, Youn BD, Kim HS. Reliability-based design optimization of laminated \ncomposite structures under delamination and material property uncertainties. Int J \nMech Sci 2021. https://doi.org/10.1016/j.ijmecsci.2021.106561 . \n[18] Li Z, Duan LB, Cheng AG, Yao ZP, Chen T, Yao W. Lightweight and crashworthiness \ndesign of an electric vehicle using a six-sigma robust design optimization method. \nEng Optim 2019. https://doi.org/10.1080/0305215X.2018.1521396 . \n[19] Gholinezhad H, Torabi SH. Reliability-based multidisciplinary design optimization \nof an underwater vehicle including cost analysis. J Mar Sci Technol 2021. https:// \ndoi.org/10.1007/s00773-021-00804-2 . \n[20] Lee KH, Park GJ. Robust optimization considering tolerances of design variables. \nComput Struct 2001;79:77 \u201386. https://doi.org/10.1016/S0045-7949(00)00117-6 . \n[21] Anderson TV, Mattson CA. Propagating skewness and kurtosis through engineering \nmodels for low-cost, meaningful, nondeterministic design. J Mech Des Trans \nASME. 2012. https://doi.org/10.1115/1.4007389 . \n[22] Zhou Q, Wang Y, Choi SK, Jiang P, Shao X, Hu J, Shu L. A robust optimization \napproach based on multi-fidelity metamodel. Struct Multidiscip Optim 2018. \nhttps://doi.org/10.1007/s00158-017-1783-4 . \n[23] Wang GG, Shan S. Review of metamodeling techniques in support of engineering \ndesign optimization. J Mech Des Trans ASME. 2007;129:370 \u201380. https://doi.org/ \n10.1115/1.2429697 . \n[24] Chatterjee T, Chakraborty S, Chowdhury R. A critical review of surrogate assisted \nrobust design optimization. Arch Comput Methods Eng 2019;26:245 \u201374. https:// \ndoi.org/10.1007/s11831-017-9240-5 . \n[25] Chatterjee T, Friswell MI, Adhikari S, Chowdhury R. A global two-layer meta- \nmodel for response statistics in robust design optimization. Eng Optim 2021. \nhttps://doi.org/10.1080/0305215X.2020.1861262 . \n[26] Guo X, Zhao X, Zhang W, Yan J, Sun G. Multi-scale robust design and optimization \nconsidering load uncertainties. Comput Methods Appl Mech Eng 2015;283: \n994\u20131009. https://doi.org/10.1016/j.cma.2014.10.014 . \n[27] Jerez DJ, Jensen HA, Beer M. Reliability-based design optimization of structural \nsystems under stochastic excitation: an overview. Mech Syst Signal Process 2022. \nhttps://doi.org/10.1016/j.ymssp.2021.108397 . \n[28] Li W, Gao L, Xiao M. Multidisciplinary robust design optimization under parameter \nand model uncertainties. Eng Optim 2020;52:426 \u201345. https://doi.org/10.1080/ \n0305215X.2019.1590564 . \n[29] Beyer HG, Sendhoff B. Robust optimization - a comprehensive survey. Comput \nMethods Appl Mech Eng 2007;196:3190 \u2013218. https://doi.org/10.1016/j. \ncma.2007.03.003 . \n[30] Motta R, de S, Afonso SMB. An efficient procedure for structural reliability-based \nrobust design optimization. Struct Multidiscip Optim 2016;54:511 \u201330. https://doi. \norg/10.1007/s00158-016-1418-1 . [31] Yildiz AR. Comparison of evolutionary-based optimization algorithms for \nstructural design optimization. Eng Appl Artif Intell 2013;26:327 \u201333. https://doi. \norg/10.1016/j.engappai.2012.05.014 . \n[32] Beck AT, Gomes WJDS. A comparison of deterministic, reliability-based and risk- \nbased structural optimization under uncertainty. Probab Eng Mech 2012;28:18 \u201329. \nhttps://doi.org/10.1016/j.probengmech.2011.08.007 . \n[33] Acar, E., Bayrak, G., Jung, Y., Lee, I., Ramu, P., Ravichandran, S.S.: Modeling, \nanalysis, and optimization under uncertainties: a review, (2021). 10.1007/s001 \n58-021-03026-7 . \n[34] Georghiou A, Kuhn D, Wiesemann W. The decision rule approach to optimization \nunder uncertainty: methodology and applications. Comput Manag Sci 2019. \nhttps://doi.org/10.1007/s10287-018-0338-5 . \n[35] Braydi O, Lafon P, Younes R. Study of uncertainties and objective function \nmodeling effects on probabilistic optimization results. ASCE ASME J Risk \nUncertain Eng Syst Part B Mech Eng 2019. https://doi.org/10.1115/1.4044152 . \n[36] Liu WS, Cheung SH. Reliability based design optimization with approximate failure \nprobability function in partitioned design space. Reliab Eng Syst Saf 2017;167: \n602\u201311. https://doi.org/10.1016/j.ress.2017.07.007 . \n[37] Chiralaksanakul A, Mahadevan S. First-order approximation methods in reliability- \nbased design optimization. J Mech Des Trans ASME 2005. https://doi.org/ \n10.1115/1.1899691 . \n[38] Doltsinis I, Kang Z, Cheng G. Robust design of non-linear structures using \noptimization methods. Comput Methods Appl Mech Eng 2005;194:1779 \u201395. \nhttps://doi.org/10.1016/j.cma.2004.02.027 . \n[39] Taflanidis AA, Beck JL. Stochastic Subset Optimization for optimal reliability \nproblems. Probab Eng Mech 2008. https://doi.org/10.1016/j. \nprobengmech.2007.12.011 . \n[40] Au SK, Beck JL. Estimation of small failure probabilities in high dimensions by \nsubset simulation. Probab Eng Mech 2001;16:263 \u201377. https://doi.org/10.1016/ \nS0266-8920(01)00019-4 . \n[41] Jia GF, Taflanidis AA. Non-parametric stochastic subset optimization for optimal- \nreliability design problems. Comput Struct 2013;126:86 \u201399. https://doi.org/ \n10.1016/j.compstruc.2012.12.009 . \n[42] Taflanidis AA. Stochastic subset optimization incorporating moving least squares \nresponse surface methodologies for stochastic sampling. Adv Eng Softw 2012;44: \n3\u201314. https://doi.org/10.1016/j.advengsoft.2011.07.009 . \n[43] Khalid MA, Bansal S. Framework for robust design optimization of tuned mass \ndampers by stochastic subset optimization. Int J Struct Stab Dyn 2023;23. https:// \ndoi.org/10.1142/S0219455423501559 . \n[44] Au SK. Reliability-based design sensitivity by efficient simulation. Comput Struct \n2005;83:1048 \u201361. \n[45] Taflanidis AA, Beck JL. An efficient framework for optimal robust stochastic system \ndesign using stochastic simulation. Comput Methods Appl Mech Eng 2008. https:// \ndoi.org/10.1016/j.cma.2008.03.029 . \n[46] Robert CP, Casella G. Monte carlo statistical methods. New York, NY: Springer; \n2004 . \n[47] Li HS. Subset simulation for unconstrained global optimization. Appl Math Model \n2011;35:5108 \u201320. https://doi.org/10.1016/j.apm.2011.04.023 . \n[48] Khalid MA, Bansal S, Ramamohan V. An augmented formulation for robust design \noptimization of structures using stochastic simulation method. Res Eng Des 2022. \nhttps://doi.org/10.1007/s00163-022-00405-z . \n[49] Taflanidis AA, Beck JL. An efficient framework for optimal robust stochastic system \ndesign using stochastic simulation. Comput Methods Appl Mech Eng 2008;198: \n88\u2013101. https://doi.org/10.1016/j.cma.2008.03.029 . \n[50] Kandemir EC, Mortazavi A. Optimization of seismic base isolation system using a \nfuzzy reinforced swarm intelligence. Adv Eng Softw 2022;174:103323. https://doi. \norg/10.1016/j.advengsoft.2022.103323 . \n[51] Rebay S. Efficient unstructured mesh generation by means of delaunay \ntriangulation and Bowyer-Watson algorithm. J Comput Phys 1993;106:125 \u201338. \n[52] Wade N, Graham-Brady L. Estimating microstructural feature distributions from \nimage data using a Bayesian framework. J Microsc 2023:1 \u201316. https://doi.org/ \n10.1111/jmi.13184 . \n[53] Duan X, Li L, Ge Y, Liu B. Exact Voronoi diagram for topographic spatial analysis. \nGIScience Remote Sens 2023;60. https://doi.org/10.1080/ \n15481603.2023.2171703 . \n[54] MATLAB and parallel computing toolbox release. Natick, Massachusetts, United \nStates: The Mathworks, Inc.; 2021 . M.A. Khalid and S. Bansal                                                                                                                                                                                                                   "
]