[
    "Framing the News:\nFrom Human Perception to Large Language Model Inferences\nDavid Alonso del Barrio\nddbarrio@idiap.ch\nIdiap Research Institute\nSwitzerlandDaniel Gatica-Perez\ngatica@idiap.ch\nIdiap Research Institute and EPFL\nSwitzerland\nABSTRACT\nIdentifying the frames of news is important to understand the arti-\ncles\u2019 vision, intention, message to be conveyed, and which aspects\nof the news are emphasized. Framing is a widely studied concept\nin journalism, and has emerged as a new topic in computing, with\nthe potential to automate processes and facilitate the work of jour-\nnalism professionals. In this paper, we study this issue with articles\nrelated to the Covid-19 anti-vaccine movement. First, to under-\nstand the perspectives used to treat this theme, we developed a\nprotocol for human labeling of frames for 1786 headlines of No-\nVax movement articles of European newspapers from 5 countries.\nHeadlines are key units in the written press, and worth of analysis\nas many people only read headlines (or use them to guide their\ndecision for further reading.) Second, considering advances in Nat-\nural Language Processing (NLP) with large language models, we\ninvestigated two approaches for frame inference of news headlines:\nfirst with a GPT-3.5 fine-tuning approach, and second with GPT-\n3.5 prompt-engineering. Our work contributes to the study and\nanalysis of the performance that these models have to facilitate\njournalistic tasks like classification of frames, while understanding\nwhether the models are able to replicate human perception in the\nidentification of these frames.\nCCS CONCEPTS\n\u2022Computing methodologies \u2192Information extraction ;\u2022\nHuman-centered computing \u2192Text input .\nKEYWORDS\nCovid-19 no-vax, news framing, GPT-3, prompt-engineering, trans-\nformers, large language models\nACM Reference Format:\nDavid Alonso del Barrio and Daniel Gatica-Perez. 2023. Framing the News:\nFrom Human Perception to Large Language Model Inferences . In Inter-\nnational Conference on Multimedia Retrieval (ICMR \u201923), June 12\u201315, 2023,\nThessaloniki, Greece. ACM, New York, NY, USA, 9 pages. https://doi.org/10.\n1145/3591106.3592278\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\n\u00a92023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0178-8/23/06. . . $15.00\nhttps://doi.org/10.1145/3591106.35922781 INTRODUCTION\nIn recent years, there has been a proliferation in the use of concepts\nsuch as data journalism, computational journalism, and computer-\nassisted reporting [ 15] [29], which all share the vision of bridging\njournalism and technology. The progress made in NLP has been\ngradually integrated into the journalistic field [ 5][8][54]. More\nspecifically, machine learning models based on transformers have\nbeen integrated in the media sector in different tasks [ 41] such as\nthe creation of headlines with generative languages models [ 17],\nsummarization of news articles [ 28][27], false news detection [ 49],\nand topic modeling and sentiment analysis [ 25]. The development of\nlarge language models such as GPT-3 [ 9], BLOOM [ 51] or ChatGPT\nshow a clear trend towards human-machine interaction becoming\neasier and more intuitive, opening up a wide range of research\npossibilities. At the same time, the use of these models is also\nassociated with a lack of transparency regarding how these models\nwork, but efforts are being made to bring some transparency to\nthese models, and to analyze use cases where they can be useful and\nwhere they cannot [ 35]. Based on the premises that these models\nopen up a wide range of research directions [ 7], and that at the same\ntime (and needless to say) they are not the solution to all problems,\nwe are interested in identifying use cases and tasks where they\ncan be potentially useful, while acknowledging and systematically\ndocumenting their limitations [ 56]. More specifically, the aim of\nthis work is to analyze the performance of GPT-3.5 for a specific\nuse case, namely the analysis of frames in news, from an empirical\npoint of view, with the objective of shedding light on a potential\nuse of generative models in journalistic tasks.\nFrame analysis is a concept from journalism, which consists of\nstudying the way in which news stories are presented on an issue,\nand what aspects are emphasized: Is a merely informative vision\ngiven in an article? Or is it intended to leave a moral lesson? Is\na news article being presented from an economic point of view?\nOr from a more human, emotional angle? The examples above\ncorrespond to different frames with which an article can be written.\nThe concept of news framing has been studied in computing as\na step beyond topic modeling and sentiment analysis, and for this\npurpose, in recent years, pre-trained language models have been\nused for fine-tuning the classification process of these frames [ 60]\n[10], but the emergence of generative models opens the possibility\nof doing prompt-engineering of these classification tasks, instead\nof the fine-tuning approach investigated so far.\nOur work aims to address this research gap by posing the fol-\nlowing research questions:\nRQ1 : What are the main frames in the news headlines about\nthe anti-vaccine movement, as reported in newspapers across 5\nEuropean countries?\n627\n",
    "ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece David Alonso del Barrio and Daniel Gatica-Perez\nRQ2 : Can prompt engineering be used for classification of head-\nlines according to frames?\nBy addressing the above research questions, our work makes the\nfollowing contributions:\nContribution 1. We implemented a process to do human an-\nnotation of the main frame of 1786 headlines of articles about the\nCovid-19 no-vax movement, as reported in 19 newspapers from 5\nEuropean countries (France, Italy, Spain, Switzerland and United\nKingdom.) At the headline level, we found that the predominant\nframe was human interest, where this frame corresponds to a per-\nsonification of an event, either through a statement by a person,\nor the explanation of a specific event that happened to a person.\nFurthermore, we found a large number of headlines annotated as\ncontaining no frame, as they simply present information without\nentering into evaluations. We also found that for all the countries\ninvolved, the distribution of frame types was very similar, i.e., hu-\nman interest and no frame are the two predominant frames. Finally,\nthe generated annotations allowed to subsequently study the per-\nformance of a large language model.\nContribution 2. We studied the performance of GPT-3.5 on\nthe task of frame classification of headlines. In addition to using\nthe fine-tuning approach from previous literature, we propose an\nalternative approach for frame classification that requires no labeled\ndata for training, namely prompt-engineering using GPT-3.5. The\nresults show that fine-tuning with GPT-3.5 produces 72% accuracy\n(slightly higher than other smaller models), and that the prompt-\nengineering approach results in lower performance (49% accuracy.)\nOur analysis also shows that the subjectivity of the human labeling\ntask has an effect on the obtained accufracy.\nThe paper is organized as follows. In Section 2, we discuss related\nwork. In Section 3, we describe the news dataset. In Section 4, we\ndescribe the methodology for both human labeling and machine\nclassification of news frames. We present and discuss results for\nRQ1 and RQ2 in Sections 5 and 6, respectively. Finally, we provide\nconclusions in Section 7.\n2 RELATED WORK\nFraming has been a concept widely studied in journalism, with a\ndefinition that is rooted in the study of this domain [ 23]: \u201cTo frame\nis to select some aspects of a perceived reality and make them more\nsalient in a communicating text, in such a way as to promote a par-\nticular problem definition, causal interpretation, moral evaluation,\nand/or treatment recommendation for the item described.\u201d\nFor frame recognition, there are two main approaches: the induc-\ntive approach [ 16], where one can extract the frames after reading\nthe article, and the deductive approach [ 38], where a predefined\nlist of frames exists and the goal is to interpret if any of them ap-\npears in the article. In the deductive case, there are generic frames\nand subject-specific frames, and the way to detect them typically\ninvolves reading and identifying one frame at a time, or through\nanswers to yes/no questions that represent the frames. Semetko et\nal. [52] used 5 types of generic frames (attribution of responsibility,\nhuman interest, conflict, morality, and economic consequences)\nbased on previous literature, and they defined a list of 20 yes/no\nquestions to detect frames in articles. For instance, the questions\nabout morality are the following: \"Does the story contain any moralmessage? Does the story make reference to morality, God, and other\nreligious tenets? Does the story offer specific social prescriptions\nabout how to behave?\", and so on for each of the frame types. This\ncategorization of frames has been used in various topics such as\nclimate change [ 18] [19], vaccine hesitance [ 13], or immigration\n[34].\nWe now compare the two approaches on a common topic, such\nas Covid-19. Ebrahim et al. [ 21] followed an inductive approach\nin which the frames were not predefined but emerged from the\ntext (e.g., deadly spread, stay home, what if, the cost of Covid-19)\nusing headlines as the unit of analysis. In contrast, the deductive\napproach has studied very different labels. El-Behary et al. [ 22]\nfollowed the method of yes/no questions, but in addition to the\n5 generic frames presented before, they also used blame frame\nand fear frame. Adiprasetio et al. [ 1] and Rodelo [ 50] used the 5\ngeneric frames with yes/no questions, while Catal\u00e1n-Matamoros et\nal. [14] used the 5 frames and read the headline and subheadline\nto decide the main frame. Table 1 summarizes some of the the\nexisting approaches. This previous work showed how frame labels\ncan be different, and also that frame analysis has been done at both\nheadline and article levels. These two approaches (inductive and\ndeductive) that originated in journalism have since been replicated\nin the computing literature.\nWe decided to follow the deductive approach because a prede-\nfined list of frames allows to compare among topics, countries,\nprevious literature, and also because they represent a fixed list of\nlabels for machine classification models. Furthermore, the induc-\ntive approach tends to be more specific to a topic, and from the\ncomputing viewpoint, past work has tried to justify topic modeling\nas a technique to extract frames from articles.\nYl\u00e4-Antitila et al. [ 60] proposed topic modeling as a frame ex-\ntraction technique. They argued that topics can be interpreted as\nframes if three requirements are met: frames are operationalized as\nconnections between concepts; subject-specific data is selected; and\ntopics are adequately validated as frames, for which they suggested\na practical procedure. This approach was based on the choice of a\nspecific topic (e.g., climate change) and the use of Latent Dirichlet\nAllocation (LDA) as a technique to extract a number of subtopics.\nIn a second phase, a qualitative study of the top 10 words of each\nsubtopic was performed, and the different subtopics were elimi-\nnated or grouped, reducing the number and establishing a tentative\ndescription. In a third phase, the top 10 articles belonging to that\nframe/topic were taken, and if the description of the topic fitted\nat least 8 of the 10 articles, that topic/frame remained. The frames\nfound in this article were: green growth, emission cuts, negotiations\nand treaties, environmental risk, cost of carbon emissions, Chinese\nemissions, economics of energy production, climate change, en-\nvironmental activism, North-South burden sharing, state leaders\nnegotiating, and citizen participation.\nFrom Entman\u2019s definition of frame [ 23], it seems that the deduc-\ntive approach is more refined than the inductive approach (which\nseems to resemble the detection of sub-themes.) For example, with\nregard to climate change, there are stories on how people have been\naffected by climate change from an emotional point of view, thus\npersonalizing the problem. In this case, we could categorize the\ncorresponding frame as human interest, as the writer of the article\nis selecting \"some aspects of a perceived reality and make them\n628",
    "Framing the News:\nFrom Human Perception to Large Language Model Inferences ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\nmore salient\". The language subtleties with which news articles are\npresented cannot be captured with basic topic modeling.\nIsoaho et al.[ 30] held the position that while the benefits of\nscale and scope in topic modeling were clear, there were also a\nnumber of problems, namely that topic outputs do not correspond\nto the methodological definition of frames, and thus topic modeling\nremained an incomplete method for frame analysis. Topic modeling,\nin the practice of journalistic research, is a useful technique to deal\nwith the large datasets that are available, yet is often not enough to\ndo more thorough analyses [ 31]. In our work, we clearly notice that\nframe analysis is not topic modeling. For example, two documents\ncould be about the same topic, say Covid-19 vaccination, but one\narticle could emphasize the number of deaths after vaccination,\nwhile the other emphasized the role of the vaccine as a solution to\nthe epidemic.\nWe also consider that the larger the number of possible frame\ntypes, the more likely it is to end up doing topic modeling instead of\nframe analysis. Using a deductive approach, Dallas et al. [ 12] created\na dataset with articles about polemic topics such as immigration,\nsame sex marriage, or smoking, and they defined 15 types of frames:\n\"economic, capacity and resources, morality, fairness and equality,\nlegality, constitutionality and jurisprudence, policy prescription and\nevaluation, crime and punishment, security and defense, health and\nsafety, quality of life, cultural identity, political, external regulation\nand reputation, other\". In this case, they authors did not use a list\nof questions. Instead, for each article, annotators were asked to\nidentify any of the 15 framing dimensions present in the article\nand to label text blurbs that cued them (based on the definitions of\neach of the frame dimensions) and decide the main frame of each\narticle. In our case, we followed the idea of detecting the main frame\nby reading the text instead of answering questions, but instead of\nusing the 15 frames proposed in [ 12] , we used the 5 generic frames\nproposed in [52].\nA final decision in our work was the type of text to analyze,\nwhether headlines or whole article. For this decision, the chosen\nclassification method was also going to be important. For example,\nKhanehzar et al. [ 33] used traditional approaches such as SVMs as\nbaseline, and demonstrated the improvement in frame classifica-\ntion with the use of pre-trained languages models such as BERT,\nRoBERTa and XLNet, following a fine-tuning approach, setting\nas input text a maximum of 256 tokens (although the maximum\nnumber of input tokens in these models is 512 tokens.) Liu et al.\n[37] classified news headlines about the gun problem in the United\nStates, arguing for the choice of headlines as a unit of analysis\nbased on previous journalism literature [ 6], [44], that advocated\nfor the importance and influence of headlines on readers and the\nsubsequent perception of articles. From a computational viewpoint,\nusing headlines is also an advantage, since you avoid the 512 token\nlimitation in BERT-based models. Therefore, we decided to work\nwith headlines about a controversial issue, namely the Covid-19\nno-vax movement.\nContinuing with the question of the methods used for classi-\nfication, much work has been developed in prompt engineering,\nespecially since the release of GPT-3. Liu et al.[ 36] presented a good\noverview of the work done on this new NLP paradigm, not only\nexplaining the concept of prompt engineering, but also the differ-\nent strategies that can be followed both in the design of prompts,Table 1: Summary of deductive approaches for frame analysis\nRef Frames Goal Technique Number of\nsamples\n[12]15 generic frames: \"Economic\", \"Capac-\nity and resources\", \"Morality\", \"Fair-\nness and equality\", \"Legality, constitu-\ntionality and jurisprudence\", \"Policy\nprescription and evaluation\", \"Crime\nand punishment\", \"Security and de-\nfense\", \"Health and safety\", \"Quality of\nlife\", \"Cultural identity\", \"Public opin-\nion\", \"Political\", \"External regulation\nand reputation\", \"Other\".To label frames of full\narticlesReading the\nfull article,\nthe annotator\ndefines the\nmain frame20000 articles\n[33]15 generic frames Classification BERT based\nmodels12000 articles\n[52]5 generic frames: \"human interest\",\n\"conflict\", \"morality\", \"attribution of\nresponsibility\", and \"economic conse-\nquences\".To label frames of full\narticlesYes/No ques-\ntions.2600 articles\nand 1522 tv\nnews stories\n[37]9 specific frames:\u201cPolitics\u201d, \u201cPublic\nopinion\u201d, \u201cSociety/Culture\u201d, and\n\u201cEconomic consequences\u201d , \u201c2nd\nAmendment\u201d (Gun Rights), \u201cGun\ncontrol/regulation\u201d, \u201cMental health\u201d,\n\u201cSchool/Public space safety\u201d, and\n\u201cRace/Ethnicity\u201d.To label frames of full\narticles/ ClassificationReading the\nfull article,\nthe annotator\ndefines the\nmain frame.\nBERT based\nmodels2990 headlines\n[22]5 generic frames + blame frame and\nfear frameTo label frames of full\narticlesYes/No ques-\ntions.1170 articles\n[1]5 generic frames To label frames of full\narticlesReading the\nfull article,\nthe annotator\ndefines the\nmain frame.6713 articles\n[50]5 generic frames + pandemic frames To label frames of full\narticlesYes/No ques-\ntions.2742 articles\n[14]5 generic frames, journalistic role and\npandemic framesTo label frames of full\narticlesReading the\nheadline and\nsubheadline,\nthe annotator\ndefines the\nmain frame.131 headlines +\nsubheadlines\nthe potential applications, and the challenges to face when using\nthis approach. Prompt engineering applications include knowledge\nprobing [ 46], information extraction [ 53], NLP reasoning [ 57], ques-\ntion answering [ 32], text generation [ 20], multi-modal learning [ 58],\nand text classification [ 24], the latter being the prompt-engineering\nuse case in our work. Puri et al.[ 45] presented a very interesting\nidea that we apply to our classification task. This consists of pro-\nviding the language model with natural language descriptions of\nclassification tasks as input, and training it to generate the correct\nanswer in natural language via a language modeling objective. It is\na zero-shot learning approach, in which no examples are used to\nexplain the task to the model. Radford et al. [ 48] demonstrated that\nlanguage models can learn tasks without any explicit supervision.\nWe have followed this approach to find an alternative way to do\nframe analysis.\nAs mentioned before, the emergence of giant models like GPT-3,\nBLOOM, and ChatGPT are a very active research topic. To the best\nof our knowledge, on one hand our work extends the computational\nanalysis of news related to the covid-19 no-vax movement, which\nillustrates the influence of the press on the ways societies think\nabout relevant issues [ 40], [59], and on the other hand it adds to\nthe literature of human-machine interaction, regarding the design\nof GPT-3 prompts for classification tasks [39], [2].\n3 DATA: EUROPEAN COVID-19 NEWS\nDATASET\nWe used part of the European Covid-19 News dataset collected in\nour recent work [ 3]. This dataset contains 51320 articles on Covid-\n19 vaccination from 19 newspapers from 5 different countries: Italy,\n629",
    "ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece David Alonso del Barrio and Daniel Gatica-Perez\nFrance, Spain, Switzerland and UK. The articles cover a time period\nof 22 months, from January 2020 to October 2021. All content was\ntranslated into English to be able to work in a common language.\nThe dataset was used for various analyses, such as name entity\nrecognition, sentiment analysis, and subtopic modeling, to under-\nstand how Covid-19 vaccination was reported in Europe through\nthe print media (in digital format.) The subtopic modeling analysis\nrevealed a subsample of articles on the no-vax movement, which is\nthe one we have used in this paper. We took the headlines of the\narticles associated with the no-vax movement, selecting all articles\ncontaining any of the keywords in Table 2 in the headline or in the\nmain text. This corresponds to a total of 1786 headlines.\nTable 2: Keywords used to identify no-vax articles\nKeywords\nNO VAX TOPIC \"anti-vaxxers\", \"anti-vaccine\", \"anti-vaxx\", \"anti-corona\", \"no-vax\", \"no vax\",\"anti-vaccin\"\nIn Table 3, we show the number of headlines per country and\nnewspaper. France is the country with the most no-vax articles in\nthe corpus, with 523 articles, followed by Italy with 508. However,\nnote that there are 6 newspapers from France, while only 2 from\nItaly. Corriere della Sera is the newspaper that dealt most frequently\nwith the subject (429 articles), while The Telegraph is the second\none (206 articles). The total number of articles normalized by the\nnumber of newspapers per country is also shown in the last column\nof the Table. Using these normalized values, the ranking is Italy,\nUK, France, Switzerland, and Spain.\nTable 3: Number of headlines by newspaper and country\nCOUNTRY NEWSPAPER HEADLINES TOTAL (NORM. TOTAL)\nFRANCE La Croix 94 523 (87.1)\nLe Monde 125\nLes Echos 49\nLiberation 97\nLyon Capitale 8\nOuest France 150\nITALY Corriere della Sera 429 508 (254.0)\nIl Sole 24 Ore 79\nSPAIN 20 minutos 27 303 (50.5)\nABC 50\nEl Diario 32\nEl Mundo 77\nEl Espa\u00f1ol 22\nLa Vanguardia 95\nSWITZERLAND 24 heures 97 230 (76.6)\nLa Libert\u00e9 22\nLe Temps 111\nUNITED KINGDOM The Irish News 16 222 (111.0)\nThe Telegraph 206\n1786\n4 METHODOLOGY\n4.1 Human labeling of news frames\nTo carry out the labeling of the frames in our corpus of headlines, we\nfirst designed a codebook, which contained the definitions of each\nof the frame types and a couple of examples of each type, as well\nas a definition of the corpus subject matter and definitions of the\nconcept of frame analysis, so that the annotators could understand\nthe task to be performed. The codebook follows the proposed by[52] with 5 generic frames (attribution of responsibility, human\ninterest, conflict, morality, and economic consequences) plus one\nadditional \u2019no-frame\u2019 category. Two researchers were engaged to\nannotate a sample of the collected newspaper articles following a\nthree-phase training procedure.\nIn the first phase, annotators had to read the codebook and get\nfamiliar with the task. In the second phase, they were asked to\nidentify the main frame in the same subset of 50 headlines. At the\nend of the second phase, the intercoder reliability (ICR) was 0.58\nbetween the 2 annotators. We analyzed those cases where there\nwere discrepancies, and observed that in some cases, there was not a\nunique main frame, because both annotators had valid arguments to\nselect one of the frames. In other cases, the discrepancies were due\nto slight misunderstanding of the definitions. In the third phase, the\nannotators coded again 50 headlines, and the ICR increased to was\n0.66. We realized that the possibility of having two frames remained.\nThey discussed the cases in which they had disagreed, and if the\nother person\u2019s arguments were considered valid, it could be said that\nthere were two frames. After this three-phase training procedure,\nannotators were ready to annotate the dataset independently. We\ndivided the dataset into two equal parts, and each person annotated\n893 headlines.\n4.2 Fine-tuning GPT-3.5 and BERT-based\nmodels\nWith the annotated dataset, we investigated two NLP approaches:\nthe first one involves fine-tuning a pre-trained model; the second\none is prompt engineering. Pre-trained language models have been\nFigure 1: Pre-train, fine-tune, prompt\ntrained with large text strings based on two unsupervised tasks,\nnext sentence prediction and masked language model. Figure 1\nsummarizes these techniques.\nIn the first approach, a model with a fixed architecture is pre-\ntrained as a language model (LM), predicting the likelihood of the\nobserved textual data. This can be done due to the availability of\nlarge, raw text data needed to train LMs. This learning process can\nproduce general purpose features of the modeled language. The\nlearning process produces robust, general-purpose features of the\nlanguage being modeled. The above pre-trained LM is then adapted\nto different downstream tasks, by introducing additional parameters\nand adjusting them using task-specific objective functions. In this\napproach, the focus was primarily on goal engineering, designing\nthe training targets used in both the pre-training and the fine-tuning\nstages [36].\n630",
    "Framing the News:\nFrom Human Perception to Large Language Model Inferences ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\nWe present an example to illustrate the idea. Imagine that the\ntask is sentiment analysis, and we have a dataset with sentences\nand their associated sentiment, and a pre-trained model, which is a\nsaved neural network trained with a much larger dataset. For that\npre-trained model to address the target task, we unfreeze a few of\nthe top layers of the saved model base and jointly train both the\nnewly-added classifier layers and the last layers of the base model.\nThis allows to \"fine-tune\" the higher-order feature representations\nin the base model to make them more relevant for the sentiment\nanalysis task. In this way, instead of having to obtain a very large\ndataset with target labels to train a model, we can reuse the pre-\ntrained model and use a much smaller train dataset. We use a part\nof our dataset as examples for the model to learn the task, while\nthe other part of the dataset is used to evaluate model performance.\nPrevious works related to frame classification in the computing\nliterature have used fine-tuning, BERT-based models. In our work,\nwe have done the same as a baseline, but we aimed to go one step\nfurther and also produce results using fine-tuning of GPT-3.5.\n4.3 Prompt-engineering with GPT-3.5\nModel fine-tuning has been widely used, but with the emergence\nof generative models such as GPT-3, another way to approach\nclassification tasks has appeared. The idea is to use the pre-trained\nmodel directly and convert the task to be performed into a format\nas close as possible to the tasks for which it has been pre-trained.\nThat is, if the model has been pre-trained from next word prediction\nas in the case of GPT-3, classification can be done by defining a\nprompt, where the input to the model is an incomplete sentence,\nand the model must complete it with a word or several words, just\nas it has been trained. This avoids having to use part of the already\nlabeled dataset to teach the task to be performed to the model, and\na previous labeling is not needed [36].\nIn this approach, instead of adapting pre-trained LMs to down-\nstream tasks via objective engineering, downstream tasks are re-\nformulated to look more like those solved during the original LM\ntraining with the help of a textual prompt. For example, when recog-\nnizing the emotion of a social media post, \u201cI missed the bus today.\u201d,\nwe may continue with a prompt \u201cI felt so _\u201d, and ask the LM to\nfill the blank with an emotion-bearing word. Or if we choose the\nprompt \u201cEnglish: I missed the bus today. French: _\u201d), an LM may\nbe able to fill in the blank with a French translation. In this way,\nby selecting the appropriate prompts, we can influence the model\nbehavior so that the pre-trained LM itself can be used to predict the\ndesired output, even without any additional task-specific training\n[36].\nWe use this emerging NLP approach to classify frames at headline\nlevel. We are not aware of previous uses of this strategy to classify\nframes as we propose here. The idea is the following. Prompt engi-\nneering consists of giving a prompt to the model, and understands\nthat prompt as an incomplete sentence. To do prompt engineer-\ning with our dataset, we needed to define an appropriate prompt\nthat would produce the headline frames as output. We defined sev-\neral experiments with the Playground of GPT-3, in order to find\nthe best prompt for our task. In our initial experiments, we fol-\nlowed existing approaches in prompt engineering to do sentiment\nanalysis, where the individual answer was an adjective, and thisadjective was matched with a sentiment. In a similar fashion, we\ndecided to build a thesaurus of adjectives that define each of the\nframes. For instance, the human interest frame could be \u2019interest-\ning\u2019, \u2019emotional\u2019, \u2019personal\u2019, \u2019human\u2019. The conflict frame could be:\n\u2019conflictive\u2019, \u2019bellicose\u2019, \u2019troublesome\u2019, \u2019rowdy\u2019, \u2019quarrelsome\u2019, \u2019trou-\nblemaker\u2019, \u2019agitator\u2019, etc. After the list of adjectives was defined,\nwe needed to define the prompt in order to get, as an answer, one\nof the adjectives in our thesaurus to match them with the frame.\nWe used the GPT-3 playground using the headline as input and\nasking for the frame as output, but the strategy did not work. In\nour final experiment, instead of giving the headline as input, we\ngave the definitions of each type of frame plus the headline, and we\nasked the model to choose between the different types of frames\nas output. In this way, the output of the model was directly one of\nthe frames, and we avoided the step of matching adjectives with\nframes. An example is shown in Figure 2.\nFigure 2: GPT-3.5 for frame inference: input and output\nFor the GPT-3 configuration1, there are 3 main concepts:\n\u2022TEMPERATURE [0-1]. This parameter controls randomness,\nlowering it results in less random completions.\n\u2022TOP_P [0-1]. This parameter controls diversity via nucleus\nsampling.\n\u2022MAX_TOKENS[1-4000]. This parameter indicates the maxi-\nmum number of tokens to generate,\n\u2022MODEL. GPT-3 offer four main models with different levels\nof power, suitable for different tasks. Davinci is the most\ncapable model, and Ada is the fastest.\nAfter testing with the GPT-3 playground and varying different\nhyper-parameters to assess performance, we set the temperature to\n0, since the higher the temperature the more random the response.\nFurthermore, the Top-p parameter was set to 1, as it would likely\nget a set of the most likely words for the model to choose from. The\nmaximum number of tokens was set to 2; in this way, the model\nis asked to choose between one of the responses. As a model, we\nused the one with the best performance at the time of experimental\ndesign, which was TEXT-DAVINCI-003, recognized as GPT 3.5.\n5 RESULTS: HUMAN LABELING OF FRAMES\nIN NO-VAX NEWS HEADLINES (RQ1)\nIn this section, we present and discuss the results of the analysis\nrelated to our first RQ.\nFigure 3 shows the distribution of frames per country at headline\nlevel, with human interest and no-frame being the predominant\n1https://beta.openai.com/docs/introduction\n631",
    "ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece David Alonso del Barrio and Daniel Gatica-Perez\nones. Attribution of responsibility is the third one except in Switzer-\nland, where the corresponding frame is conflict. Finally, morality\nand economic are the least represented in the dataset for every\ncountry.\nFigure 3: Non-normalized distribution of frames per country\nThe monthly distribution of frames aggregated for all countries\nis shown in Fig. 4. We can see two big peaks, the first one in January\n2021 and the second one in August 2021. In all countries, the vac-\ncination process started at the end of December 2020, so it makes\nsense that the no-vax movement started to be more predominant in\nthe news in January 2021. Human interest is the most predominant\nframe. Manual inspection shows that this is because the headlines\nare about personal cases of people who are pro- or anti- vaccine.\nAttribution of responsibility is also present. Manual inspection in-\ndicates that local politicians and health authorities had to make\ndecisions about who could be vaccinated at the beginning of the\nprocess. The second peak at the end of summer 2021 coincided\nwith the health pass (also called Covid passport in some countries),\nand we can observe a peak in the curve corresponding to the con-\nflict frame, reflecting the demonstrations against the measure of\nmandatory health passes taken by country governments.\nIn Figure 5, we compare the sentiment per frame and per country,\nto understand if there were any major differences. The sentiment\nanalysis labels were obtained using BERT-sent from the Hugging\nFace package [ 47], used in our previous work (please refer to our\noriginal analysis in [ 3] for details.) We normalized the results be-\ntween 0 and 1 to compare frames between countries. We see that the\nsentiment is predominantly neutral (in blue). Examining in more\nFigure 4: Non-normalized monthly distribution of frames.\ndetail the negative and positive sentiment of each frame category,\nwe observed a few trends:\n\u2022Attribution of responsibility: Negative sentiment represents\n30-40% of the cases, while positive tone is only found in\nresidual form in Italy, Switzerland, and the United Kingdom.\n\u2022Conflict: Negative sentiment represents 20-35% of the cases.\n\u2022Economic: Predominantly neutral, with only negative tone\nin Italy and UK (in the latter case, all headlines with this\nframe were considered negative.)\n\u2022Human interest: Negative sentiment represents 30-40% of\nthe cases, while positive tone is only found in residual form\nin Italy, Spain, and Switzerland.\n\u2022Morality: Predominantly neutral, with negative tone in Italy,\nSwitzerland, and the United Kingdom,\n\u2022No frame: 20-30% of negative content.\nFigure 5: Sentiment of headline by frame and by country\n632",
    "Framing the News:\nFrom Human Perception to Large Language Model Inferences ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\nRegarding the results of the annotation process, the fact that the\ndistribution of the 6 frame types is relatively similar between coun-\ntries suggests that the anti-vaccine movement issue was treated\nin a similar way in these countries. The fact that human interest\nis the most dominant frame indicates that this issue was treated\nfrom a more human and emotional approach, with headlines about\npersonal experiences, celebrities giving their opinion about vacci-\nnation, and politicians defending vaccine policies. Moreover, the\nreason for many headlines being classified as no-frame is partly\ndue to how data was selected. We chose articles that contained\nwords related to no-vax, either in the headline or in the article. This\nresulted in many headlines not containing anything specific related\nto no-vax, while the no-vax content was actually included in the\nmain text of the corresponding articles.\nIt is worth mentioning that prior to obtaining the results, we had\nexpected that attribution of responsibility would be among the most\nprominent frames, since governments took many measures such as\nmandatory health pass requirements to access certain sites; we had\nalso expected that the conflict frame would be prominent, since\nthere were many demonstrations in Europe. In reality, however,\nthese frames categories were not reflected as frequently at the\nheadline level.\nRegarding the analysis at the temporal level, it is clear that certain\nevents were captured by the press, such as the start of vaccination\nor the mandatory vaccination passport.\nFinally, the sentiment analysis of the different frames shows that\nthe predominant tone in all of them is neutral or negative, with very\nsimilar trends between countries. This association between senti-\nment analysis and frames has been discussed in previous literature\n[11] [43].\n6 RESULTS: GPT-3.5 FOR FRAME\nCLASSIFICATION OF HEADLINES (RQ2)\nHere, we present and discuss the results related to our second RQ.\n6.1 Fine-tuning GPT-3.5\nTable 4 shows the results of the 6-class classification task using\n5-cross validation. Three models were used: GPT-3.5 and two BERT-\nbased models. We observe that, on average, GPT-3.5 performs better\nthan the BERT-based models. This is somehow expected as GPT-\n3.5 is a much larger model. Overall, in the case of fine-tuning, the\nbest performance for the six-class frame classification task is 72%\naccuracy, which is promising, with an improvement over previous\nmodels based on BERT. Yet, it should be noted that the performance\ndifferences are modest (2% improvement between GPT-3.5 and\nRoBERTa).\nTable 4: Classification results for six-class frame classifica-\ntion and 5-fold cross validation\nACCURACY 0 1 2 3 4 AVERAGE\nBERT 0.68 0.69 0.72 0.64 0.70 0.67\nRoBERTa 0.70 0.72 0.72 0.67 0.71 0.70\nGPT3 0.75 0.70 0.72 0.71 0.71 0.72On the other hand, BERT is open-source, while GPT-3 has an\neconomic cost as the use of the model is not free, which monetarily\nlimits the number of experiments that can be performed with it,\nas well as the different configurations one can explore to improve\nperformance. This is important because much of the improvement\nin performance requires empirical explorations of model parameters\nMore specifically, the cost of an experiment for each of the folds has\na cost of 4 dollars (at the time of writing this paper.) This represents\na limitation in practice.\nFurthermore, GPT-3 has a significant carbon footprint. Similarly,\nfor prompt engineering (discussed in the next subsection), choosing\nthe right prompt (i.e., the words that best define the task so that the\nmodel is able to perform adequately) is also based on trial and error.\nThis also has an impact on carbon footprint. In connection with\nthis topic, Strubell et al.[ 55] argue that improvements in the accu-\nracy of models depend on the availability of large computational\nresources, which involve large economic and environmental costs.\nA criticism has been made as \u2019the rich get richer\u2019, in the sense that\nnot all research groups have sufficient infrastructure resources and\naccess to funding needed to use these models and improve their\nperformance. Also in relation to this analysis, the work of Bender\net al. [ 4] evaluates the costs and risks of the use of large language\nmodels, stating that researchers should be aware of the impact that\nthese models have on the environment, and assess whether the\nbenefits outweigh the risks. The work in [ 4] provides a very telling\nexample, where people living in the Maldives or Sudan are affected\nby floods and pay the environmental price of training English LLMs,\nwhen similar models have not been produced for languages like\nDhivehi or Sudanese Arab. In short, there is a need to establish\nways to use this technological development responsibly, and it all\nstarts with being aware of the risks it presents.\n6.2 Prompt-engineering with GPT-3.5\nFor each headline, we got the frame that the model considered the\nmost likely, and we compared these GPT-3.5 inferences with the\nframes labeled by the annotators. The agreement between model\nand annotator was of 49%. Analyzing the results, and specifically\nlooking at the cases where the annotator and GPT-3.5 disagreed,\nwe discovered that according to the frame definitions, the model\nin some cases proposed a frame that indeed made sense. This ob-\nservation, together with our previous experience in the annotation\nprocess, where headlines could have more than one valid frame,\nled us to design a second post-hoc experiment. We took all the\nheadlines where each of the two annotators had disagreed with\nGPT-3.5, and we asked the annotators to state whether they would\nagree (or not) with each GPT-inferred label for a given headline.\nIt is important to emphasize that the annotators did not know the\norigin of that label, i.e., they did not know if it was the label they\nhad originally assigned, or if it was a random one. In this way, we\ncould quantify how GPT-3.5 worked according to valid arguments\nprovided by the annotators. In this post-hoc experiment, the model\nagreed in 76% of cases with the annotators.\nLooking at the results of the classification models, the 49% accu-\nracy of the prompt-engineering approach can be considered low,\nyet we consider that it is a valid avenue for further investigation,\nas in the second post-hoc analysis, we found that the model agrees\n633",
    "ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece David Alonso del Barrio and Daniel Gatica-Perez\nwith human annotators in 76% of the cases. Clearly, framing in-\nvolves aspects of subjectivity [ 42]. Much of what we do as people\nhas a subjective component, influenced by how we feel or how we\nexpress opinions.\nNews reading is never fully objective, and the annotators en-\ngaged in the frame classification task, influenced by their personal\nstate of mind, experience, and culture, may perceive information\ndifferently. Monarch affirms that \"for simple tasks, like binary labels\non objective tasks, the statistics are fairly straightforward to decide\nwhich is the \u2018correct\u2019 label when different annotators disagree. But\nfor subjective tasks, or even objective tasks with continuous data,\nthere are no simple heuristics for deciding what the correct label\nshould be\" [42].\nSubjectivity is involved in both the generation and perception\nof information: the assumption that there is only one frame is com-\nplicated by the point of view of the reader. In the case of news, the\ninformation sender (the journalist) has an intention, but the receiver\n(the reader) plays a role and is influenced by it. In psychology, this\nis known as the lens model of interpersonal communication, where\nthe sender has certain objectives, but the receiver can interpret\nor re-interpret what the sender wants to say, with more or less\naccuracy [26].\nFollowing this discussion on subjectivity, the question arose as to\nwhat would happen if, instead of headlines, we used the complete\narticle as a source of analysis. We wondered if longer text could\nmake the frame labeling task clearer than when using headlines.\nYet another possible hypothesis is that having to read longer texts\ncould lead to the same subject being presented from different angles.\nPlease recall that in the existing literature discussed in Section 2,\nboth headlines and full articles have been used from frame analysis\n(see Table 1.) This remains as an issue for future work.\n7 CONCLUSIONS\nIn this paper, we first presented an analysis of human-generated\nnews frames on the covid-19 no-vax movement in Europe, and\nthen studied different approaches using large language models for\nautomatic inference of frames. We conclude by answering the two\nresearch questions we posed:\nRQ1: What are the main frames in the news headlines about the\ncovid-19 anti-vaccine movement in 5 European countries? After\nannotating the headlines, we found that of the 1786 headlines,\nthe predominant frame is human interest (45.3% of cases), which\npresents a news item with an emotional angle, putting a face to a\nproblem or situation. We also found that a substantial proportion\nof headlines were annotated as not presenting any frame (40.2% of\ncases). Finally, the other frame types are found more infrequently.\nRQ2: Can prompt engineering be used for classification of head-\nlines according to frames? We first used fine-tuning of a number of\nlanguage models, and found that GPT-3.5 produced classification ac-\ncuracy of 72% on a six-frame classification task. This represented a\nmodest 2% improvement over BERT-based models, at a significantly\nlarger environmental cost. We then presented a new way of classi-\nfying frames using prompts. At the headline level, inferences made\nwith GPT-3.5 reached 49% of agreement with human-generated\nframe labels. In many cases, the GPT-3.5 model inferred frame\ntypes that were considered as valid choices by human annotators,and in an post-doc experiment, the human-machine agreement\nreached 76%. These results have opened several new directions for\nfuture work.\nACKNOWLEDGMENTS\nThis work was supported by the AI4Media project, funded by the\nEuropean Commission (Grant 951911) under the H2020 Programme\nICT-48-2020. We also thank the newspapers for sharing their online\narticles. Finally, we thank our colleagues Haeeun Kim and Emma\nBouton-Bessac for their support with annotations, and Victor Bros\nand Oleksii Polegkyi for discussions.\nREFERENCES\n[1]Justito Adiprasetio and Annissa Winda Larasati. 2020. Pandemic crisis in online\nmedia: Quantitative framing analysis on Detik. com\u2019s coverage of Covid-19.\nJurnal Ilmu Sosial Dan Ilmu Politik 24, 2 (2020), 153\u2013170.\n[2]Neel Alex, Eli Lifland, Lewis Tunstall, Abhishek Thakur, Pegah Maham, C Jess\nRiedel, Emmie Hine, Carolyn Ashurst, Paul Sedille, Alexis Carlier, et al .2021.\nRAFT: A real-world few-shot text classification benchmark. arXiv preprint\narXiv:2109.14076 (2021).\n[3]David Alonso del Barrio and Daniel Gatica-Perez. 2022. How Did Europe\u2019s Press\nCover Covid-19 Vaccination News? A Five-Country Analysis. (2022), 35\u201343.\nhttps://doi.org/10.1145/3512732.3533588\n[4]Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret\nShmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models\nBe Too Big? (2021), 610\u2013623.\n[5]Santosh Kumar Biswal and Nikhil Kumar Gouda. 2020. Artificial intelligence in\njournalism: A boon or bane? In Optimization in machine learning and applications .\nSpringer, 155\u2013167.\n[6]Erik Bleich, Hannah Stonebraker, Hasher Nisar, and Rana Abdelhamid. 2015.\nMedia portrayals of minorities: Muslims in British newspaper headlines, 2001\u2013\n2012. Journal of Ethnic and Migration Studies 41, 6 (2015), 942\u2013962.\n[7]Michael Bommarito and Daniel Martin Katz. 2022. GPT Takes the Bar Exam.\nhttps://doi.org/10.48550/ARXIV.2212.14402\n[8]Meredith Broussard, Nicholas Diakopoulos, Andrea L Guzman, Rediet Abebe,\nMichel Dupagne, and Ching-Hua Chuan. 2019. Artificial intelligence and jour-\nnalism. Journalism & Mass Communication Quarterly 96, 3 (2019), 673\u2013695.\n[9]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al .2020. Language models are few-shot learners. Advances in neural\ninformation processing systems 33 (2020), 1877\u20131901.\n[10] Bj\u00f6rn Burscher, Daan Odijk, Rens Vliegenthart, Maarten De Rijke, and Claes H\nDe Vreese. 2014. Teaching the computer to code frames in news: Comparing\ntwo supervised machine learning approaches to frame analysis. Communication\nMethods and Measures 8, 3 (2014), 190\u2013206.\n[11] Bjorn Burscher, Rens Vliegenthart, and Claes H de Vreese. 2016. Frames beyond\nwords: Applying cluster and sentiment analysis to news coverage of the nuclear\npower issue. Social Science Computer Review 34, 5 (2016), 530\u2013545.\n[12] Dallas Card, Amber Boydstun, Justin Gross, Philip Resnik, and Noah Smith. 2015.\nThe Media Frames Corpus: Annotations of Frames Across Issues. 2 (01 2015),\n438\u2013444. https://doi.org/10.3115/v1/P15-2072\n[13] Daniel Catalan-Matamoros and Carlos El\u00edas. 2020. Vaccine hesitancy in the age\nof coronavirus and fake news: analysis of journalistic sources in the Spanish\nquality press. International Journal of Environmental Research and Public Health\n17, 21 (2020), 8136.\n[14] Daniel Catal\u00e1n-Matamoros and Carmen Pe\u00f1afiel-Saiz. 2019. Media and mistrust\nof vaccines: a content analysis of press headlines. Revista latina de comunicaci\u00f3n\nsocial 74 (2019), 786\u2013802.\n[15] Mark Coddington. 2015. Clarifying journalism\u2019s quantitative turn: A typology\nfor evaluating data journalism, computational journalism, and computer-assisted\nreporting. Digital journalism 3, 3 (2015), 331\u2013348.\n[16] Stephen D Cooper. 2010. The oppositional framing of bloggers. In Doing News\nFraming Analysis . Routledge, 151\u2013172.\n[17] Robert Dale. 2021. GPT-3: What\u2019s it good for? Natural Language Engineering 27,\n1 (2021), 113\u2013118.\n[18] Astrid Dirikx and Dave Gelders. 2010. To frame is to explain: A de-\nductive frame-analysis of Dutch and French climate change coverage dur-\ning the annual UN Conferences of the Parties. Public Understanding of\nScience 19, 6 (2010), 732\u2013742. https://doi.org/10.1177/0963662509352044\narXiv:https://doi.org/10.1177/0963662509352044 PMID: 21560546.\n[19] Astrid Dirikx and Dave Gelders. 2010. To frame is to explain: A deductive frame-\nanalysis of Dutch and French climate change coverage during the annual UN\nConferences of the Parties. Public understanding of science 19, 6 (2010), 732\u2013742.\n634",
    "Framing the News:\nFrom Human Perception to Large Language Model Inferences ICMR \u201923, June 12\u201315, 2023, Thessaloniki, Greece\n[20] Zi-Yi Dou, Pengfei Liu, Hiroaki Hayashi, Zhengbao Jiang, and Graham Neubig.\n2020. Gsum: A general framework for guided neural abstractive summarization.\narXiv preprint arXiv:2010.08014 (2020).\n[21] Sumayya Ebrahim. 2022. The corona chronicles: Framing analysis of online news\nheadlines of the COVID-19 pandemic in Italy, USA and South Africa. Health SA\nGesondheid (Online) 27 (2022), 1\u20138.\n[22] Hend Abdelgaber Ahmed El-Behary. 2021. A Feverish Spring: A Comparative\nAnalysis of COVID-19 News Framing in Sweden, the UK, and Egypt. (2021).\n[23] Robert M Entman. 1993. Framing: Towards clarification of a fractured paradigm.\nMcQuail\u2019s reader in mass communication theory 390 (1993), 397.\n[24] Tianyu Gao, Adam Fisch, and Danqi Chen. 2020. Making pre-trained language\nmodels better few-shot learners. arXiv preprint arXiv:2012.15723 (2020).\n[25] Piyush Ghasiya and Koji Okamura. 2021. Investigating COVID-19 news across\nfour nations: a topic modeling and sentiment analysis approach. Ieee Access 9\n(2021), 36645\u201336656.\n[26] Robert Gifford. 1994. A Lens-Mapping Framework for Understanding the En-\ncoding and Decoding of Interpersonal Dispositions in Nonverbal Behavior.\nJournal of Personality and Social Psychology 66 (02 1994), 398\u2013412. https:\n//doi.org/10.1037//0022-3514.66.2.398\n[27] Quentin Grail, Julien Perez, and Eric Gaussier. 2021. Globalizing BERT-based\ntransformer architectures for long document summarization. In Proceedings of\nthe 16th Conference of the European Chapter of the Association for Computational\nLinguistics: Main Volume . 1792\u20131810.\n[28] Anushka Gupta, Diksha Chugh, Rahul Katarya, et al .2022. Automated news\nsummarization using transformers. In Sustainable Advanced Computing . Springer,\n249\u2013259.\n[29] Alfred Hermida and Mary Lynn Young. 2017. Finding the data unicorn: A hierar-\nchy of hybridity in data and computational journalism. Digital Journalism 5, 2\n(2017), 159\u2013176.\n[30] Karoliina Isoaho, Daria Gritsenko, and Eetu M\u00e4kel\u00e4. 2021. Topic modeling and\ntext analysis for qualitative policy research. Policy Studies Journal 49, 1 (2021),\n300\u2013324.\n[31] Carina Jacobi, Wouter Van Atteveldt, and Kasper Welbers. 2016. Quantitative\nanalysis of large amounts of journalistic texts using topic modelling. Digital\njournalism 4, 1 (2016), 89\u2013106.\n[32] Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020. How can we\nknow what language models know? Transactions of the Association for Computa-\ntional Linguistics 8 (2020), 423\u2013438.\n[33] Shima Khanehzar, Andrew Turpin, and Gosia Miko\u0142ajczak. 2019. Modeling\nPolitical Framing Across Policy Issues and Contexts. In ALTA .\n[34] Jeesun Kim and Wayne Wanta. 2018. News framing of the US immigration debate\nduring election years: Focus on generic frames. The Communication Review 21, 2\n(2018), 89\u2013115.\n[35] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michi-\nhiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al .\n2022. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110\n(2022).\n[36] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and\nGraham Neubig. 2021. Pre-train, prompt, and predict: A systematic survey of\nprompting methods in natural language processing. (2021). https://doi.org/10.\n48550/ARXIV.2107.13586\n[37] Siyi Liu, Lei Guo, Kate Mays, Margrit Betke, and Derry Tanti Wijaya. 2019.\nDetecting frames in news headlines and its application to analyzing news framing\ntrends surrounding US gun violence. In Proceedings of the 23rd conference on\ncomputational natural language learning (CoNLL) .\n[38] J\u00f6rg Matthes and Matthias Kohring. 2008. The Content Analysis of Media Frames:\nToward Improving Reliability and Validity. Journal of Communication 58 (06\n2008). https://doi.org/10.1111/j.1460-2466.2008.00384.x\n[39] Selina Meyer, David Elsweiler, Bernd Ludwig, Marcos Fernandez-Pichel, and\nDavid E Losada. 2022. Do We Still Need Human Assessors? Prompt-Based GPT-3\nUser Simulation in Conversational AI. In Proceedings of the 4th Conference on\nConversational User Interfaces . 1\u20136.\n[40] Stuart E Middleton, Symeon Papadopoulos, and Yiannis Kompatsiaris. 2018. Social\ncomputing for verifying social media content in breaking news. IEEE Internet\nComputing 22, 2 (2018), 83\u201389.\n[41] Marko Milosavljevi\u0107 and Igor Vobi\u010d. 2021. \u2018Our task is to demystify fears\u2019:\nAnalysing newsroom management of automation in journalism. Journalism 22,\n9 (2021), 2203\u20132221.\n[42] R. Monarch. 2021. Human-in-the-Loop Machine Learning: Active Learning and\nAnnotation for Human-centered AI . Manning. https://books.google.ch/books?\nid=LCh0zQEACAAJ\n[43] Tom Nicholls and Pepper D Culpepper. 2021. Computational identification of\nmedia frames: Strengths, weaknesses, and opportunities. Political Communication\n38, 1-2 (2021), 159\u2013181.\n[44] Zhongdang Pan and Gerald M Kosicki. 1993. Framing analysis: An approach to\nnews discourse. Political communication 10, 1 (1993), 55\u201375.\n[45] Raul Puri and Bryan Catanzaro. 2019. Zero-shot text classification with generative\nlanguage models. arXiv preprint arXiv:1912.10165 (2019).[46] Guanghui Qin and Jason Eisner. 2021. Learning how to ask: Querying lms with\nmixtures of soft prompts. arXiv preprint arXiv:2104.06599 (2021).\n[47] Rabindra Lamsal. 2021. Sentiment Analysis of English Tweets with BERTsent.\nhttps://huggingface.co/rabindralamsal/finetuned-bertweet-sentiment-analysis.\n[48] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,\net al.2019. Language models are unsupervised multitask learners. OpenAI blog\n1, 8 (2019), 9.\n[49] Nishant Rai, Deepika Kumar, Naman Kaushik, Chandan Raj, and Ahad Ali. 2022.\nFake News Classification using transformer based enhanced LSTM and BERT.\nInternational Journal of Cognitive Computing in Engineering 3 (2022), 98\u2013105.\nhttps://doi.org/10.1016/j.ijcce.2022.03.003\n[50] Frida V Rodelo. 2021. Framing of the Covid-19 pandemic and its organizational\npredictors. Cuadernos. info 50 (2021), 91\u2013112.\n[51] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel\nHesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias\nGall\u00e9, et al .2022. Bloom: A 176b-parameter open-access multilingual language\nmodel. arXiv preprint arXiv:2211.05100 (2022).\n[52] Holli Semetko and Patti Valkenburg. 2000. Framing European Politics: A Content\nAnalysis of Press and Television News. Journal of Communication 50 (06 2000),\n93 \u2013 109. https://doi.org/10.1111/j.1460-2466.2000.tb02843.x\n[53] Richard Shin, Christopher H Lin, Sam Thomson, Charles Chen, Subhro Roy, Em-\nmanouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, and Benjamin\nVan Durme. 2021. Constrained language models yield few-shot semantic parsers.\narXiv preprint arXiv:2104.08768 (2021).\n[54] Efstathios Sidiropoulos and Andreas Veglis. 2017. Computer Supported Collab-\norative Work trends on Media Organizations: Mixing Qualitative and Quan-\ntitative Approaches. Studies in Media and Communication 5 (04 2017), 63.\nhttps://doi.org/10.11114/smc.v5i1.2279\n[55] Emma Strubell, Ananya Ganesh, and Andrew McCallum. 2019. Energy and policy\nconsiderations for deep learning in NLP. arXiv preprint arXiv:1906.02243 (2019).\n[56] Alex Tamkin, Miles Brundage, Jack Clark, and Deep Ganguli. 2021. Understanding\nthe capabilities, limitations, and societal impact of large language models. arXiv\npreprint arXiv:2102.02503 (2021).\n[57] Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning.\narXiv preprint arXiv:1806.02847 (2018).\n[58] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and\nFelix Hill. 2021. Multimodal few-shot learning with frozen language models.\nAdvances in Neural Information Processing Systems 34 (2021), 200\u2013212.\n[59] Sandra A Vannoy and Prashant Palvia. 2010. The social influence model of\ntechnology adoption. Commun. ACM 53, 6 (2010), 149\u2013153.\n[60] Tuukka Yl\u00e4-Anttila, Veikko Eranti, and Anna Kukkonen. 2022. Topic modeling\nfor frame analysis: A study of media debates on climate change in India and USA.\nGlobal Media and Communication 18, 1 (2022), 91\u2013112.\n635"
]