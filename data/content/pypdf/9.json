[
    "Generating Diverse Code Explanations\nusing the GPT-3 Large Language Model\nStephen MacNeil\nstephen.macneil@temple.edu\nTemple University\nPhiladelphia, PA, USAAndrew Tran\nandrew.tran10@temple.edu\nTemple University\nPhiladelphia, PA, USADan Mogil\ndaniel.mogil@temple.edu\nTemple University\nPhiladelphia, PA, USA\nSeth Bernstein\nseth.bernstein@temple.edu\nTemple University\nPhiladelphia, PA, USAErin Ross\nerinross@temple.edu\nTemple University\nPhiladelphia, PA, USAZiheng Huang\nz8huang@ucsd.edu\nUniversity of California\u2014San Diego\nLa Jolla, CA, USA\nKEYWORDS\nlarge language models, natural language processing, code explana-\ntions, computer science education\nACM Reference Format:\nStephen MacNeil, Andrew Tran, Dan Mogil, Seth Bernstein, Erin Ross,\nand Ziheng Huang. 2022. Generating Diverse Code Explanations using the\nGPT-3 Large Language Model. In Proceedings of the 2022 ACM Conference\non International Computing Education Research V.2 (ICER 2022), August 7\u201311,\n2022, Lugano and Virtual Event, Switzerland. ACM, New York, NY, USA,\n3 pages. https://doi.org/10.1145/3501709.3544280\n1 ABSTRACT\nGood explanations are essential to efficiently learning introductory\nprogramming concepts [ 10]. To provide high-quality explanations\nat scale, numerous systems automate the process by tracing the\nexecution of code [ 8,12], defining terms [ 9], giving hints [ 16],\nand providing error-specific feedback [ 10,16]. However, these ap-\nproaches often require manual effort to configure and only explain\na single aspect of a given code segment. Large language models\n(LLMs) are also changing how students interact with code [ 7]. For\nexample, Github\u2019s Copilot can generate code for programmers [ 4],\nleading researchers to raise concerns about cheating [ 7]. Instead,\nour work focuses on LLMs\u2019 potential to support learning by explain-\ning numerous aspects of a given code snippet. This poster features\na systematic analysis of the diverse natural language explanations\nthat GPT-3 can generate automatically for a given code snippet. We\npresent a subset of three use cases from our evolving design space\nofAI Explanations of Code .\n2 USE CASES\nTo understand the types of explanations GPT-3 [ 2] can generate,\nwe issued over 700 prompts across numerous code snippets. An\nexample prompt and resulting explanation is shown in Figure 1.\nWe discovered eight explanation types and Figure 2 includes three\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nICER 2022, August 7\u201311, 2022, Lugano and Virtual Event, Switzerland\n\u00a92022 Copyright held by the owner/author(s).\nACM ISBN 978-1-4503-9195-5/22/08.\nhttps://doi.org/10.1145/3501709.3544280explanation types to illustrate the explanatory power of GPT-3. The\nadditional types include: 1) tracing the execution of code, 2) fixing\nbugs and explaining how they were fixed, 3) generating analogies\nto real world settings, 4) listing relevant programming concepts,\nand 5) predicting the console output.\nFigure 1: A prompt and explanation based on analogy.\n2.1 Analyzing and explaining time complexity\nInstructors rate time complexity as the most difficult programming\ntopic [ 17]. However, understanding time complexity is important [ 6,\n13] because it facilitates decision-making so students choose an\nappropriate algorithm for a given problem. This use case shows\nGPT-3 can identify and explain time complexity.\n2.2 Identifying common mistakes made by\nbeginner programmers\nCommonality exists in how students solve programming prob-\nlems [ 15] and the mistakes they make [ 1,11]. Pedagogical tech-\nniques, such as the \u2018muddiest point\u2019 highlight these common and\nmost confusing concepts [ 3,14]. GPT-3 can automatically create\na checklist of common mistakes students might make regarding a\ngiven code snippet.\n37\n",
    "ICER 2022, August 7\u201311, 2022, Lugano and Virtual Event, Switzerland MacNeil et al.\nFigure 2: Three example explanations automatically generated by GPT-3 for an \u2018anonymized\u2019 Binary Search code snippet.\n2.3 Summarizing code at multiple levels of\nabstraction\nBefore understanding how a code snippet executes, it is often useful\nto understand the purpose of the code [ 5]. The summary gener-\nated by GPT-3 and shown in Figure 2 defines the goal, traces the\nexecution, and highlights relevant CS concepts such as arrays.\n3 DISCUSSION\nOur three use cases demonstrate the potential for GPT-3 to explain\ncode for intro CS students. Our poster presentation will feature all\neight explanation types as a design space of explanations to convey\nthe diversity of explanations that can be generated by LLMs. We will\nhighlight best practices for generating effective explanations and\npitfalls that lead to less effective explanations. We are evaluating\nthe usefulness of these explanations in a series of summer classes.\nREFERENCES\n[1]Amjad Altadmri and Neil CC Brown. 2015. 37 million compilations: Investigating\nnovice programming mistakes in large-scale student data. In Proceedings of the\n46th ACM Technical Symposium on Computer Science Education . 522\u2013527.\n[2]Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al .2020. Language models are few-shot learners. Advances in Neural\nInformation Processing Systems 33 (2020), 1877\u20131901.\n[3]Adam Carberry, Stephen Krause, Casey Ankeny, and Cynthia Waters. 2013.\n\u201cUnmuddying\u201d course content using muddiest point reflections. In 2013 IEEE\nFrontiers in Education Conference (FIE) . IEEE, 937\u2013942.\n[4]Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira\nPinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman,\net al.2021. Evaluating large language models trained on code. arXiv preprint\narXiv:2107.03374 (2021).[5]Kathryn Cunningham, Yike Qiao, Alex Feng, and Eleanor O\u2019Rourke. 2022. Bring-\ning \"High-Level\" Down to Earth: Gaining Clarity in Conversational Program-\nmer Learning Goals. In Proceedings of the 53rd ACM Technical Symposium\non Computer Science Education V. 1 (Providence, RI, USA) (SIGCSE 2022) . As-\nsociation for Computing Machinery, New York, NY, USA, 551\u2013557. https:\n//doi.org/10.1145/3478431.3499370\n[6]Elvina Elvina and Oscar Karnalim. 2017. Complexitor: An educational tool for\nlearning algorithm time complexity in practical manner. ComTech: Computer,\nMathematics and Engineering Applications 8, 1 (2017), 21\u201327.\n[7]James Finnie-Ansley, Paul Denny, Brett A. Becker, Andrew Luxton-Reilly, and\nJames Prather. 2022. The Robots Are Coming: Exploring the Implications of Ope-\nnAI Codex on Introductory Programming. In Australasian Computing Education\nConference (Virtual Event, Australia) (ACE \u201922) . ACM, New York, NY, USA, 10\u201319.\nhttps://doi.org/10.1145/3511861.3511863\n[8]Philip J Guo. 2013. Online python tutor: embeddable web-based program visual-\nization for cs education. In Proceeding of the 44th ACM technical symposium on\nComputer science education . 579\u2013584.\n[9]Andrew Head, Codanda Appachu, Marti A Hearst, and Bj\u00f6rn Hartmann. 2015.\nTutorons: Generating context-relevant, on-demand explanations and demonstra-\ntions of online code. In 2015 IEEE Symposium on Visual Languages and Human-\nCentric Computing (VL/HCC) . IEEE, 3\u201312.\n[10] Samiha Marwan, Ge Gao, Susan Fisk, Thomas W. Price, and Tiffany Barnes. 2020.\nAdaptive Immediate Feedback Can Improve Novice Programming Engagement\nand Intention to Persist in Computer Science. In Proceedings of the 2020 ACM\nConference on International Computing Education Research (Virtual Event, New\nZealand) (ICER \u201920) . Association for Computing Machinery, New York, NY, USA,\n194\u2013203. https://doi.org/10.1145/3372782.3406264\n[11] Davin McCall and Michael K\u00f6lling. 2014. Meaningful categorisation of novice pro-\ngrammer errors. In 2014 IEEE Frontiers in Education Conference (FIE) Proceedings .\nIEEE, 1\u20138.\n[12] Greg L Nelson, Benjamin Xie, and Amy J Ko. 2017. Comprehension first: eval-\nuating a novel pedagogy and tutoring system for program tracing in CS1. In\nProceedings of the 2017 ACM conference on international computing education\nresearch . 2\u201311.\n[13] Miranda Parker and Colleen Lewis. 2014. What makes big-O analysis difficult:\nunderstanding how students understand runtime analysis. Journal of Computing\nSciences in Colleges 29, 4 (2014), 164\u2013174.\n[14] Daniel Perez, Leila Zahedi, Monique Ross, Jia Zhu, Tiffany Vinci-Cannava, Laird\nKramer, and Maria Charters. 2020. WIP: An exploration into the muddiest points\n38",
    "Generating Diverse Explanations with Large Language Models ICER 2022, August 7\u201311, 2022, Lugano and Virtual Event, Switzerland\nand self-efficacy of students in introductory computer science courses. In 2020\nIEEE Frontiers in Education Conference (FIE) . IEEE, 1\u20135.\n[15] Chris Piech, Mehran Sahami, Jonathan Huang, and Leonidas Guibas. 2015. Au-\ntonomously generating hints by inferring problem solving policies. In Proceedings\nof the second (2015) acm conference on learning@ scale . 195\u2013204.[16] Thomas W Price, Yihuan Dong, and Dragan Lipovac. 2017. iSnap: towards\nintelligent tutoring in novice programming environments. In Proceedings of the\n2017 ACM SIGCSE Technical Symposium on computer science education . 483\u2013488.\n[17] Carsten Schulte and Jens Bennedsen. 2006. What do teachers teach in introductory\nprogramming?. In Proceedings of the second international workshop on Computing\neducation research . 17\u201328.\n39"
]