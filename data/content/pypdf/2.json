[
    "Semantic Analysis and Classification of Emails through\nInformative Selection of Features and Ensemble AI Model\nShivangi Sachan\u2217\nDepartment of CSE\nIIIT Lucknow\nLucknow, UP, India\nmcs21025@iiitl.ac.inKhushbu Doulani\nVardhaman College of Engineering\nHyderabad, India\nkhushidoulani@gmail.comMainak Adhikari\nDepartment of CSE\nIIIT Lucknow\nUP, India\nmainak.ism@gmail.com\nABSTRACT\nThe emergence of novel types of communication, such as email, has\nbeen brought on by the development of the internet, which radically\nconcentrated the way in that individuals communicate socially and\nwith one another. It is now establishing itself as a crucial aspect of\nthe communication network which has been adopted by a variety\nof commercial enterprises such as retail outlets. So in this research\npaper, we have built a unique spam-detection methodology based\non email-body sentiment analysis. The proposed hybrid model is\nput into practice and preprocessing the data, extracting the proper-\nties, and categorizing data are all steps in the process. To examine\nthe emotive and sequential aspects of texts, we use word embed-\nding and a bi-directional LSTM network. this model frequently\nshortens the training period, then utilizes the Convolution Layer to\nextract text features at a higher level for the BiLSTM network. Our\nmodel performs better than previous versions, with an accuracy\nrate of 97\u201398%. In addition, we show that our model beats not just\nsome well-known machine learning classifiers but also cutting-edge\nmethods for identifying spam communications, demonstrating its\nsuperiority on its own. Suggested Ensemble model\u2019s results are\nexamined in terms of recall, accuracy, and precision\nCCS CONCEPTS\n\u2022Computer systems organization \u2192Embedded systems ;Re-\ndundancy ; Robotics; \u2022Networks\u2192Network reliability.\nKEYWORDS\nDataset, KNN, Gaussian Naive Bayes, LSTM, SVM, Bidirectional\nLSTM, GRU, Word-Embeddings, CNN\nACM Reference Format:\nShivangi Sachan, Khushbu Doulani, and Mainak Adhikari. 2023. Semantic\nAnalysis and Classification of Emails through Informative Selection of\nFeatures and Ensemble AI Model. In 2023 Fifteenth International Conference\non Contemporary Computing (IC3-2023) (IC3 2023), August 03\u201305, 2023, Noida,\nIndia. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/3607947.\n3607979\n\u2217Both authors contributed equally to this research.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nIC3 2023, August 03\u201305, 2023, Noida, India\n\u00a92023 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0022-4/23/08. . . $15.00\nhttps://doi.org/10.1145/3607947.36079791 INTRODUCTION\nOver the past few years, a clear surge of both the amount of spam-\nmers as well as spam emails. This is likely due to a fact that the\ninvestment necessary for engaging in the spamming industry is\nrelatively low. As a result of this, we currently have a system that\nidentifies every email as suspicious, which has caused major expen-\nditures in the investment of defense systems [ 12]. Emails are used\nfor online crimes like fraud, hacking, phishing, E-mail bombing, bul-\nlying, and spamming. [ 16]. Algorithms that are based on machine\nlearning (ML) are now the most effective and often used approach to\nthe recognition of spam. Phishing, which is defined as a fraudulent\nattempt to acquire private information by masquerading as a trust-\nworthy party in electronic communication, has rapidly advanced\npast use of simple techniques and the tactic of casting a wide net;\ninstead, spear phishing uses a variety of sophisticated techniques\nto target a single high-value individual. Other researchers used NB,\nDecision Trees, and SVM to compare the performance of supervised\nML algorithms for spam identification [ 6]. Spam emails clog up re-\ncipients\u2019 inboxes with unsolicited communications, which frustrate\nthem and push them into the attacker\u2019s planned traps [ 7]. As a re-\nsult, spam messages unquestionably pose a risk to both email users\nand the Internet community. In addition, Users may occasionally\nread the entire text of an unsolicited message that is delivered to\nthe target users\u2019 inboxes without realizing that the message is junk\nand then choosing to avoid it. Building a framework for email spam\ndetection is the aim of this project. In this approach, we combine the\nWord-Embedding Network with the CNN layer, Bi-LSTM, and GRU\n(BiLSTM+GRU). CNN layers are used to speed up training time\nbefore the Bi-LSTM network, and more advanced textual character-\nistics are extracted with the use of this network in comparison to\nthe straight LSTM network, in less time. Gated recurrent neural net-\nworks (GRUs) are then added because they train more quickly and\nperform better for language modeling. To evaluate and investigate\nvarious machine learning algorithms for predicting email spam,\nand develop a hybrid classification algorithm to filter email spam\nbefore employing an ensemble classification algorithm to forecast\nit. To put an innovative technique into practice and compare it to\nthe current method in terms of various metrics. Ensemble learn-\ning, a successful machine learning paradigm, combines a group of\nlearners rather than a single learner to forecast unknown target\nattributes. Bagging, boosting, voting, and stacking are the four main\ntypes of ensemble learning techniques. To increase performance,\nan integrated method and the combining of two or three algorithms\nare also suggested. Extraction of text-based features takes a long\ntime. Furthermore, it can be challenging to extract all of the crucial\ninformation from a short text. Over the span associated with this\n181\n",
    "IC3 2023, August 03\u201305, 2023, Noida, India Sachan et al.\nresearch, we utilize Bidirectional Large Short-Term Memories (Bi-\nLSTM) in conjunction with Convolutional Neural Networks (CNN)\nto come up with an innovative method to the detection of spam.\nBagging and boosting approaches were widely preferred in this\nstudy. Contribution and paper organization is as follows: section 1.1\ndescribes literature study, section 1.2 describe motivation for this\nresearch work, section 2 sketches procedure of details implemen-\ntation, Section 3 present experimental setup, dataset description\nand evaluation metrics, and section 4 summarizing outcomes of the\nexperiment.\n1.1 Related Work\nEmail is indeed the second most frequently utilized Internet appli-\ncation as well as the third most common method of cyberbullying,\nclaims one study. Cybercriminals exploit it in a number of ways,\nincluding as sending obscene or abusive messages, adding viruses\nto emails, snatching the private information of victims, and ex-\nposing it to a broad audience. Spam letters made up 53.95% of all\nemail traffic in March 2020. We examine three main types of un-\nlawful emails in our study. First are fake emails, which are sent\nto manipulate recipients to submit sensitive information. The sec-\nond as being cyberbullying\u2019s use of harassing emails to threaten\nindividuals. Suspicious emails that describe illegal activities belong\nto the third category. Many researchers have earlier contributed\nmassively to this subject. The researcher claims there is some proof\nthat suspicious emails were sent before to the events of 9/11. [ 14].\nWhen it comes to data labeling, there are also convinced rule-based\napproaches and technologies ( like VADER) that are used, even\nthough their efficiency of the are together is adversely affected. A\nhidden layer, which itself is essential for vectorization, is the top\nlayer of the model. We use oversampling methods for this minority\nclass because of the absence of data. Sampling techniques can help\nwith multicollinearity, but they have an impact on simulation re-\nsults. Oversampling causes data to be randomly repeated, which\naffects test data because dividing data may result in duplicates. Un-\ndersampling may result in the loss of some strong information. In\norder to advance email research, it is crucial to provide datasets on\ncriminal activity. P. Garg et al. (2021) [ 5], which revealed that spam\nin an email was detected in 70 percent of business emails, spam was\nestablished as an obstacle for email administrators. Recognizing\nspam and getting rid of it were the primary concerns, as spam can\nbe offensive, may lead to other internet sites being tricked, which\ncan offer harmful data, and can feature those who are not particu-\nlar with their content using NLP. To select the best-trained model,\neach mail transmission protocol requires precise and effective email\nclassification, a machine learning comparison is done. Our study\nhas suggested that innovative deep learning outperforms learning\nalgorithms like SVM and RF. Current studies on the classification\nof emails use a variety of machine learning (ML) techniques, with\na few of them focusing on the study of the sentiments consisted of\nwithin email databases. The lack of datasets is a significant obstacle\nto email classification. There are few publicly accessible E-mail\ndatasets, thus researchers must use these datasets to test their hy-\npotheses or gather data on their own. Authors[ 15] describe supplied\ntwo-phased outlier detection models to enhance the IIOT network\u2019s\ndependability. Artificial Neural Network, SVM, Gaussian NB, andRF (random forest) ensemble techniques were performed to forecast\nclass labels, and the outputs were input into a classifying unit to\nincrease accuracy. A method for content-based phishing detection\nwas presented by the authors in [ 2], to classify phishing emails,\nthey employed RF. They categorize spam and phishing emails. They\nenhanced phishing email classifiers with more accurate predictions\nby extracting features. They showed some effective Machine learn-\ning spam filtering techniques. When the PCA method is used, it will\nlower the number of features in the dataset. The collected features\ngo through the PCA algorithm to reduce the number of features.\nThe PCA method is used to make a straightforward representation\nof the information which illustrates the amount of variability there\nis in the data. The authors of [ 20] presented the Fuzzy C-means\nmethod for classifying spam email. To stop spam, they implemented\na membership threshold value. A methodology to identify unla-\nbeled data was put forth by the authors of [ 1] and applied motive\nanalysis to the Enron data collection. They divided the data into\ncategories that were favorable, negative, and neutral. They grouped\nthe data using k-means clustering, an unsupervised ML technique\nand then classified it using the supervised ML techniques SVM and\nNB. Hina, Maryam, and colleagues (2021) implemented Sefaced:\nDeep learning-based semantic analysis and categorization of e-mail\ndata using a forensic technique. For multiclass email classification,\nSeFACED employs a Gated Recurrent Neural Network (GRU) based\non Long Short-Term Memory (LSTM). Different random weight ini-\ntializations affect LSTMs [ 9]. Zhang, Yan, et al.(2019) Experiments\non three-way game-theoretic rough set (GTRS) email spam filter-\ning show that it is feasible to significantly boost coverage without\ndecreasing accuracy [ 23]. According to Xia et al. [ 22], SMS spam\nhas been identified using machine learning model such as naive\nbayes , vector-space modeling, support vector machines (SVM),\nlong selective memory machines (LSTM), and convolutional neural\nnetworks including every instance of a method for categorizing\ndata. Elshoush, Huwaida, et al. (2019) Using adaboost and stochastic\ngradient descent (sgd) algorithms for e-mail filtering with R and\norange software spam [ 3]. Orange software was used to create the\nclassifications, which included Adaboost and SGD. The majority of\nresearchers focused on text-based email spam classification meth-\nods because image-based spam can be filtered in the early stages\nof pre-processing. There are widely used word bag (BoW) model,\nwhich believes that documents are merely unordered collections\nof words, is the foundation for these techniques. Kumaresan [ 11]\nexplains SVM with a cuckoo search algorithm was used to extract\ntextual features for spam detection. Renuka and Visalakshi made\nuse of svm [ 17] spam email identification, followed by selecting\nfeatures using Latent Semantic Indexing (LSI). Here we have used\nlabeled dataset to train the hybrid classifier. We used TF-IDF for\nfeature extraction [ 20] and Textual features for spam detection\nwere extracted using SVM and a cuckoo search algorithm. [ 4] for\nfiltering out the spam email. Combining the integrated strategy to\nthe pure SVM and NB methods, overall accuracy is really improved.\nMoreover, accurate detection for spam email has been proposed\nusing the Negative Selection Algorithm (NSA) and Particle Swarm\nOptimization\u2019s (PSO) algorithm. PSO is used in this instance to\nimprove the effectiveness of the classifier.\n182",
    "Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model IC3 2023, August 03\u201305, 2023, Noida, India\n1.2 Motivation and Novelty\nEmail is most common form of communication between people\nin this digital age. Many users have been victims of spam emails,\nand their personal information has been compromised. The email\nClassification technique is employed to identify and filter junk\nmail, junk, and virus-infected emails prior to reach a user\u2019s inbox.\nExisting email classification methods result in irrelevant emails\nand/or the loss of valuable information. Keeping these constraints\nin mind, the following contributions are made in this paper:\n\u2022Text-based feature extraction is a lengthy process. Further-\nmore, extracting every important feature from text is difficult.\nIn this paper, we show how to employ GRU with Convo-\nlutional Neural Networks and Bidirectional-LSTM to find\nspam.\n\u2022Used Word-Embeddings, BiLSTM, and Gated Recurrent Neu-\nral Networks to examine the relationships, sentimental con-\ntent, and sequential way of email contents.\n\u2022Applied CNN before the Bi-LSTM network, training time can\nbe sped up. This network can also extract more advanced\ntextual features faster than the Bi-LSTM network alone when\ncombined with the GRU network.\n\u2022We use Enorn Corpora datasets and compute precision, re-\ncall, and f-score to assess how well the suggested technique\nperforms. Our model outperforms several well-known ma-\nchine learning techniques as well as more contemporary\nmethods for spam message detection.\n2 PROPOSED SYSTEM ARCHITECTURE AND\nMODEL\nE-mail is a valuable tool for communicating with other users. Email\nallows the sender to efficiently forward millions of advertisements\nat no cost. Unfortunately, this scheme is now being used in a variety\nof organizations. As a result, a massive amount of redundant emails\nis known as spam or junk mail, many people are confused about the\nemails in their E- Mailboxes. Each learning sequence is given for-\nward as well as backward to two different LSTM networks that are\nattached to the same outputs layer in order for bidirectional Lstms\nto function. This indicates that the Bi-LSTM has detailed sequential\ninformation about all points before and following each point in a\nspecific sequence. In other words, we concatenate the outputs from\nboth the forward and the backward LSTM at each time step rather\nthan just encoding the sequence in the forward direction. Each\nword\u2019s encoded form now comprehends the words that come before\nand after it. This is a problem for the Internet community. The di-\nagram depicts various stages that aid in the prediction of email spam:\nBecause real-world data is messy and contains unnecessary infor-\nmation and duplication, data preprocessing is critical in naturallanguage processing (NLP). The major preprocessing steps are de-\npicted below.\n2.1 NLP Tokenization\nTokenization of documents into words follows predefined rules.\nThe tokenization step is carried out in Python with spacy library.\n2.2 Stop Words Removal\nStop words appear infrequently or frequently in the document, but\nthey are less significant in terms of importance. As a result, these\nare removed to improve data processing.\n2.3 Text Normalization\nA word\u2019s lexicon form or order may differ. Thus, they must all be\nchanged to their root word to be correctly analyzed. Lemmatization\nand stemming are the two methods that can be used for normal-\nization. When a word\u2019s final few characters are removed to create\na shorter form, even if that form has no meaning, the procedure\nis known as stemming. lemmatization [ 21] is a mixture of corpus-\nbased an rule-based methods, and it retains the context of a term\nwhile changing it back to its root.\n2.4 Feature Extraction\nfeature extraction which transforms the initial text into its features\nso that it may be used for modeling after being cleaned up and\nnormalized. Before predicting them, we use a specific way to give\nweights to specific terms in our document. While it is simple for a\ncomputer to process numbers, we choose to represent individual\nwords numerically. In such cases, we choose word embeddings. IDF\nis the count of documents containing the term divided by the total\nnumber of documents, and occurrence is the amount of instances a\nword appears in a document. We derive characteristics based on\nequations. 1,2,3,4,5, and 6. We use equations to derive properties.\n\ud835\udc47\ud835\udc53\ud835\udc3c\ud835\udc51\ud835\udc53 =\ud835\udc61\ud835\udc53\u2217\u00121\n\ud835\udc51\ud835\udc53\u0013\n(1)\n\ud835\udc47\ud835\udc53\ud835\udc3c\ud835\udc51\ud835\udc53 =\ud835\udc61\ud835\udc53\u2217Inverse(\ud835\udc51\ud835\udc53) (2)\n\ud835\udc47\ud835\udc53\ud835\udc3c\ud835\udc51\ud835\udc53(\ud835\udc61,\ud835\udc51,\ud835\udc37)=\ud835\udc47\ud835\udc53(\ud835\udc61,\ud835\udc51).\ud835\udc3c\ud835\udc51\ud835\udc53(\ud835\udc61,\ud835\udc37) (3)\n\ud835\udc47\ud835\udc3c\ud835\udc51\ud835\udc53(\ud835\udc61,\ud835\udc51)=log\ud835\udc41\n|\ud835\udc51\ud835\udf16\ud835\udc37\ud835\udc61\ud835\udf16\ud835\udc37|(4)\nA word2vec neural network-based approach is the method that is\nutilized for this goal as the tool. The following equation, referred\nto as 5, shows how word2vec handles word context through the\nuse of probability-accurate measurements. Here letter D stands for\nthe paired-wise display of a set of words, while the letters w and c0\nor c1 represent paired word context that originated from a larger\ncollection of set D.\n\ud835\udc43(\ud835\udc37=1|\ud835\udc64,\ud835\udc50 11:\ud835\udc58)=1\n1+\ud835\udc52\u2212(\ud835\udc64\u00b7\ud835\udc5011+\ud835\udc64\u00b7\ud835\udc5012+...+\ud835\udc64\u00b7\ud835\udc501\ud835\udc58)(5)\n\ud835\udc43(\ud835\udc37=1|\ud835\udc64,\ud835\udc50 1:\ud835\udc58)=1\n1+\ud835\udc52\u2212(\ud835\udc64\u00b7\ud835\udc500)(6)\n183",
    "IC3 2023, August 03\u201305, 2023, Noida, India Sachan et al.\n2.5 Word-Embeddings\nWord-Embedding helps to improve on the typical \"bag-of-words\"\nworldview, which requires a massive sparse feature vector to score\nevery word individually to represent this same entire vocabulary.\nThis perception is sparse because the vocabulary is large, and each\nword or document is defined by a massive vector. Using a word\nmap-based dictionary, word embedding needs to be converted terms\n(words) into real value feature vectors. There are two basic issues\nwith standard feature engineering techniques for deep learning.\nData is represented using sparse vectors, and the second is that\nsome of the meanings of words are not taken into consideration.\nSimilar phrases will have values in embedding vectors that are\nalmost real-valued. The Input length in our proposed study is set\nto 700 for our suggested model. If the texts seemed to be integer\nencoded with value systems between 10 and 20, the vocabulary\ndistance would be 11. Our data is encoded as integers, and the input\nand output dimensions are both set to 50,000. The embedding layer\noutcome will be used in successive layers and for BiLSTM and GRU\nlayers.\n2.6 Machine Learning Model\nWithin the scope of the research, we are using the subsequent ma-\nchine learning techniques, to examine and compare the overall\nefficacy of our suggested Bi-LSTM strategy: Support Vector Ma-\nchine, Gaussian NB, Logistic Regression, K - nearest neighbors, and\nRandom Forest (RF).\n2.7 Convolution Network\nThe popular RNN model generally performs well but takes too\nlong to train the model incorporating the textual sequential data.\nWhen a layer is added after the RNN layer, the model\u2019s learning\nduration is considerably decreased. Higher-level feature extraction\nis another benefit. [ 19] additionally possible using the convolutional\nlayer. In essence, the convolution layer looks for combinations of\nthe various words or paragraphs in the document that involve the\nfilters. We use features with 128 dimensions and a size 10 for each.\nFor this task, the Relu activation function is utilized. After that, the\none-dimensional largest pooling layers with a pooling size of 4 are\nput on the data in order to obtain higher-level features.\n2.8 BiLSTM Network with GRU\nRecurrent Neural Network (RNN) technique of text sentiment anal-\nysis is particularly well-liked and frequently applied. Recurrent\nneural networks (RNN) surpass conventional neural networks. be-\ncause it can remember the information from earlier time steps\nthanks to its memory. A state vector is combined with an RNN\u2019s\ndata to create a new state vector. The resulting state vector uses the\npresent to recollect past knowledge. The RNN is straightforward\nand is based on the following equations:\n\u210e\ud835\udc61=tanh(\ud835\udc4a\u210e\u210e\u210e\ud835\udc61\u22121+\ud835\udc4a\ud835\udf0b\u210e\ud835\udc65\ud835\udc61) (7)\n\ud835\udc66\ud835\udc61=\ud835\udc4a\u210e\ud835\udc66\u210e\ud835\udc61 (8)\nThe vanilla RNN[ 18]is not very good at remembering previous\nsequences. In addition to that, RNN struggles with diminishing\ngradient descent. A kind of RNN is a long short-term recall network\n(LSTM), solves a vanishing gradient descent problem and learnslong-term dependencies[ 10]. LSTM was actually created to address\nthe problem of long-term reliance. LSTM has the unique ability to\nrecall. The cell state is the LSTM model\u2019s central concept. With\nonly a small amount of linear interaction, the cell state follows the\nsequence essentially unmodified from beginning to end. gate of\nan LSTM is also significant. Under the command of these gates,\ninformation is safely inserted to or eliminated from the cell stated.\nThe following equations are used by the LSTM model to update\neach cell:\n\ud835\udc53\ud835\udc61=\ud835\udf0e\u0010\n\ud835\udc4a\ud835\udc53\u00b7[\u210e\ud835\udc61\u22121,\ud835\udc65\ud835\udc61]+\ud835\udc4f\ud835\udc53\u0011\n(9)\nIn this case, Xt denotes input, and ht is the hidden state at the t\ntime step. The following is the revised cell state Ct:\n\ud835\udc56t=\ud835\udf0e(\ud835\udc4a\ud835\udc56[\u210e\ud835\udc61\u22121,\ud835\udc65\ud835\udc61]+\ud835\udc4f\ud835\udc56) (10)\n\ud835\udc36\ud835\udc47=tanh(\ud835\udc4a\ud835\udc50[\u210e\ud835\udc61\u22121,\ud835\udc65\ud835\udc61]+\ud835\udc4f\ud835\udc50\ud835\udc61) (11)\n\ud835\udc36\ud835\udc61=\ud835\udc53\ud835\udc61\u2217\ud835\udc36\ud835\udc61\u22121+\ud835\udc56\ud835\udc61\u2217\ud835\udc36\ud835\udc47 (12)\nHere, we may compute the output and hidden state at t time steps\nusing the point-wise multiplication operator *.\n\ud835\udc5c\ud835\udc61=\ud835\udf0e(\ud835\udc4a\ud835\udc5c\u00b7[\u210e\ud835\udc61\u22121,\ud835\udc65\ud835\udc61]+\ud835\udc4f\ud835\udc5c) (13)\n\u210e\ud835\udc61=\ud835\udc5c\ud835\udc61\u2217tanh(\ud835\udc36\ud835\udc61) (14)\nDue to the reality it only considers all prior contexts from the\npresent one, LSTM does have a few drawbacks. As a result of this,\nit may accept data from preceding time steps through LSTM as well\nas RNN. Therefore, in order to avoid this issue, further improve-\nments are carried out with the help of a bidirectional recurrent\nneural network(Bi-RNN). BiRNN [ 13] can handle two pieces of in-\nformation from both the front and the back. Bi-LSTM is created\nby combining the Bi-RNN and LSTM. As a result, operating LSTM\nhas advantages such as cell state storage so that BiRNN have way\nto acknowledge from the context before and after. As a conse-\nquence of this, it provides the Bi-LSTM with the advantages of an\nLSTM with feedback for the next layer. Remembering long-term\ndependencies is a significant new benefit of Bi-LSTM. The output,\nwhich is a feature vector, will be based on the call state. Finally,\nwe forecast the probability of email content as Normal, Fraudu-\nlent, Harassment, and Suspicious Emails using as an input to the\nsoftmax activation function, which is a weighted sum of the dense\nlayer\u2019s outputs. To regulate the information flow, GRU employs\nthe point-wise multiplying function and logistic sigmoid activation.\nThe GRU has hidden states of storage memory and does not have\ndistinct memory cells or units for state control. The W, U, and b\nvectors, which stand for weights, gates, and biases, respectively, are\ncrucial variables that must be calculated during the creation of the\nGRU model. For training reasons, the pre-trained word embedding\nknown as the Glove vector is used. They made it clear that GRU\nis the superior model when there is a large amount of training\ndata for textual groups and word embedding is available. BiLSTM,\nCNN, and GRU is required so as to compensate for the deletion\nof the document\u2019s long-term and short-term connections. In our\ncase, the embedding dimension, maximum sequence length, and\nlexicon size were used to start the LSTM embedding layer in three\nseparate LSTM models. The input vector was modified to make it\nappropriate for such a Conv1D layer, prior situations\u2019 sequences are\nreturned by LSTM layer. The \"return sequences\" of the LSTM layer\nmust be set to False when the subsequent state is free of the gated\n184",
    "Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model IC3 2023, August 03\u201305, 2023, Noida, India\narchitecture. Quantity of learning parameters must be taken into\nconsideration. A 350-unit LSTM layer was set - up, and different\nLSTM unit combinations were tested. More importantly, because\nit has more parts, the model made with BiLSTM will take longer\nto train. Bidirectional LSTM is the name of a particular kind of\nrecurrent neural network that is primarily used for the processing\nof natural languages. (BiLSTM). It is able to use data from both\nsides, and, in contrast to regular LSTM, it enables input flow in\nboth directions. It is an effective instrument for demonstrating the\nlogical relationships between words and phrases, and this involves\nboth the forward and backward directions of the sequence. In con-\nclusion, BiLSTM works by adding one extra layer of LSTM, causing\nthe information flow to travel in the other direction. It only denotes\nthat the input sequence runs in reverse at the next LSTM layer. Mul-\ntiple operations, including averaging, summation, multiplication,\nand concatenation, are then applied to the results of the two LSTM\nlayers. The gated design of Bi-LSTM and GRU networks solves\nthe disappearing gradient and exploding problems. A good way to\nhandle more long sequences is to use Bi-LSMT and GRU together.\nGRU works well with datasets that don\u2019t have text. In two to three\nrounds, the complicated CNN+BiLSTM+GRU model learns the long\nsequence of email text well. We have used word embedding, cnn,\nbidirectional lstm and gru networks as our three building blocks\nto separate email messages based on their sentiment and text\u2019s\nsequential features. Also, we succinctly demonstrate below why\nthese blocks help identify email spam:\n\u2022First, We have used the Sequence - to - sequence Lstm as the\ncurrent block in the networks since it can retrieve both the\nprevious and next sequences from the current. More so than\na straightforward LSTM network, it can also recognize and\nextract text sentiment and sequential properties.\n\u2022Second, we extract the more complex and advanced charac-\nteristics for Bi-LSTM network using Convolutional Network\nblock, which is the network\u2019s second block after the Bi-LSTM\nblock. Bi-LSTM takes a long time to extract text-based fea-\ntures, hence one of the reasons for using this block is to\nreduce the network\u2019s overall training time.\n3 EXPERIMENTAL EVALUATION\n3.1 Experimental Setup\nWe divided the information into training and testing groups of\n80/20. We divided the remaining 20% of the 80 percent training\ndata into test data for the model. Construct, compute, and evaluate\nthe efficacy of the suggested method using the Pythonic packages\nKeras, as TensorFlow and Scikit learn.\n3.2 Dataset Description\nEmail spam detection is the foundation of this research project. The\ndataset includes normal emails from the Enron corpora, deceptive\nemails from phished email corpora, harassment emails chosen from\nhate speech, and the offensive dataset. Only the content of the email\nbody is used for analysis; all header information, including sender,\ntopic, CC, and BCC, are eliminated. Word2vector, TF-IDF, and Word\nEmbedding are used to extract characteristics from the email mes-\nsage and classify them. This dataset[ 8] is publicly available. The\npresented model is implemented using Python, and several metrics,including accuracy, precision, and recall, are used to examine the\noutcomes.\n3.3 Evaluation Metrics and Results\nClassifier performance is assessed Using metrics such as accuracy,\nprecision, and recall. Four terms make up a confusion matrix that\nis used to calculate these metrics.\n\u2022True positives (TP) are positive values that have been accu-\nrately assigned the positive label.\n\u2022The negative values that are accurately identified as negative\nare known as True Negatives (TN).\n\u2022True Negative values are those that can be accurately identi-\nfied as being negative (TN).\n\u2022Positive readings that have been mistakenly labeled as nega-\ntive are known as False Negatives (FN).\nAssess the efficacy of the suggested model is listed below:\n3.3.1 Accuracy. Accuracy reveals how frequently the ML model\nwas overall correct.\nAccuracy =\ud835\udc47\ud835\udc43+\ud835\udc47\ud835\udc41\n\ud835\udc47\ud835\udc43+\ud835\udc47\ud835\udc41+\ud835\udc39\ud835\udc43+\ud835\udc39\ud835\udc41(15)\n3.3.2 Precision. The accuracy of the model gauges how effectively\nit can predict a specific category.\nPrecision =\ud835\udc47\ud835\udc43\n\ud835\udc47\ud835\udc43+\ud835\udc39\ud835\udc43(16)\n3.3.3 Recall. Recall tells us how often the model was able to rec-\nognize a specific category.\nRecall =\ud835\udc47\ud835\udc43\n\ud835\udc47\ud835\udc43+\ud835\udc39\ud835\udc41(17)\nModel Accuracy Precision Recall\nGaussian NB 91.3 90.1 91.8\nRandom Forest 88.41 90 88\nKNN 86.6 89 87\nSVM 92.4 91 92\nLSTM 95.2 95 95.7\nProposed Ensemble\n(CNN,BiLSTM+GRU)97.32 95.6 95.3\nTable 1: Differet Model\u2019s Score on Test Data\nAccuracy, Precision, and Recall metrics are computed. In the\ngiven Table 1 where six different classifiers are Gaussian NB, Ran-\ndom Forest, KNN, SVM, LSTM, and Propose Ensemble Hybrid\nModel (CNN+BiLSTM+GRU) have been used in this work. In the\nCNN, Bi-LSTM, and GRU architectures which enable sequence pre-\ndiction, CNN strands for feature extraction on data input which are\ncombined with LSTM. It requires less time training and a higher\nexpandable model. Any bottlenecks are created by predictions and\nthe increasing number of distinct units of information. This model\nis useful for dealing with issue-related classifications that consist\nof two or more than two classes. So suggested Ensemble model, out\nof these six classifiers, produces more accurate findings.\n185",
    "IC3 2023, August 03\u201305, 2023, Noida, India Sachan et al.\nFigure 1: Performance Analysis\n3.4 Comparative Analysis\nAmodel\u2019s ability to fit new data is measured by the validation\nloss, whereas its ability to fit training data is determined by the\ntraining loss. The two main variables that decide whether in which\nlearning is efficient or not are validation loss and training loss.\nLSTM and Suggested Ensemble hybrid Models have equivalent loss\nand accuracy. In this context, we are contrasting the LSTM with the\nproposed model (CNN, Bilstm, and GRU) in terms of their respective\nvalidation accuracies and losses. The model\u2019s accuracy was at its\nhighest after 14 epochs of operation when it achieved an accuracy\nof roughly 97-98% while minimizing model loss.\nFigure 2: LSTM Model Training and Validation Accuracy\nFigure 3: LSTM Model Training and Validation Loss\nFigure 4: Ensemble Model (CNN,BiLSTM+GRU) Training\nand Validation Accuracy\nFigure 5: Ensemble Model (CNN,BiLSTM+GRU)Training\nand Validation Loss\n186",
    "Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model IC3 2023, August 03\u201305, 2023, Noida, India\nIn this Proposed ensemble hybrid model\u2019s train accuracy is 98.7%\nValidation accuracy is 97.32% and LSTM has train accuracy of 97.41%\nand validation accuracy is 95.2%. So based on figures 3 and 5 indicate\nthe validation loss for LSTM and the proposed ensemble hybrid\nmodel to be 0.93 and 0.84, respectively, and figures 2 and 4 show the\nvalidation accuracy to be 95.2% and 97.3%, respectively. LSTM and\nthe proposed hybrid model used ensemble artificial intelligence,\nwith the proposed hybrid model outperforming the LSTM. We\ndecide on dense architecture as the final model for identifying the\ntext messages as spam or nonspam based on loss, accuracy, and the\naforementioned charts. The loss and accuracy over epochs are more\nstable than LSTM, and the Proposed classifier has a straightforward\nstructure.\n4 CONCLUSION\nThe model is composed of four networks Word-Embeddings, CNN,\nBi-LSTM, and GRU. We may train the model more quickly by using\nthe convolutional layer first, followed by the word-embedding layer,\nand then the BiLSTM network. The Bidirectional LSTM network\nalso has higher-level properties that we can extract. We have used\na bidirectional LSTM(BiLSTM)and GRU network to memorize a\nsentence\u2019s contextual meaning and sequential structure, which im-\nproves the model\u2019s performance accuracy to roughly 97.32 percent.\nREFERENCES\n[1]Rayan Salah Hag Ali and Neamat El Gayar. 2019. Sentiment analysis using unla-\nbeled email data. In 2019 International Conference on Computational Intelligence\nand Knowledge Economy (ICCIKE) . IEEE, 328\u2013333.\n[2]Ali Shafigh Aski and Navid Khalilzadeh Sourati. 2016. Proposed efficient algo-\nrithm to filter spam using machine learning techniques. Pacific Science Review A:\nNatural Science and Engineering 18, 2 (2016), 145\u2013149.\n[3]Huwaida T Elshoush and Esraa A Dinar. 2019. Using adaboost and stochastic\ngradient descent (sgd) algorithms with R and orange software for filtering e-mail\nspam. In 2019 11th Computer Science and Electronic Engineering (CEEC) . IEEE,\n41\u201346.\n[4]Weimiao Feng, Jianguo Sun, Liguo Zhang, Cuiling Cao, and Qing Yang. 2016. A\nsupport vector machine based naive Bayes algorithm for spam filtering. In 2016\nIEEE 35th International Performance Computing and Communications Conference\n(IPCCC) . IEEE, 1\u20138.\n[5]Pranjul Garg and Nancy Girdhar. 2021. A Systematic Review on Spam Filtering\nTechniques based on Natural Language Processing Framework. In 2021 11th Inter-\nnational Conference on Cloud Computing, Data Science & Engineering (Confluence) .\nIEEE, 30\u201335.\n[6]Adam Kavon Ghazi-Tehrani and Henry N Pontell. 2021. Phishing evolves: Ana-\nlyzing the enduring cybercrime. Victims & Offenders 16, 3 (2021), 316\u2013342.\n[7]Radicati Group et al .2015. Email Statistics Report 2015\u20132019. Radicati Group.\nAccessed August 13 (2015), 2019.\n[8]Maryam Hina, Mohsin Ali, and Javed. 2021. Sefaced: Semantic-based forensic\nanalysis and classification of e-mail data using deep learning. IEEE Access 9\n(2021), 98398\u201398411.\n[9]Maryam Hina, Mohsin Ali, Abdul Rehman Javed, Fahad Ghabban, Liaqat Ali\nKhan, and Zunera Jalil. 2021. Sefaced: Semantic-based forensic analysis and\nclassification of e-mail data using deep learning. IEEE Access 9 (2021), 98398\u2013\n98411.\n[10] Weicong Kong, Zhao Yang Dong, Youwei Jia, David J Hill, Yan Xu, and Yuan\nZhang. 2017. Short-term residential load forecasting based on LSTM recurrent\nneural network. IEEE transactions on smart grid 10, 1 (2017), 841\u2013851.\n[11] T Kumaresan and C Palanisamy. 2017. E-mail spam classification using S-cuckoo\nsearch and support vector machine. International Journal of Bio-Inspired Compu-\ntation 9, 3 (2017), 142\u2013156.\n[12] Nuha H Marza, Mehdi E Manaa, and Hussein A Lafta. 2021. Classification of\nspam emails using deep learning. In 2021 1st Babylon International Conference on\nInformation Technology and Science (BICITS) . IEEE, 63\u201368.\n[13] Tomas Mikolov and Geoffrey Zweig. 2012. Context dependent recurrent neural\nnetwork language model. In 2012 IEEE Spoken Language Technology Workshop\n(SLT) . IEEE, 234\u2013239.\n[14] Sarwat Nizamani, Nasrullah Memon, Mathies Glasdam, and Dong Duong Nguyen.\n2014. Detection of fraudulent emails by employing advanced feature abundance.Egyptian Informatics Journal 15, 3 (2014), 169\u2013174.\n[15] V Priya, I Sumaiya Thaseen, Thippa Reddy Gadekallu, Mohamed K Aboudaif,\nand Emad Abouel Nasr. 2021. Robust attack detection approach for IIoT using\nensemble classifier. arXiv preprint arXiv:2102.01515 (2021).\n[16] Justinas Rastenis, Simona Ramanauskait \u02d9e, Justinas Janulevi\u010dius, Antanas \u010cenys,\nAsta Slotkien \u02d9e, and K\u0119stutis Pakrijauskas. 2020. E-mail-based phishing attack\ntaxonomy. Applied Sciences 10, 7 (2020), 2363.\n[17] Karthika D Renuka and P Visalakshi. 2014. Latent semantic indexing based SVM\nmodel for email spam classification. (2014).\n[18] Shuvendu Roy, Sk Imran Hossain, MAH Akhand, and N Siddique. 2018. Sequence\nmodeling for intelligent typing assistant with Bangla and English keyboard. In\n2018 International Conference on Innovation in Engineering and Technology (ICIET) .\nIEEE, 1\u20136.\n[19] Tara N Sainath, Oriol Vinyals, Andrew Senior, and Ha\u015fim Sak. 2015. Convolu-\ntional, long short-term memory, fully connected deep neural networks. In 2015\nIEEE international conference on acoustics, speech and signal processing (ICASSP) .\nIeee, 4580\u20134584.\n[20] Anuj Kumar Singh, Shashi Bhushan, and Sonakshi Vij. 2019. Filtering spam\nmessages and mails using fuzzy C means algorithm. In 2019 4th International\nConference on Internet of Things: Smart Innovation and Usages (IoT-SIU) . IEEE,\n1\u20135.\n[21] Kristina Toutanova and Colin Cherry. 2009. A global model for joint lemmati-\nzation and part-of-speech prediction. In Proceedings of the Joint Conference of\nthe 47th Annual Meeting of the ACL and the 4th International Joint Conference on\nNatural Language Processing of the AFNLP . 486\u2013494.\n[22] Tian Xia. 2020. A constant time complexity spam detection algorithm for boosting\nthroughput on rule-based filtering systems. IEEE Access 8 (2020), 82653\u201382661.\n[23] Yan Zhang, PengFei Liu, and JingTao Yao. 2019. Three-way email spam filtering\nwith game-theoretic rough sets. In 2019 International conference on computing,\nnetworking and communications (ICNC) . IEEE, 552\u2013556.\nReceived 15 April 2023\n187"
]