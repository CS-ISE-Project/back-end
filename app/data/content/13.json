[
    "Advances in Engineering Software 188 (2024) 103571 Available online 7 December 2023 09659978/ 2023 Elsevier Ltd. All rights reserved. Contents lists available at ScienceDirect Advances in Engineering Software journal homepage: www.elsevier.com/locate/advengsoft Research paper SMT 2.0: A Surrogate Modeling Toolbox with a focus on hierarchical and mixed variables Gaussian processes Paul Saves a,b,,1, R\u00e9mi Lafage a,1, Nathalie Bartoli a,1, Youssef Diouane c,1, Jasper Bussemaker d,1, Thierry Lefebvre a,1, John T. Hwang e,1, Joseph Morlier f,1, Joaquim R.R.A. Martins g,1 a ONERA/DTIS, Universit\u00e9 de Toulouse, Toulouse, France b ISAESUPAERO, Universit\u00e9 de Toulouse, Toulouse, France c Polytechnique Montr\u00e9al, Montreal, QC, Canada d German Aerospace Center (DLR), Institute of System Architectures in Aeronautics, Hamburg, Germany e University of California San Diego, Department of Mechanical and Aerospace Engineering, La Jolla, CA, USA f ICA, Universit\u00e9 de Toulouse, ISAESUPAERO, INSA, CNRS, MINES ALBI, UPS, Toulouse, France g University of Michigan, Department of Aerospace Engineering, Ann Arbor, MI, USA A R T I C L E I N F O Dataset link: https://colab.research.google.com /github/SMTorg/smt/blob/master/tutorial/No tebookRunTestCases_Paper_SMT_v2.ipynb Keywords: Surrogate modeling Gaussian process Kriging Hierarchical problems Hierarchical and mixedcategorical inputs Meta variables A B S T R A C T The Surrogate Modeling Toolbox (SMT) is an opensource Python package that offers a collection of surrogate modeling methods, sampling techniques, and a set of sample problems. This paper presents SMT 2.0, a major new release of SMT that introduces significant upgrades and new features to the toolbox. This release adds the capability to handle mixedvariable surrogate models and hierarchical variables. These types of variables are becoming increasingly important in several surrogate modeling applications. SMT 2.0 also improves SMT by extending sampling methods, adding new surrogate models, and computing variance and kernel derivatives for Kriging. This release also includes new functions to handle noisy and use multifidelity data. To the best of our knowledge, SMT 2.0 is the first opensource surrogate library to propose surrogate models for hierarchical and mixed inputs. This opensource software is distributed under the New BSD license.2 1. Motivation and significance With the increasing complexity and accuracy of numerical models, it has become more challenging to run complex simulations and computer codes [1,2]. As a consequence, surrogate models have been recognized as a key tool for engineering tasks such as design space exploration, uncertainty quantification, and optimization [3]. In practice, surrogate models are used to reduce the computational effort of these tasks by replacing expensive numerical simulations with closedform approxi mations [4, Ch. 10]. To build such a model, we start by evaluating the original expensive simulation at a set of points through a Design of Experiments (DoE). Then, the corresponding evaluations are used to build the surrogate model according to the chosen approximation, such as Kriging, quadratic interpolation, or least squares regression. The Surrogate Modeling Toolbox (SMT) is an opensource frame work that provides functions to efficiently build surrogate models [5]. Corresponding author at: ONERA/DTIS, Universit\u00e9 de Toulouse, Toulouse, France. Email addresses: paul.saves@onera.fr (P. Saves), remi.lafage@onera.fr (R. Lafage), nathalie.bartoli@onera.fr (N. Bartoli), youssef.diouane@polymtl.ca (Y. Diouane), jasper.bussemaker@dlr.de (J. Bussemaker), thierry.lefebvre@onera.fr (T. Lefebvre), jhwang@eng.ucsd.edu (J.T. Hwang), joseph.morlier@isaesupaero.fr (J. Morlier), jrram@umich.edu (J.R.R.A. Martins). 1 All authors contributed to this work, research and manuscript. 2 https://github.com/SMTorg/SMT Kriging models (also known as Gaussian processes) that take advantage of derivative information are one of SMTs key features [6]. Numerical experiments have shown that SMT achieved lower prediction error and computational cost than Scikitlearn [7] and UQLab [8] for a fixed number of points [9]. SMT has been applied to rocket engine coaxialinjector optimization [10], aircraft engine consumption mod eling [11], numerical integration [12], multifidelity sensitivity analy sis [13], highorder robust finite elements methods [14,15], planning for photovoltaic solar energy [16], wind turbines design optimiza tion [17], porous material optimization for a high pressure turbine vane [18], chemical process design [19] and many other applications. In systems engineering, architecturelevel choices significantly in fluence the final system performance, and therefore, it is desirable to consider such choices in the early design phases [20]. Architectural choices are parameterized with discrete design variables; examples in clude the selection of technologies, materials, component connections, https://doi.org/10.1016/j.advengsoft.2023.103571 Received 22 August 2023; Received in revised form 23 October 2023; Accepted 26 November 2023",
    "Advances in Engineering Software 188 (2024) 103571 2 P. Saves et al. Table 1 Comparison of software packages for hierarchical and mixed Kriging models. = implemented. * = userdefined. Package BOTorch Dakota DiceKriging KerGP LVGP Parmoo Spearmint SMT 2.0 Reference [25] [26] [27] [32] [28] [29] [30] This paper License MIT EPL GPL GPL GPL BSD GNU BSD Language Python C R R R Python Python Python Mixed var. GD kernel * CR kernel HH kernel EHH kernel * Hierarchical var. and number of instantiated elements. When design problems include both discrete variables and continuous variables, they are said to have mixed variables. When architectural choices lead to different sets of design variables, we have hierarchical variables [21,22]. For example, consider differ ent aircraft propulsion architectures [23]. A conventional gas turbine would not require a variable to represent a choice in the electrical power source, while hybrid or pure electric propulsion would require such a variable. The relationship between the choices and the sets of variables can be represented by a hierarchy. Handling hierarchical and mixed variables requires specialized sur rogate modeling techniques [24]. To address these needs, SMT 2.0 is offering researchers and practitioners a collection of cuttingedge tools to build surrogate models with continuous, mixed and hierarchical variables. The main objective of this paper is to detail the new enhance ments that have been added in this release compared to the original SMT 0.2 release [5]. There are two new major capabilities in SMT 2.0: the ability to build surrogate models involving mixed variables and the support for hierarchical variables within Kriging models. To handle mixed variables in Kriging models, existing libraries such as BoTorch [25], Dakota [26], DiceKriging [27], LVGP [28], Parmoo [29], and Spearmint [30] implement simple mixed models by using either continuous relax ation (CR), also known as onehot encoding [30], or a Gower distance (GD) based correlation kernel [31]. KerGP [32] (developed in R) imple ments more general kernels but there is no Python opensource toolbox that implements more general kernels to deal with mixed variables, such as the homoscedastic hypersphere (HH) [33] and exponential homoscedastic hypersphere (EHH) [34] kernels. Such kernels require the tuning of a large number of hyperparameters but lead to more accurate Kriging surrogates than simpler mixed kernels [34,35]. SMT 2.0 implements all these kernels (CR, GD, HH, and EHH) through a unified framework and implementation. To handle hierarchical vari ables, no library in the literature can build peculiar surrogate models except SMT 2.0, which implements two Kriging methods for these variables. Notwithstanding, most softwares are compatible with a na\u00efve strategy called the imputation method [24] but this method lacks depth and depends on arbitrary choices. This is why Hutter and Osborne [21] proposed a first kernel, called ArcKernel which in turn was generalized by Horn et al. [36] with a new kernel called the Wedge Kernel [37]. None of these kernels are available in any opensource modeling software. Furthermore, thanks to the framework introduced in Audet et al. [38], our proposed kernels are sufficiently general so that all existing hierarchical kernels are included within it. Section 4 describes the two kernels implemented in SMT 2.0 that are referred as SMT ArcKernel and SMT AlgKernel. In particular, Alg Kernel is a novel hierarchical kernel introduced in this paper. Table 1 outlines the main features of the stateoftheart modeling software that can handle hierarchical and mixed variables. SMT 2.0 introduces other enhancements, such as additional sam pling procedures, new surrogate models, new Kriging kernels (and their derivatives), Kriging variance derivatives, and an adaptive criterion for highdimensional problems. SMT 2.0 adds applications of Bayesian optimization (BO) with hierarchical and mixed variables or noisy co Kriging that have been successfully applied to aircraft design [39], data fusion [40], and structural design [41]. The SMT 2.0 interface is more userfriendly and offers an improved and more detailed documentation for users and developers.3 SMT 2.0 is hosted publicly4 and can be directly imported within Python scripts. It is released under the New BSD License and runs on Linux, MacOS, and Windows operating sys tems. Regression tests are run automatically for each operating system whenever a change is committed to the repository. In short, SMT 2.0 builds on the strengths of the original SMT package while adding new features. On one hand, the emphasis on derivatives (including predic tion, training and output derivatives) is maintained and improved in SMT 2.0. On the other hand, this new release includes support for hierarchical and mixed variables Kriging based models. For the sake of reproducibility, an opensource notebook is available that gathers all the methods and results presented on this paper.5 The remainder of the paper is organized as follows. First, we in troduce the organization and the main implemented features of the release in Section 2. Then, we describe the mixedvariable Kriging model with an example in Section 3. Similarly, we describe and provide an example for a hierarchicalvariable Kriging model in Section 4. The Bayesian optimization models and applications are described in Section 5. Finally, we describe the other relevant contributions in Section 6 and conclude in Section 7. 2. SMT 2.0 : an improved surrogate modeling toolbox From a software point of view, SMT 2.0 maintains and improves the modularity and generality of the original SMT version [5]. In this section, we describe the software as follows. Section 2.1 describes the legacy of SMT 0.2. Then, Section 2.2 describes the organization of the repository. Finally, Section 2.3 shows the new capabilities implemented in the SMT 2.0 update. 2.1. Background on SMT former version: SMT 0.2 SMT [5] is an opensource collaborative work originally developed by ONERA, NASA Glenn, ISAESUPAERO/ICA and the University of Michigan. Now, both Polytechnique Montr\u00e9al and the University of California San Diego are also contributors. SMT 2.0 updates and ex tends the original SMT repository capabilities among which the original publication [5] focuses on different types of derivatives for surrogate models detailed hereafter. 3 http://smt.readthedocs.io/en/latest 4 https://github.com/SMTorg/smt 5 https://github.com/SMTorg/smt/tree/master/tutorial/ NotebookRunTestCases_Paper_SMT_v2.ipynb",
    "Advances in Engineering Software 188 (2024) 103571 3 P. Saves et al. Table 2 Impact of using Numba on training time of the hierarchical Goldstein problem. Speedup is calculated excluding the JIT compilation table, as this step is only needed once after SMT installation. Training set Without numba Numba Speedup JIT overhead 15 points 1.3 s 1.1 s 15% 24 s 150 points 38 s 7.4 s 80% 23 s A Python surrogate modeling framework with derivatives. One of the original main motivations for SMT was derivative support. In fact, none of the existing packages for surrogate modeling such as Scikitlearn in Python [7], SUMO in Matlab [42] or GPML in Matlab and Octave [43] focuses on derivatives. Three types of derivatives are distinguished: prediction derivatives, training derivatives, and output derivatives. SMT also includes new models with derivatives such as Kriging with Partial Least Squares (KPLS) [44] and Regularized Minimalenergy Tensorproduct Spline (RMTS) [3]. These developed derivatives were even used in a novel algorithm called GradientEnhanced Kriging with Partial Least Squares (GEKPLS) [6] to use with adjoint methods, for example [45]. Software architecture, documentation, and automatic testing. SMT is orga nized along three main submodules that implement a set of sampling techniques (sampling_methods), benchmarking functions (problems), and surrogate modeling techniques (surrogate_models). The toolbox documentation6 is created using reStructuredText and Sphinx, a doc umentation generation package for Python, with custom extensions. Code snippets in the documentation pages are taken directly from actual tests in the source code and are automatically updated. The output from these code snippets and tables of options are generated dynamically by custom Sphinx extensions. This leads to highquality documentation with minimal effort. Along with user documentation, developer documentation is also provided to explain how to contribute to SMT. This includes a list of API methods for the SurrogateModel, SamplingMethod, and Problem classes, that must be implemented to create a new surrogate modeling method, sampling technique, or benchmarking problem. When a developer submits a pull request, it is merged only after passing the automated tests and receiving approval from at least one reviewer. The repository on GitHub7 is linked to continuous integration tests (GitHub Actions) for Windows, Linux and MacOS, to a coverage test on coveralls.io and to a dependency version check for Python with DependaBot. Various parts of the source code have been accelerated using Numba [46], an LLVMbased justintime (JIT) compiler for numpyheavy Python code. Numba is applied to con ventional Python code using function decorators, thereby minimizing its impact on the development process and not requiring an additional build step. For a mixed Kriging surrogate with 150 training points, a speedup of up to 80% is observed, see Table 2. The JIT compilation step only needs to be done once when installing or upgrading SMT and adds an overhead of approximately 24 s on a typical workstation In this paper, all results are obtained using an Intel Xeon CPU E52650 v4 @ 2.20 GHz core and 128 GB of memory with a Broadwell generation processor frontend and a compute node of a peak power of 844 GFlops. 2.2. Organization of SMT 2.0 The main features of the opensource repository SMT 2.0 are described in Fig. 1. More precisely, Sampling Methods, Problems and Surrogate models are kept from SMT 0.2 and two new sections Models applications and Interactive notebooks have been added to the architecture of the code. These sections are 6 https://smt.readthedocs.org 7 https://github.com/SMTorg/smt highlighted in blue and detailed on Fig. 1. The new major features implemented in SMT 2.0 are highlighted in lavender whereas the legacy features that were already in present in the original publication for SMT 0.2 [5] are in black. 2.3. New features within SMT 2.0 The main objective of this new release is to enable Kriging surrogate models for use with both hierarchical and mixed variables. Moreover, for each of these five submodules described in Section 2.2, several improvements have been made between the original version and the SMT 2.0 release. Hierarchical and mixed design space. A new design space definition class DesignSpace has been added that implements hierarchical and mixed functionalities. Design variables can either be continu ous (FloatVariable), ordered (OrdinalVariable) or categorical (CategoricalVariable). The integer type (IntegerVariable) rep resents a special case of the ordered variable, specified by bounds (inclusive) rather than a list of possible values. The hierarchical struc ture of the design space can be defined using declare_decreed_var: this function declares that a variable is a decreed variable that is activated when the associated meta variable takes one of a set of specified values, see Section 4 for background. The DesignSpace class also implements mechanisms for sampling valid design vectors (i.e. design vectors that adhere to the hierarchical structure of the design space) using any of the belowmentioned samplers, for cor recting and imputing design vectors, and for requesting which design variables are acting in a given design vector. Correction ensures that variables have valid values (e.g. integers for discrete variables) [24], and imputation replaces nonacting variables by some default value (0 for discrete variables, midway between the bounds for continuous variables in SMT 2.0) [47]. Sampling. SMT implements three methods for sampling. The first one is a na\u00efve approach, called Random that draws uniformly points along every dimension. The second sampling method is called Full Fac torial and draws a point for every cross combination of variables, to have an exhaustive design of experiments. The last one is the Latin Hypercube Sampling (LHS) [48] that draws a point in every Latin square parameterized by a certain criterion. For LHS, a new criterion to manage the randomness has been implemented and the sampling method was adapted for multifidelity and mixed or hierarchical variables. More details about the new sampling techniques are given in Section 6.1. Problems. SMT implements two new engineering problems: a mixed variant of a cantilever beam described in Section 3 and a hierarchical neural network described in Section 4. Surrogate models. In order to keep up with stateofart, several re leases done from the original version developed new options for the already existing surrogates. In particular, compared to the original publication [5], SMT 2.0 adds gradientenhanced neural networks [45] and marginal Gaussian process [49] models to the list of available surrogates. More details about the new models are given in Section 6.2. Applications. Several applications have been added to the toolbox to demonstrate the surrogate models capabilities. The most relevant ap plication is efficient global optimization (EGO), a Bayesian optimiza tion algorithm [50,51]. EGO optimizes expensivetoevaluate blackbox problems with a chosen surrogate model and a chosen optimization criterion [52]. The usage of EGO with hierarchical and mixed variables is described in Section 5.",
    "Advances in Engineering Software 188 (2024) 103571 4 P. Saves et al. Fig. 1. Functionalities of SMT 2.0. The new major features implemented in SMT 2.0 compared to SMT 0.2 are highlighted with the lavender color. Interactive notebooks. These tutorials introduce and explain how to use the toolbox for different surrogate models and applications.8 Every tutorial is available both as a .ipynb file and directly on Google colab.9 In particular, a hierarchical and mixed variables dedicated notebook is available to reproduce the results presented on this paper.10 In the following, Section 3 details the Kriging based surrogate models for mixed variables, and Section 4 presents our new Kriging surrogate for hierarchical variables. Section 5 details the EGO applica tion and the other new relevant features aforementioned are described succinctly in Section 6. 8 https://github.com/SMTorg/smt/tree/master/tutorial 9 https://colab.research.google.com/github/SMTorg/smt/ 10 https://github.com/SMTorg/smt/tree/master/tutorial/ NotebookRunTestCases_Paper_SMT_v2.ipynb 3. Surrogate models with mixed variables in SMT 2.0 As mentioned in Section 1, design variables can be either of continu ous or discrete type, and a problem with both types is a mixedvariable problem. Discrete variables can be ordinal or categorical. A discrete variable is ordinal if there is an order relation within the set of possible values. An example of an ordinal design variable is the number of engines in an aircraft. A possible set of values in this case could be 2, 4, 8. A discrete variable is categorical if no order relation is known between the possible choices the variable can take. One example of a categorical variable is the color of a surface. A possible example of a set of choices could be blue, red, green. The possible choices are called the levels of the variable. Several methods have been proposed to address the recent increase interest in mixed Kriging based models [3033,35,39,53,54]. The main difference from a continuous Kriging model is in the estimation of",
    "Advances in Engineering Software 188 (2024) 103571 5 P. Saves et al. Table 3 Categorical kernels implemented in SMT 2.0. Name \ud835\udf05(\ud835\udf19) \ud835\udef7(\ud835\udee9\ud835\udc56) # of hyperparam. SMT GD exp(\ud835\udf19) [\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udc57,\ud835\udc57 = 1 2 \ud835\udf03\ud835\udc56 ; [\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udc57\ud835\udc57 = 0 1 SMT CR exp(\ud835\udf19) [\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udc57,\ud835\udc57 = [\ud835\udee9\ud835\udc56]\ud835\udc57,\ud835\udc57 ; [\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udc57\ud835\udc57 = 0 \ud835\udc3f\ud835\udc56 SMT EHH exp(\ud835\udf19) [\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udc57,\ud835\udc57 = 0 ; [\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udc57\ud835\udc57 = log \ud835\udf16 2 ([\ud835\udc36(\ud835\udee9\ud835\udc56)\ud835\udc36(\ud835\udee9\ud835\udc56)]\ud835\udc57,\ud835\udc57 1) 1 2 (\ud835\udc3f\ud835\udc56)(\ud835\udc3f\ud835\udc56 1) SMT HH \ud835\udf19 [\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udc57,\ud835\udc57 = 1 ; [\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udc57\ud835\udc57 = [\ud835\udc36(\ud835\udee9\ud835\udc56)\ud835\udc36(\ud835\udee9\ud835\udc56)]\ud835\udc57,\ud835\udc57 1 2 (\ud835\udc3f\ud835\udc56)(\ud835\udc3f\ud835\udc56 1) the categorical correlation matrix, which is critical to determine the mean and variance predictions. As mentioned in Section 1, approaches such as CR [30,39], continuous latent variables [54], and GD [31] use a kernelbased method to estimate the correlation matrix. Other methods estimate the correlation matrix by modeling the correlation entries directly [32,35,53], such as HH [33] and EHH [34]. The HH correlation kernel is of particular interest because it generalizes simpler kernels such as CR and GD [34]. In SMT 2.0, the correlation kernel is an option that can be set to either CR (CONT_RELAX_KERNEL), GD (GOWER_KERNEL), HH (HOMO_HSPHERE_KERNEL) or EHH (EXP_HOMO_HSPHERE_KERNEL). 3.1. Mixed Gaussian processes The continuous and ordinal variables are both treated similarly in SMT 2.0 with a continuous kernel, where the ordinal values are converted to continuous through relaxation. For categorical variables, four models (GD, CR, EHH and HH) can be used in SMT 2.0 if specified by the API. This is why we developed a unified mathematical formulation that allows a unique implementation for any model. Denote \ud835\udc59 the number of categorical variables. For a given \ud835\udc56 {1, , \ud835\udc59}, the \ud835\udc56th categorical variable is denoted \ud835\udc50\ud835\udc56 and its number of levels is denoted \ud835\udc3f\ud835\udc56. The hyperparameter matrix peculiar to this variable \ud835\udc50\ud835\udc56 is \ud835\udee9\ud835\udc56 = [\ud835\udee9\ud835\udc56]1,1 \ud835\udc7a\ud835\udc9a\ud835\udc8e. [\ud835\udee9\ud835\udc56]1,2 [\ud835\udee9\ud835\udc56]2,2 [\ud835\udee9\ud835\udc56]1,\ud835\udc3f\ud835\udc56 [\ud835\udee9\ud835\udc56]\ud835\udc3f\ud835\udc561,\ud835\udc3f\ud835\udc56 [\ud835\udee9\ud835\udc56]\ud835\udc3f\ud835\udc56,\ud835\udc3f\ud835\udc56 , and the categorical parameters are defined as \ud835\udf03\ud835\udc50\ud835\udc4e\ud835\udc61 = {\ud835\udee91, , \ud835\udee9\ud835\udc59}. For two given inputs in the DoE, for example, the \ud835\udc5fth and \ud835\udc60th points, let \ud835\udc50\ud835\udc5f \ud835\udc56 and \ud835\udc50\ud835\udc60 \ud835\udc56 be the associated categorical variables taking respectively the \ud835\udcc1\ud835\udc56 \ud835\udc5f and the \ud835\udcc1\ud835\udc56 \ud835\udc60 level on the categorical variable \ud835\udc50\ud835\udc56. The categorical correlation kernel is defined by \ud835\udc58\ud835\udc50\ud835\udc4e\ud835\udc61(\ud835\udc50\ud835\udc5f, \ud835\udc50\ud835\udc60, \ud835\udf03\ud835\udc50\ud835\udc4e\ud835\udc61) = \ud835\udc59 \ud835\udc56=1 \ud835\udf05([\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udcc1\ud835\udc5f \ud835\udc56 ,\ud835\udcc1\ud835\udc60 \ud835\udc56 ) \ud835\udf05([\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udcc1\ud835\udc60 \ud835\udc56 ,\ud835\udcc1\ud835\udc5f \ud835\udc56 )\ud835\udf05([\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udcc1\ud835\udc5f \ud835\udc56 ,\ud835\udcc1\ud835\udc5f \ud835\udc56 )\ud835\udf05([\ud835\udef7(\ud835\udee9\ud835\udc56)]\ud835\udcc1\ud835\udc60 \ud835\udc56 ,\ud835\udcc1\ud835\udc60 \ud835\udc56 ) (1) where \ud835\udf05 is either a positive definite kernel or identity and \ud835\udef7(.) is a symmetric positive definite (SPD) function such that the matrix \ud835\udef7(\ud835\udee9\ud835\udc56) is SPD if \ud835\udee9\ud835\udc56 is SPD. For an exponential kernel, Table 3 gives the parameterizations of \ud835\udef7 and \ud835\udf05 that correspond to GD, CR, HH, and EHH kernels. The complexity of these different kernels depends on the number of hyperparameters that characterizes them. As defined by Saves et al. [34], for every categorical variable \ud835\udc56 {1, , \ud835\udc59}, the matrix \ud835\udc36(\ud835\udee9\ud835\udc56) R\ud835\udc3f\ud835\udc56\ud835\udc3f\ud835\udc56 is lower triangular and built using a hypersphere decomposition [55,56] from the symmetric matrix \ud835\udee9\ud835\udc56 R\ud835\udc3f\ud835\udc56\ud835\udc3f\ud835\udc56 of hyperparameters. The variable \ud835\udf16 is a small positive constant and the variable \ud835\udf03\ud835\udc56 denotes the only positive hyperparameter that is used for the Gower distance kernel. Another Kriging based model that can use mixed variables is Kriging with partial least squares (KPLS) [57]. KPLS adapts Kriging to high dimensional problems by using a reduced number of hyperparameters thanks to a projection into a smaller space. Also, for a general surrogate, not necessarily Kriging, SMT 2.0 uses continuous relaxation to allow whatever model to handle mixed variables. For example, we can use mixed variables with least squares (LS) or quadratic polynomial (QP) models. We now illustrate the abilities of the toolbox in terms of mixed modeling over an engineering test case. Table 4 Results of the cantilever beam models [34, Table 4]. Categorical kernel Displacement error (cm) Likelihood # of hyperparam. SMT GD 1.3861 111.13 3 SMT CR 1.1671 155.32 14 SMT EHH 0.1613 236.25 68 SMT HH 0.2033 235.66 68 3.2. An engineering design testcase A classic engineering problem commonly used for model validation is the beam bending problem [32,58]. This problem is illustrated on Fig. 2(a) and consists of a cantilever beam in its linear range loaded at its free end with a force \ud835\udc39. As in Cheng et al. [58], the Young modulus is \ud835\udc38 = 200 GPa and the chosen load is \ud835\udc39 = 50 kN. Also, as in Roustant et al. [32], 12 possible crosssections can be used. These 12 sections consist of 4 possible shapes that can be either hollow, thick or full as illustrated in Fig. 2(b). To compare the mixed Kriging models of SMT 2.0, we draw a 98 point LHS as training set and the validation set is a grid of 12 30 30 = 10800 points. For the four implemented methods, displacement error (computed with a rootmeansquare error criterion), likelihood, number of hyperparameters and computational time for every model are shown in Table 4. For the continuous variables, we use the square exponential kernel. More details are found in [34]. As expected, the complex EHH and HH models lead to a lower displacement error and a higher likelihood value, but use more hyperparameters and increase the computational cost compared to GD and CR. On this test case, the kernel EHH is easier to optimize than HH but in general, they are similar in terms of performance. Also, by default SMT 2.0 uses CR as it is known to be a good tradeoff between complexity and performance [59]. 4. Surrogate models with hierarchical variables in SMT 2.0 To introduce the newly developed Kriging model for hierarchical variables implemented in SMT 2.0, we present the general mathe matical framework for hierarchical and mixed variables established by Audet et al. [38]. In SMT 2.0, two variants of our new method are implemented, namely SMT AlgKernel and SMT ArcKernel. In particular, the SMT AlgKernel is a novel correlation kernel introduced in this paper. 4.1. The hierarchical variables framework A problem structure is classified as hierarchical when the sets of active variables depend on architectural choices. This occurs frequently in industrial design problems. In hierarchical problems, we can classify variables as neutral, meta (also known as dimensional) or decreed (also known as conditionally active) as detailed in Audet et al. [38]. Neutral variables are the variables that are not affected by the hierarchy whereas the value assigned to meta variables determines which decreed variables are activated. For example, a meta variable could be the number of engines. If the number of engines changes, the number of decreed bypass ratios that every engine should specify also changes.",
    "Advances in Engineering Software 188 (2024) 103571 6 P. Saves et al. Fig. 2. Cantilever beam problem [34, Figure 6]. Fig. 3. Variables classification as used in SMT 2.0. However, the wing aspect ratio being neutral, it is not affected by this hierarchy. Problems involving hierarchical variables are generally dependant on discrete architectures and as such involve mixed variables. Hence, in addition to their role (neutral, meta or decreed), each variable also has a variable type amongst categorical, ordinal or continuous. For the sake of simplicity and because both continuous and ordinal variables are treated similarly [34], we chose to regroup them as quantitative variables. For instance, the neutral variables \ud835\udc65neu may be partitioned into different variable types, such that \ud835\udc65neu = (\ud835\udc65cat neu, \ud835\udc65qnt neu) where \ud835\udc65cat neu represents the categorical variables and \ud835\udc65qnt neu are the quantitative ones. The variable classification scheme in SMT 2.0 is detailed in Fig. 3. To explain the framework and the new Kriging model, we illustrate the inputs variables of the model using a classical machine learn ing problem related to the hyperparameters optimization of a fully connected MultiLayer Perceptron (MLP) [38] on Fig. 4. In Table 5, we detail the input variables of the model related to the MLP problem (i.e., the hyperparameters of the neural network, together with their types and roles). To keep things clear and concise, the chosen problem is a simplification of the original problem developed by Audet et al. [38]. Regarding the MLP problem of Fig. 4 and following the classi fication scheme of Fig. 3, we start by separating the input variables according to their role. In fact, 1. changing the number of hidden layers modifies the number of inputs variables. Therefore, # of hidden layers is a meta variable. 2. The number of neurons in the hidden layer number \ud835\udc58 is either included or excluded. For example, the # of neurons in the 3rd layer would be excluded for an input that only has 2 hidden layers. Therefore, # of neurons hidden layer \ud835\udc58 are decreed variables. 3. The Learning rate, Momentum, Activation function and Batch size are not affected by the hierarchy choice. Therefore, they are neutral variables. According to their types, the MLP input variables can be classified as follows: 4. The meta variable # of hidden layers is an integer and, as such, is represented by the component \ud835\udc65qnt met. 5. The decreed variables # of neurons hidden layer \ud835\udc58 are integers and, as such, are represented by the component \ud835\udc65qnt dec. 6. The Learning rate, Momentum, Activation function and Batch size are, respectively, continuous, for the first two (ev ery value between two bounds), categorical (qualitative between three choices) and integer (quantitative between 6 choices). Therefore, the Activation function and the Momentum are represented by the component \ud835\udc65cat neu. The Learning rate and the Batch size are represented by the component \ud835\udc65qnt neu. To model hierarchical variables, as proposed in [38], we separate the input space as (neu, met, dec) where dec = \ud835\udc65metmet inc(\ud835\udc65met).",
    "Advances in Engineering Software 188 (2024) 103571 7 P. Saves et al. Fig. 4. The MultiLayer Perceptron (MLP) problem. Source: Figure adapted from [38, Figure 1]. Table 5 A detailed description of the variables in the MLP problem. MLP Hyperparameters Variable Domain Type Role Learning rate \ud835\udc5f [105, 102] FLOAT NEUTRAL Momentum \ud835\udefc [0, 1] FLOAT NEUTRAL Activation function \ud835\udc4e {\ud835\udc45\ud835\udc52\ud835\udc3f\ud835\udc48, \ud835\udc46\ud835\udc56\ud835\udc54\ud835\udc5a\ud835\udc5c\ud835\udc56\ud835\udc51, \ud835\udc47 \ud835\udc4e\ud835\udc5b\u210e} ENUM NEUTRAL Batch size \ud835\udc4f {8, 16, , 128, 256} ORD NEUTRAL # of hidden layers \ud835\udc59 {1, 2, 3} ORD META # of neurons hidden layer \ud835\udc58 \ud835\udc5b\ud835\udc58 {50, 51, , 55} ORD DECREED Hence, for a given point \ud835\udc65 , one has \ud835\udc65 = (\ud835\udc65neu, \ud835\udc65met, \ud835\udc65inc(\ud835\udc65met)), where \ud835\udc65neu neu, \ud835\udc65met met and \ud835\udc65inc(\ud835\udc62met) inc(\ud835\udc62met) are defined as follows: The components \ud835\udc65neu neu gather all neutral variables that are not impacted by the meta variables but needed. For ex ample, in the MLP problem, neu gathers the possible learning rates, momentum, activation functions and batch sizes. Namely, from Table 5, neu = [105, 102][0, 1] {ReLu, Sigmoid, Tanh} {8, 16, , 256}. The components \ud835\udc65met gather the meta (also known as dimen sional) variables that determine the inclusion or exclusion of other variables. For example, in the MLP problem, met represents the possible numbers of layers in the MLP. Namely, from Table 5, met = {1, 2, 3}. The components \ud835\udc65inc(\ud835\udc65met), contain the decreed variables whose inclusion (decreedincluded) or exclusion (decreedexcluded) is determined by the values of the meta components \ud835\udc65met. For exam ple, in the MLP problem, dec represents the number of neurons in the decreed layers. Namely, from Table 5, inc(\ud835\udc65met = 3) = [50, 55]3, inc(\ud835\udc65met = 2) = [50, 55]2 and inc(\ud835\udc65met = 1) = [50, 55]. 4.2. A Kriging model for hierarchical variables In this section, a new method to build a Kriging model with hierar chical variables is introduced based on the framework aforementioned. The proposed methods are included in SMT 2.0. 4.2.1. Motivation and stateoftheart Assuming that the decreed variables are quantitative, Hutter and Osborne [21] proposed several kernels for the hierarchical context. A classic approach, called the imputation method (ImpKernel) leads to an efficient paradigm in practice that can be easily integrated into a more general framework as proposed by Bussemaker et al. [24]. However, this kernel lacks depth and depends on arbitrary choices. Therefore, Hutter and Osborne [21] also proposed a more general kernel, called ArcKernel and Horn et al. [36] generalized this kernel even more and proposed a new formulation called the Wedge Kernel [37]. The drawbacks of these two methods are that they add some extra hyperparameters for every decreed dimension (respectively one extra hyperparameter for the ArcKernel and two hyperparam eters for the WedgeKernel) and that they need a normalization according to the bounds. More recently, Pelamatti et al. [60] developed a hierarchical kernel for Bayesian optimization. However, our work is also more general thanks to the framework introduced earlier [38] that considers variablewise formulation and is more flexible as we are allowing subproblems to be intersecting. In the following, we describe our new method to build a correlation kernel for hierarchical variables. In particular, we introduce a new alge braic kernel called AlgKernel that behaves like the ArcKernel whilst correcting most of its drawbacks. In particular, our kernel does not add any hyperparameters, and the normalization is handled in a natural way. 4.2.2. A new hierarchical correlation kernel For modeling purposes, we assume that the decreed space is quan titative, i.e., dec = qnt dec. Let \ud835\udc62 be an input point partitioned as",
    "Advances in Engineering Software 188 (2024) 103571 8 P. Saves et al. \ud835\udc62 = (\ud835\udc62neu, \ud835\udc62met, \ud835\udc62inc(\ud835\udc62met)) and, similarly, \ud835\udc63 is another input such that \ud835\udc63 = (\ud835\udc63neu, \ud835\udc63met, \ud835\udc63inc(\ud835\udc63met)). The new kernel \ud835\udc58 that we propose for hierarchical variables is given by \ud835\udc58(\ud835\udc62, \ud835\udc63) = \ud835\udc58neu(\ud835\udc62neu, \ud835\udc63neu) \ud835\udc58met(\ud835\udc62met, \ud835\udc63met) \ud835\udc58met,dec([\ud835\udc62met, \ud835\udc62inc(\ud835\udc62met)], [\ud835\udc63met, \ud835\udc63inc(\ud835\udc63met)]), (2) where \ud835\udc58neu, \ud835\udc58met and \ud835\udc58met,dec are as follows: \ud835\udc58neu represents the neutral kernel that encompasses both categor ical and quantitative neutral variables, i.e., \ud835\udc58neu can be decom posed into two parts \ud835\udc58neu(\ud835\udc62neu, \ud835\udc63neu) = \ud835\udc58cat(\ud835\udc62cat neu, \ud835\udc63cat neu)\ud835\udc58qnt(\ud835\udc62qnt neu, \ud835\udc63qnt neu). The categorical kernel, denoted \ud835\udc58cat, could be any Symmetric Positive Definite (SPD) [34] mixed kernel (see Section 3). For the quantitative (integer or continuous) variables, a distance based kernel is used. The chosen quantitative kernel (Exponential, Mat\u00e9rn, ...), always depends on a given distance \ud835\udc51. For example, the \ud835\udc5bdimensional exponential kernel gives \ud835\udc58qnt(\ud835\udc62qnt, \ud835\udc63qnt) = \ud835\udc5b \ud835\udc56=1 exp(\ud835\udc51(\ud835\udc62qnt \ud835\udc56 , \ud835\udc63qnt \ud835\udc56 )). (3) \ud835\udc58met is the meta variables related kernel. It is also separated into two parts: \ud835\udc58met(\ud835\udc62met, \ud835\udc63met) = \ud835\udc58cat(\ud835\udc62cat met, \ud835\udc63cat met)\ud835\udc58qnt(\ud835\udc62qnt met, \ud835\udc63qnt met) where the quantitative kernel is ordered and not continuous because meta variables take value in a finite set. \ud835\udc58met,dec is an SPD kernel that models the correlations between the meta levels (all the possible subspaces) and the decreed variables. In what comes next, we detailed this kernel. 4.2.3. Towards an algebraic metadecreed kernel Metadecreed kernels like the imputation kernel or the ArcKernel were first proposed in [21,47] and the distancebased kernels such as ArcKernel or WedgeKernel [37] were proven to be SPD. Nevertheless, to guarantee this SPD property, the same hyperparameters are used to model the correlations between the meta levels and between the decreed variables [47]. For this reason, the ArcKernel includes additional continuous hyperparameters which makes the training of the GP models more expensive and introduces more numerical stability issues. In this context, we are proposing a new algebraic metadecreed kernel (denoted as AlgKernel) that enjoys similar properties as ArcKernel but without using additional continuous hyperparameters nor rescaling. In the SMT 2.0 release, we included AlgKernel and a simpler version of ArcKernel that do not relies on additional hyperparameters. Our proposed AlgKernel kernel is given by \ud835\udc58alg met,dec([\ud835\udc62met, \ud835\udc62inc(\ud835\udc62met)], [\ud835\udc63met, \ud835\udc63inc(\ud835\udc63met)]) = \ud835\udc58alg met(\ud835\udc62met, \ud835\udc63met) \ud835\udc58alg dec(\ud835\udc62inc(\ud835\udc62met), \ud835\udc63inc(\ud835\udc63met)). (4) Mathematically, we could consider that there is only one meta variable whose levels correspond to every possible included subspace. Let \ud835\udc3csub denotes the components indices of possible subspaces, the subspaces parameterized by the meta component \ud835\udc62met are defined as inc(\ud835\udc62met = \ud835\udc59), \ud835\udc59 \ud835\udc3csub. It follows that the fully extended continuous decreed space writes as dec = \ud835\udc59\ud835\udc3csub inc(\ud835\udc62met = \ud835\udc59) and \ud835\udc3cdec is the set of the associated indices. Let \ud835\udc3c\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f \ud835\udc62,\ud835\udc63 denotes the set of components related to the space inc(\ud835\udc62met, \ud835\udc63met) containing the variables decreedincluded in both inc(\ud835\udc62met) and inc(\ud835\udc63met). Since the decreed variables are quantitative, one has \ud835\udc58alg dec(\ud835\udc62inc(\ud835\udc62met), \ud835\udc63inc(\ud835\udc63met)) = \ud835\udc58qnt(\ud835\udc62inc(\ud835\udc62met), \ud835\udc63inc(\ud835\udc63met)) = \ud835\udc56\ud835\udc3c\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f \ud835\udc62,\ud835\udc63 \ud835\udc58qnt([\ud835\udc62inc(\ud835\udc62met)]\ud835\udc56, [\ud835\udc63inc(\ud835\udc63met)]\ud835\udc56) (5) The construction of the quantitative kernel \ud835\udc58qnt depends on a given distance denoted \ud835\udc51alg. The kernel \ud835\udc58alg met is an induced meta kernel that depends on the same distance \ud835\udc51alg to preserve the SPD property of \ud835\udc58alg met,dec. For every \ud835\udc56 \ud835\udc3cdec, if \ud835\udc56 \ud835\udc3c\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f \ud835\udc62,\ud835\udc63 , the new algebraic distance is given by \ud835\udc51alg([\ud835\udc62inc(\ud835\udc62met)]\ud835\udc56, [\ud835\udc63inc(\ud835\udc63met)]\ud835\udc56) = 2|[\ud835\udc62inc(\ud835\udc62met)]\ud835\udc56 [\ud835\udc63inc(\ud835\udc63met)]\ud835\udc56| [\ud835\udc62inc(\ud835\udc62met)]\ud835\udc56 2 + 1 [\ud835\udc63inc(\ud835\udc63met)]\ud835\udc56 2 + 1 \ud835\udf03\ud835\udc56, (6) where \ud835\udf03\ud835\udc56 R+ is a continuous hyperparameter. Otherwise, if \ud835\udc56 \ud835\udc3cdec but \ud835\udc56 \ud835\udc3c\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f \ud835\udc62,\ud835\udc63 , there should be a nonzero residual distance between the two different subspaces inc(\ud835\udc62met) and inc(\ud835\udc63met) to ensure the kernel SPD property. To have a residual not depending on the decreed values, our model considers that there is a unit distance \ud835\udc51alg([\ud835\udc62inc(\ud835\udc62met)]\ud835\udc56, [\ud835\udc63inc(\ud835\udc63met)]\ud835\udc56) = 1.0 \ud835\udf03\ud835\udc56, \ud835\udc56 \ud835\udc3cdec \ud835\udc3c\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f \ud835\udc62,\ud835\udc63 . The induced meta kernel \ud835\udc58alg met(\ud835\udc62met, \ud835\udc63met) to preserve the SPD property of \ud835\udc58alg is defined as: \ud835\udc58alg met(\ud835\udc62met, \ud835\udc63met) = \ud835\udc56\ud835\udc3cmet \ud835\udc58qnt(1.0 \ud835\udf03\ud835\udc56). (7) Not only our kernel of Eq. (2) uses less hyperparameters than the Arc Kernel (as we cut off its extra parameters) but it is also a more flexible kernel as it allows different kernels for meta and decreed variables. Moreover, another advantage of our kernel is that it is numerically more stable thanks to the new nonstationary [61] algebraic distance defined in Eq. (7) [62]. Our proposed distance also does not need any rescaling by the bounds to have values between 0 and 1. In what comes next, we will refer to the implementation of the kernels ArcKernel and AlgKernel by SMT ArcKernel and SMT AlgKernel. We note also that the implementation of SMT ArcKernel differs slightly from the original ArcKernel as we fixed some hyperparameters to 1 in order to avoid adding extra hy perparameters and use the formulation of Eq. (2) and rescaling of the data. 4.2.4. Illustration on the MLP problem In this section, we illustrate the hierarchical ArcKernel on the MLP example. For that sake, we consider two design variables \ud835\udc62 and \ud835\udc63 such that \ud835\udc62 = (2.104, 0.9, ReLU, 16, 2, 55, 51) and \ud835\udc63 = (5.103, 0.8, Sigmoid, 64, 3, 50, 54, 53). Since the value of \ud835\udc62met (i.e., the number of hidden layers) differs from one point to another (namely, 2 for \ud835\udc62 and 3 for \ud835\udc63), the associated variables \ud835\udc62inc(\ud835\udc62met) have either 2 or 3 variables for the number of neurons in each layer (namely 55 and 51 for \ud835\udc62, and 50, 54 and 53 for the point \ud835\udc63). In our case, 8 hyperparame ters ([\ud835\udc451]1,2, \ud835\udf031, , \ud835\udf037) will have to be optimized where \ud835\udc58 is given by Eq. (2). These 7 hyperparameters can be described using our proposed framework as follows: For the neutral components, we have \ud835\udc62neu = (2.104, 0.9, ReLU, 16) and \ud835\udc63neu = (5.103, 0.8, Sigmoid, 64). Therefore, for a categorical matrix kernel \ud835\udc451 and a square exponential quantitative kernel, \ud835\udc58neu(\ud835\udc62neu, \ud835\udc63neu) = \ud835\udc58cat(\ud835\udc62cat neu, \ud835\udc63cat neu)\ud835\udc58qnt(\ud835\udc62qnt neu, \ud835\udc63qnt neu) = [\ud835\udc451]1,2 exp [\ud835\udf031(2.104 5.103)2] exp [\ud835\udf032(0.9 0.8)2] exp [\ud835\udf033(16 64)2]. The values [\ud835\udc451]1,2, \ud835\udf031, \ud835\udf032 and \ud835\udf033 need to be optimized. Here, [\ud835\udc451]1,2 is the correlation between \"ReLU\" and \"Sigmoid\". For the meta components, we have \ud835\udc62met = 2 and \ud835\udc63met = 3. Therefore, for a square exponential quantitative kernel, \ud835\udc58met(\ud835\udc62met, \ud835\udc63met) = \ud835\udc58cat(\ud835\udc62cat met, \ud835\udc63cat met)\ud835\udc58qnt(\ud835\udc62qnt met, \ud835\udc63qnt met) = exp [\ud835\udf034(3 2)2]. The value \ud835\udf034 needs to be optimized.",
    "Advances in Engineering Software 188 (2024) 103571 9 P. Saves et al. For the metadecreed kernel, we have [\ud835\udc62met, \ud835\udc62inc(\ud835\udc62met)] = [2, (55, 51)] and [\ud835\udc63met, \ud835\udc63inc(\ud835\udc63met)] = [3, (50, 54, 53)] which gives \ud835\udc58alg met,dec([\ud835\udc62met, \ud835\udc62inc(\ud835\udc62met)], [\ud835\udc63met, \ud835\udc63inc(\ud835\udc63met)]) = \ud835\udc58alg met(2, 3) \ud835\udc58alg dec((55, 51), (50, 54, 53)). The distance \ud835\udc51alg(51, 54) = ( 2|5154| 512+1 542+1 ) \ud835\udf036 = 2.178.103 \ud835\udf036. In general, for surrogate models, and in particular in SMT 2.0, the input data are normalized. With a unit normalization from [50, 55] to [0, 1], we would have \ud835\udc51alg(0.2, 0.8) = ( 20.6 0.22+1 0.62+1 ) \ud835\udf036 = 0.919 \ud835\udf036. Similarly, we have, between 55 and 50, \ud835\udc51alg(0, 1) = 1.414 \ud835\udf035. Hence, for a square exponential quantitative kernel, one gets \ud835\udc58alg met,dec([\ud835\udc62met, \ud835\udc62inc(\ud835\udc62met)], [\ud835\udc63met, \ud835\udc63inc(\ud835\udc63met)]) = exp [\ud835\udf037] exp [1.414 \ud835\udf035] exp [0.919 \ud835\udf036], where the meta induced component is \ud835\udc58alg met(\ud835\udc62met, \ud835\udc63met) = exp [\ud835\udf037] because the decreed value 53 in \ud835\udc63 has nothing to be compared with in \ud835\udc62 as in Eq. (7). The values \ud835\udf035, \ud835\udf036 and \ud835\udf037 need to be opti mized which complete the description of the hyperparameters. We note that for the MLP problem, AlgKernel models use 10 hyperparameters whereas the ArcKernel would require 12 hyperparameters without the meta kernel (\ud835\udf034) but with 3 extra decreed hyperparameters and the WedgeKernel would require 15 hyperparameters. For deep learning applications, a more complex perceptron with up to 10 hidden layers would require 17 hyperparameters with SMT 2.0 models against 26 for ArcKernel and 36 for WedgeKernel. The next section illustrates the interest of our method to build a surrogate model for this neural network engineering problem. 4.3. A neural network testcase using SMT 2.0 In this section, we apply our models to the hyperparameters opti mization of a MLP problem aforementioned of Fig. 4. Within SMT 2.0 an example illustrates this MLP problem. For the sake of showing the Kriging surrogate abilities, we implemented a dummy function with no significance to replace the real blackbox that would require training a whole Neural Network (NN) with big data. This function requires a number of variables that depends on the value of the meta variable, i.e the number of hidden layers. To simplify, we have chosen only 1, 2 or 3 hidden layers and therefore, we have 3 decreed variables but deep neural networks could also be investigated as our model can tackle a few dozen variables. A test case (test_hierarchical_variables_NN) shows that our model SMT AlgKernel interpolates the data prop erly, checks that the data dimension is correct and also asserts that the inactive decreed variables have no influence over the prediction. In Fig. 5 we illustrate the usage of Kriging surrogates with hierarchical and mixed variables based on the implementation of SMT 2.0 for test_hierarchical_variables_NN. To compare the hierarchical models of SMT 2.0 (SMT AlgKernel and SMT ArcKernel) with the stateoftheart imputation method previously used on industrial application (ImpKernel) [24], we draw a 99 point LHS (33 points by meta level) as a training set and the validation set is a LHS of 31000 = 3000 points. For the ImpKernel, the decreedexcluded values are replaced by 52 because the mean value 52.5 is not an integer (by default, SMT rounds to the floor value). For the three methods, the precision (computed with a rootmean square error RMSE criterion), the likelihood and the computational time are shown in Table 6. For this problem, we can see that SMT Alg kernel gives better performance than the imputation method or SMT Arckernel. Also, as all methods use the same number of hyperpa rameters, they have similar time performances. A direct application of Table 6 Results on the neural network model. Hierarchical method Prediction error (RMSE) Likelihood # of hyperparam. SMT Algkernel 3.7610 176.11 10 SMT Arckernel 4.9208 162.01 10 ImpKernel 4.5455 170.64 10 our modeling method is Bayesian optimization to perform quickly the hyperparameter optimization of a neural network [63]. 5. Bayesian optimization within SMT 2.0 Efficient global optimization (EGO) is a sequential Bayesian op timization algorithm designed to find the optimum of a blackbox function that may be expensive to evaluate [52]. EGO starts by fitting a Kriging model to an initial DoE, and then uses an acquisition function to select the next point to evaluate. The most used acquisition function is the expected improvement. Once a new point has been evaluated, the Kriging model is updated. Successive updates increase the model accu racy over iterations. This enrichment process repeats until a stopping criterion is met. Because SMT 2.0 implements Kriging models that handle mixed and hierarchical variables, we can use EGO to solve problems in volving such design variables. Other Bayesian optimization algorithms often adopt approaches based on solving subproblems with contin uous or nonhierarchical Kriging. This subproblem approach is less efficient and scales poorly, but it can only solve simple problems. Several Bayesian optimization software packages can handle mixed or hierarchical variables with such a subproblem approach. The pack ages include BoTorch [25], SMAC [65], Trieste [66], HEBO [67], OpenBox [68], and Dragonfly [69]. 5.1. A mixed optimization problem Fig. 6 compares the four EGO methods implemented in SMT 2.0: SMT GD, SMT CR, SMT EHH and SMT HH. The mixed test case that illustrates Bayesian optimization is a toy test case [64] detailed in Ap pendix A. This test case has two variables, one continuous and one categorical with 10 levels. To assess the performance of our algorithm, we performed 20 runs with different initial DoE sampled by LHS. Every DoE consists of 5 points and we chose a budget of 55 infill points. Fig. 6(a) plots the convergence curves for the four methods. In particular, the median is the solid line, and the first and third quantiles are plotted in dotted lines. To visualize better the data dispersion, the boxplots of the 20 best solutions after 20 evaluations are plotted in Fig. 6(b). As expected, the more a method is complex, the better the optimization. Both SMT HH and SMT EHH converged in around 18 evaluations whereas SMT CR and SMT GD take around 26 iterations as shown on Fig. 6(a). Also, the more complex the model, the closer the optimum is to the real value as shown on Fig. 6(b). In Fig. 7 we illustrate how to use EGO with mixed variables based on the implementation of SMT 2.0. The illustrated problem is a mixed variant of the Branin function [70]. Note that a dedicated notebook is available to reproduce the results presented in this paper and the mixed integer notebook also includes an extra mechanical application with composite materials [41].11 11 https://colab.research.google.com/github/SMTorg/smt/blob/master/ tutorial/SMT_MixedInteger_application.ipynb",
    "Advances in Engineering Software 188 (2024) 103571 10 P. Saves et al. Fig. 5. Example of usage of Hierarchical and Mixed Kriging surrogate.",
    "Advances in Engineering Software 188 (2024) 103571 11 P. Saves et al. Fig. 6. Optimization results for the Toy function [64]. Fig. 7. Example of usage of mixed surrogates for Bayesian optimization.",
    "Advances in Engineering Software 188 (2024) 103571 12 P. Saves et al. Fig. 8. Optimization results for the hierarchical Goldstein function. 5.2. A hierarchical optimization problem The hierarchical test case considered in this paper to illustrate Bayesian optimization is a modified Goldstein function [60] detailed in Appendix B. The resulting optimization problem involves 11 vari ables: 5 are continuous, 4 are integer (ordinal) and 2 are categorical. These variables consist in 6 neutral variables, 1 dimensional (or meta) variable and 4 decreed variables. Depending on the meta variable values, 4 different subproblems can be identified. The optimization problem is given by: min \ud835\udc53(\ud835\udc65cat neu, \ud835\udc65qnt neu, \ud835\udc65cat \ud835\udc5a , \ud835\udc65qnt dec) w.r.t. \ud835\udc65cat neu = \ud835\udc642 {0, 1} \ud835\udc65qnt neu = (\ud835\udc651, \ud835\udc652, \ud835\udc655, \ud835\udc673, \ud835\udc674) {0, 100}3 {0, 1, 2}2 \ud835\udc65cat \ud835\udc5a = \ud835\udc641 {0, 1, 2, 3} \ud835\udc65qnt dec = (\ud835\udc653, \ud835\udc654, \ud835\udc671, \ud835\udc672) {0, 100}2 {0, 1, 2}2 (8) Compared to the model choice of Pelamatti et al. [60], we chose to model \ud835\udc655 and \ud835\udc642 as neutral variables even if \ud835\udc53 does not depend on \ud835\udc655 when \ud835\udc642 = 0. Other modeling choices are kept; for example, \ud835\udc642 is a socalled binary variable and not a categorical one [71]. Similarly, we also keep the formulation of \ud835\udc641 as a categorical variable but a better model would be to model it as a cyclic variable [72]. The resulting problem is described in Appendix B. To assess the performance of our algorithm, we performed 20 runs with different initial DoE sampled by LHS. Every DoE consists of \ud835\udc5b + 1 = 12 points and we chose a budget of 5\ud835\udc5b = 55 infill points. To compare our method with a baseline, we also tested the random search method thanks to the expand_lhs new method [40] described in Section 6.1 and we also optimized the Goldstein function using EGO with a classic Kriging model based on imputation method (ImpKernel). This method replaces the decreed excluded variables by their mean values: 50 or 1 respectively for (\ud835\udc653, \ud835\udc654) and (\ud835\udc671, \ud835\udc672). Fig. 8(a) plots the convergence curves for the four methods. In particular, the median is the solid line and the first and third quantiles are plotted in dotted lines. To visualize better the correspond ing data dispersion, the boxplots of the 20 best solutions are plotted in Fig. 8(b). The results in Fig. 8 show that the hierarchical Kriging models of SMT 2.0 lead to better results than the imputation method or the random search both in terms of final objective value and variance over the 20 runs and in term of convergence rate. More precisely, SMT ArcKernel and SMT AlgKernel Kriging model gave the best EGO results and managed to converge correctly as shown in Fig. 8(b). More precisely, the 20 sampled DoEs led to similar performance and from one DoE, the method SMT AlgKernel managed to find the true minimum. However, this result has not been reproduced in other runs and is therefore not statistically significant. The variance between the runs is of similar magnitude regardless of the considered methods. 6. Other relevant contributions in SMT 2.0 The new release SMT 2.0 introduces several improvements be sides Kriging for hierarchical and mixed variables. This section details the most important new contributions. Recall from Section 2.2 that five submodules are present in the code: Sampling, Problems, Surrogate Models, Applications and Notebooks. 6.1. Contributions to Sampling Pseudorandom sampling. The Latin Hypercube Sampling (LHS) is a stochastic sampling technique to generate quasirandom sampling dis tributions. It is among the most popular sampling method in computer experiments thanks to its simplicity and projection properties with highdimensional problems. The LHS method uses the pyDOE package (Design Of Experiments for Python). Five criteria for the construction of LHS are implemented in SMT. The first four criteria (center, maximin, centermaximin, correlation) are the same as in pyDOE.12 The last criterion ese, is implemented by the authors of SMT [48]. In SMT 2.0 a new LHS method was developed for the Nested design of experiments (NestedLHS) [73] to use with multi fidelity surrogates. A new mathematical method (expand_lhs) [40] was developed in SMT 2.0 to increase the size of a design of exper iments while maintaining the ese property. Moreover, we proposed a sampling method for mixed variables, and the aforementioned LHS method was applied to hierarchical variables in Fig. 8. 6.2. Contributions to Surrogate models New kernels and their derivatives for Kriging. Kriging surrogates are based on hyperparameters and on a correlation kernel. Four correla tion kernels are now implemented in SMT 2.0 [74]. In SMT, these correlation functions are absolute exponential (abs_exp), Gaussian (squar_exp), Matern 5/2 (matern52) and Matern 3/2 (matern32). In addition, the implementation of gradient and Hessian for each kernel makes it possible to calculate both the first and second derivatives of the GP likelihood with respect to the hyperparameters [5]. Variance derivatives for Kriging. To perform uncertainty quantification for system analysis purposes, it could be interesting to know more about the variance derivatives of a model [7577]. For that purpose and also to pursue the original publication about derivatives [5], SMT 2.0 extends the derivative support to Kriging variances and kernels. 12 https://pythonhosted.org/pyDOE/index.html",
    "Advances in Engineering Software 188 (2024) 103571 13 P. Saves et al. Noisy Kriging. In engineering and in big data contexts with real exper iments, surrogate models for noisy data are of significant interest. In particular, there is a growing need for techniques like noisy Kriging and noisy MultiFidelity Kriging (MFK) for data fusion [78]. For that purpose, SMT 2.0 has been designed to accommodate Kriging and MFK to noisy data including the option to incorporate heteroscedastic noise (using the use_het_noise option) and to account for different noise levels for each data source [40]. Kriging with partial least squares. Beside MGP, for highdimensional problems, the toolbox implements Kriging with partial least squares (KPLS) [57] and its extension KPLSK [44]. The PLS information is computed by projecting the data into a smaller space spanned by the principal components. By integrating this PLS information into the Kriging correlation matrix, the number of inputs can be scaled down, thereby reducing the number of hyperparameters required. The result ing number of hyperparameters \ud835\udc51\ud835\udc52 is indeed much smaller than the original problem dimension \ud835\udc51. Recently, in SMT 2.0, we extended the KPLS method for multifidelity Kriging (MFKPLS and MFKPLSK) [73,79, 80]. We also proposed an automatic criterion to choose automatically the reduced dimension \ud835\udc51\ud835\udc52 based on Wolds R criterion [81]. This criterion has been applied to aircraft optimization with EGO where the number \ud835\udc51\ud835\udc52 (\ud835\ude97_\ud835\ude8c\ud835\ude98\ud835\ude96\ud835\ude99 in the code) for the model is automatically selected at every iteration [39]. Special efforts have been made to accommodate KPLS for multifidelity and mixed integer data [79,80]. Marginal Gaussian process. SMT 2.0 implements Marginal Gaussian Process (MGP) surrogate models for high dimensional problems [82]. MGP are Gaussian processes taking into account hyperparameters un certainty defined as a density probability function. Especially we sup pose that the function to model \ud835\udc53 \ud835\udefa R, where \ud835\udefa R\ud835\udc51 and \ud835\udc51 is the number of design variables, lies in a linear embedding such as = {\ud835\udc62 = \ud835\udc34\ud835\udc65, \ud835\udc65 \ud835\udefa}, \ud835\udc34 R\ud835\udc51\ud835\udc51\ud835\udc52 and \ud835\udc53(\ud835\udc65) = \ud835\udc53(\ud835\udc34\ud835\udc65) with \ud835\udc53(\ud835\udc65) = \ud835\udc53 R and \ud835\udc51\ud835\udc52 \ud835\udc51. Then, we must use a kernel \ud835\udc58(\ud835\udc65, \ud835\udc65) = \ud835\udc58(\ud835\udc34\ud835\udc65, \ud835\udc34\ud835\udc65) whose each component of the transfer matrix \ud835\udc34 is an hyperparameter. Thus we have \ud835\udc51\ud835\udc52 \ud835\udc51 hyperparameters to find. Note that \ud835\udc51\ud835\udc52 is defined as \ud835\ude97_\ud835\ude8c\ud835\ude98\ud835\ude96\ud835\ude99 in the code [49]. Gradientenhanced neural network. The new release SMT 2.0 imple ments GradientEnhanced Neural Network (GENN) models [45]. GradientEnhanced Neural Networks (GENN) are fully connected multi layer perceptrons whose training process was modified to account for gradient information. Specifically, the model is trained to minimize not only the prediction error of the response but also the prediction error of the partial derivatives: the chief benefit of gradient enhancement is better accuracy with fewer training points. Note that GENN applies to regression (singleoutput or multioutput), but not classification since there is no gradient in that case. The implementation is fully vectorized and uses ADAM optimization, minibatch, and L2norm regularization. For example, GENN can be used to learn airfoil geometries from a database. This usage is documented in SMT 2.0.13 6.3. Contributions to Applications Kriging trajectory and sampling. Sampling a GP with high resolution is usually expensive due to the large dimension of the associated covariance matrix. Several methods are proposed to draw samples of a GP on a given set of points. To sample a conditioned GP, the classic method consists in using a Cholesky decomposition (or eigende composition) of the conditioned covariance matrix of the process but some numerical computational errors can lead to non SPD matrix. A more recent approach based on KarhunenLo\u00e8ve decomposition of the covariance kernel with the Nystr\u00f6m method has been proposed in [83] 13 https://smt.readthedocs.io/en/latest/_src_docs/examples/airfoil_ parameters/learning_airfoil_parameters.html where the paths can be sampled by generating independent standard Normal distributed samples. The different methods are documented in the tutorial Gaussian Process Trajectory Sampling [84]. Parallel Bayesian optimization. Due to the recent progress made in hardware configurations, it has been of high interest to perform parallel optimizations. A parallel criterion called qEI [85] was developed to perform Efficient Global Optimization (EGO): the goal is to be able to run batch optimization. At each iteration of the algorithm, multiple new sampling points are extracted from the known ones. These new sampling points are then evaluated using a parallel computing environ ment. Five criteria are implemented in SMT 2.0: Kriging Believer (KB), Kriging Believer Upper Bound (KBUB), Kriging Believer Lower Bound (KBLB), Kriging Believer Random Bound (KBRand) and Constant Liar (CLmin) [86]. 7. Conclusion SMT 2.0 introduces significant upgrades to the Surrogate Modeling Toolbox. This new release adds support for hierarchical and mixed variables and improves the surrogate models with a particular focus on Kriging (Gaussian process) models. SMT 2.0 is distributed through an opensource license and is freely available online.14 We provide documentation that caters to both users and potential developers.15 SMT 2.0 enables users and developers collaborating on the same project to have a common surrogate modeling tool that facilitates the exchange of methods and reproducibility of results. SMT has been widely used in aerospace and mechanical modeling applications. Moreover, the toolbox is general and can be useful for anyone who needs to use or develop surrogate modeling techniques, regardless of the targeted applications. SMT is currently the only open source toolbox that can build hierarchical and mixed surrogate models. Declaration of competing interest The authors declare that they have no known competing finan cial interests or personal relationships that could have appeared to influence the work reported in this paper. Data availability Data will be made available on request. Results can be reproduced freely online at https://colab.research.google.com/github/SMTorg/smt/ blob/master/tutorial/NotebookRunTestCases_Paper_SMT_v2.ipynb. Acknowledgments We want to thank all those who contribute to this release. Namely, M. A. Bouhlel, I. Cardoso, R. Carreira Rufato, R. Charayron, R. Conde Arenzana, S. Dubreuil, A. F. L\u00f3pezLopera, M. Meliani, M. Menz, N. Mo\u00ebllo, A. Thouvenot, R. Priem, E. Roux and F. Vergnes. This work is part of the activities of ONERA ISAE ENAC joint research group. We also acknowledge the partners institutions: ONERA, NASA Glenn, ISAE SUPAERO, Institut Cl\u00e9ment Ader (ICA), the University of Michigan, Polytechnique Montr\u00e9al and the University of California San Diego. The research presented in this paper has been performed in the framework of the AGILE 4.0 project (Towards cyberphysical collabo rative aircraft development), funded by the European Union Horizon 2020 research and innovation framework programme under grant agreement n 815122 and in the COLOSSUS project (Collaborative System of Systems Exploration of Aviation Products, Services and 14 https://github.com/SMTorg/SMT 15 https://smt.readthedocs.io/en/latest/",
    "Advances in Engineering Software 188 (2024) 103571 14 P. Saves et al. Business Models) funded by the European Union Horizon Europe re search and innovation framework programme under grant agreement n 101097120. We also are grateful to E. Hall\u00e9Hannan from Polytechnique Mon tr\u00e9al for the hierarchical variables framework. Appendix A. Toy test function This Appendix gives the detail of the toy function of Section 5.1.16 First, we recall the optimization problem: min \ud835\udc53(\ud835\udc65cat, \ud835\udc65qnt) w.r.t. \ud835\udc65cat = \ud835\udc501 {0, 1, 2, 3, 4, 5, 6, 7, 8, 9} \ud835\udc65qnt = \ud835\udc651 [0, 1] (A.1) The toy function \ud835\udc53 is defined as \ud835\udc53(\ud835\udc65, \ud835\udc501) =1\ud835\udc501=0 cos(3.6\ud835\udf0b(\ud835\udc65 2)) + \ud835\udc65 1 +1\ud835\udc501=1 2 cos(1.1\ud835\udf0b exp(\ud835\udc65)) \ud835\udc65 2 + 2 +1\ud835\udc501=2 cos(2\ud835\udf0b\ud835\udc65) + 1 2\ud835\udc65 +1\ud835\udc501=3 \ud835\udc65(cos(3.4\ud835\udf0b(\ud835\udc65 1)) \ud835\udc65 1 2 ) +1\ud835\udc501=4 \ud835\udc652 2 +1\ud835\udc501=5 2 cos(0.25\ud835\udf0b exp(\ud835\udc654))2 \ud835\udc65 2 + 1 +1\ud835\udc501=6 \ud835\udc65 cos(3.4\ud835\udf0b\ud835\udc65) \ud835\udc65 2 + 1 +1\ud835\udc501=7 \ud835\udc65(cos(3.5\ud835\udf0b\ud835\udc65) + \ud835\udc65 2 ) + 2 +1\ud835\udc501=8 \ud835\udc655 2 + 1 +1\ud835\udc501=9 cos(2.5\ud835\udf0b\ud835\udc65)2 \ud835\udc65 0.5 ln(\ud835\udc65 + 0.5) 1.3 (A.2) Appendix B. Hierarchical Goldstein test function This Appendix gives the detail of the hierarchical Goldstein problem of Section 5.2.17 First, we recall the optimization problem: min \ud835\udc53(\ud835\udc65cat neu, \ud835\udc65qnt neu, \ud835\udc65cat \ud835\udc5a , \ud835\udc65qnt dec) w.r.t. \ud835\udc65cat neu = \ud835\udc642 {0, 1} \ud835\udc65qnt neu = (\ud835\udc651, \ud835\udc652, \ud835\udc655, \ud835\udc673, \ud835\udc674) [0, 100]3 {0, 1, 2}2 \ud835\udc65cat \ud835\udc5a = \ud835\udc641 {0, 1, 2, 3} \ud835\udc65qnt dec = (\ud835\udc653, \ud835\udc654, \ud835\udc671, \ud835\udc672) [0, 100]2 {0, 1, 2}2 (B.1) The hierarchical and mixed function \ud835\udc53 is defined as a hierarchical function that depends on \ud835\udc530, \ud835\udc531, \ud835\udc532 and \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont as describes in the following. \ud835\udc53(\ud835\udc651, \ud835\udc652, \ud835\udc653, \ud835\udc654, \ud835\udc671, \ud835\udc672, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc641, \ud835\udc642) = 1\ud835\udc641=0\ud835\udc530(\ud835\udc651, \ud835\udc652, \ud835\udc671, \ud835\udc672, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) + 1\ud835\udc641=1\ud835\udc531(\ud835\udc651, \ud835\udc652, \ud835\udc653, \ud835\udc672, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) + 1\ud835\udc641=2\ud835\udc532(\ud835\udc651, \ud835\udc652, \ud835\udc654, \ud835\udc671, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) + 1\ud835\udc641=3\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, \ud835\udc653, \ud835\udc654, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642). (B.2) 16 https://github.com/jbussemaker/SBArchOpt 17 https://github.com/jbussemaker/SBArchOpt Then, the functions \ud835\udc530, \ud835\udc531 and \ud835\udc532 are defined as mixed variants of \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont as such \ud835\udc530(\ud835\udc651, \ud835\udc652, \ud835\udc671, \ud835\udc672, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) = 1\ud835\udc672=0 ( 1\ud835\udc671=0\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 20, 20, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) + 1\ud835\udc671=1\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 50, 20, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) + 1\ud835\udc671=2\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 80, 20, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) ) 1\ud835\udc672=1 ( 1\ud835\udc671=0\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 20, 50, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) + 1\ud835\udc671=1\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 50, 50, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) + 1\ud835\udc671=2\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 80, 50, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) ) 1\ud835\udc672=2 ( 1\ud835\udc671=0\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 20, 80, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) + 1\ud835\udc671=1\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 50, 80, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) + 1\ud835\udc671=2\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 80, 80, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) ) (B.3) \ud835\udc531(\ud835\udc651, \ud835\udc652, \ud835\udc653, \ud835\udc672, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) = 1\ud835\udc672=0\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, \ud835\udc653, 20, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) + 1\ud835\udc672=1\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, \ud835\udc653, 50, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) + 1\ud835\udc672=2\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, \ud835\udc653, 80, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) \ud835\udc532(\ud835\udc651, \ud835\udc652, \ud835\udc654, \ud835\udc671, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) = 1\ud835\udc671=0\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 20, \ud835\udc654, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) + 1\ud835\udc671=1\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, 50, \ud835\udc652, \ud835\udc654, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) + 1\ud835\udc671=2\ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, 80, \ud835\udc654, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) To finish with, the function \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont is given by \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont(\ud835\udc651, \ud835\udc652, \ud835\udc653, \ud835\udc654, \ud835\udc673, \ud835\udc674, \ud835\udc655, \ud835\udc642) = 53.3108 + 0.184901\ud835\udc651 5.02914\ud835\udc651 3.106 + 7.72522\ud835\udc651 \ud835\udc673.108 0.0870775\ud835\udc652 0.106959\ud835\udc653 + 7.98772\ud835\udc653 \ud835\udc674.106 + 0.00242482\ud835\udc654 + 1.32851\ud835\udc654 3.106 0.00146393\ud835\udc651\ud835\udc652 0.00301588\ud835\udc651\ud835\udc653 0.00272291\ud835\udc651\ud835\udc654 + 0.0017004\ud835\udc652\ud835\udc653 + 0.0038428\ud835\udc652\ud835\udc654 0.000198969\ud835\udc653\ud835\udc654 + 1.86025\ud835\udc651\ud835\udc652\ud835\udc653.105 1.88719\ud835\udc651\ud835\udc652\ud835\udc654.106 + 2.50923\ud835\udc651\ud835\udc653\ud835\udc654.105 5.62199\ud835\udc652\ud835\udc653\ud835\udc654.105 + \ud835\udc642 ( 5 cos ( 2\ud835\udf0b 100 \ud835\udc655 ) 2 ) . (B.4) Appendix C. Supplementary data More at https://colab.research.google.com/github/SMTorg/smt/blob/ master/tutorial/NotebookRunTestCases_Paper_SMT_v2.ipynb. Supplementary material related to this article can be found online at https://doi.org/10.1016/j.advengsoft.2023.103571. References [1] Mader CA, Martins JRRA, Alonso JJ, van der Weide E. ADjoint: An approach for the rapid development of discrete adjoint solvers. AIAA J 2008;46:86373. [2] Kennedy M, OHagan A. Bayesian calibration of computer models. J R Stat Soc Ser B Stat Methodol 2001;63:42564. [3] Hwang JT, Martins JRRA. A fastprediction surrogate model for large datasets. Aerosp Sci Technol 2018;75:7487. [4] Martins JRRA, Ning A. Engineering design optimization. Cambridge University Press; 2021. [5] Bouhlel MA, Hwang JT, Bartoli N, Lafage R, Morlier J, Martins JRA. A Python surrogate modeling framework with derivatives. Adv Eng Softw 2019;135:102662. [6] Bouhlel MA, Martins J. Gradientenhanced kriging for highdimensional problems. Eng Comput 2019;35:15773. [7] Pedregosa F, Varoquaux G, Gramfort A, Thirion VMB, Grisel O, et al. Scikitlearn: Machine learning in Python. J Mach Learn Res 2011;12:282530.",
    "Advances in Engineering Software 188 (2024) 103571 15 P. Saves et al. [8] Lataniotis C, Marelli S, Sudret B. Uqlab 2.0 and uqcloud: opensource vs. cloudbased uncertainty quantification. In: SIAM conference on uncertainty quantification. 2022. [9] Faraci A, Beaurepaire P, Gayton N. Review on Python toolboxes for Kriging surrogate modelling. In: ESREL. 2022. [10] Kr\u00fcgener M, Zapata Usandivaras J, Bauerheim M, Urbano A. Coaxialinjector surrogate modeling based on Reynoldsaveraged NavierStokes simulations using deep learning. J Propuls Power 2022;38:78398. [11] Ming D, Williamson D, Guillas S. Deep Gaussian process emulation using stochastic imputation. Technometrics 2022;112. [12] Eli\u00e1\u0161 J, Vo\u0159echovsky M, Sad\u00edlekv V. Periodic version of the minimax distance criterion for Monte Carlo integration. Adv Eng Softw 2020;149:102900. [13] Drouet V, Balesdent M, Brevault L, Dubreuil S, Morio J. Multifidelity algo rithm for the sensitivity analysis of multidisciplinary problems. J Mech Des 2023;145:122. [14] Karban P, P\u00e1nek D, Orosz T, Petr\u00e1\u0161ov\u00e1 I, Dole\u017eel I. FEM based robust design optimization with Agros and Artap. Comput Math Appl 2021;81:61833. [15] Kudela J, Matousek R. Recent advances and applications of surrogate models for finite element method computations: a review. Soft Comput 2022;26:1370933. [16] Chen Y, Dababneh F, Zhang B, Kassaee S, Smith BT, Liu K, et al. Surrogate mod eling for capacity planning of charging station equipped with photovoltaic panel and hydropneumatic energy storage. J Energy Res Technol 2020;142:050907. [17] Jasa J, Bortolotti P, Zalkind D, Barter G. Effectively using multifidelity optimization for wind turbine design. Wind Energy Sci 2022;7:9911006. [18] Wang W, Tao G, Ke D, Luo J, Cui J. Transpiration cooling of high pres sure turbine vane with optimized porosity distribution. Appl Therm Eng 2023;223:119831. [19] Savage T, AlmeidaTrasvina HF, del R\u00edoChanona EA, Smith R, Zhang D. An adaptive datadriven modelling and optimization framework for complex chemical process design. Comput Aided Chem Eng 2020;48:738. [20] Chan A, Pires AF, Polacsek T. Trying to elicit and assign goals to the right actors. In: Conceptual modeling: 41st international conference. 2022. [21] Hutter F, Osborne MA. A kernel for hierarchical parameter spaces. 2013, arXiv. [22] Bussemaker JH, Ciampa PD, Nagel B. System architecture design space explo ration: An approach to modeling and optimization. In: AIAA aviation 2020 forum. 2020. [23] Fouda MEA, Adler EJ, Bussemaker J, Martins JRRA, Kurtulus DF, Boggero L, et al. Automated hybrid propulsion model construction for conceptual aircraft design and optimization. In: 33rd congress of the international council of the aeronautical sciences. 2022. [24] Bussemaker JH, Bartoli N, Lefebvre T, Ciampa PD, Nagel B. Effectiveness of surrogatebased optimization algorithms for system architecture optimization. In: AIAA aviation 2021 forum. 2021. [25] Balandat M, Karrer B, Jiang D, Daulton S, Letham B, Wilson A, et al. BoTorch: A framework for efficient MonteCarlo Bayesian optimization. Adv Neural Inf Process Syst 2020;33:2152438. [26] Adams B, Bohnhoff W, Dalbey K, Ebeida M, Eddy J, Eldred M, et al. Dakota, a multilevel parallel objectoriented framework for design optimization, pa rameter estimation, uncertainty quantification, and sensitivity analysis: Version 6.13 users manual. Technical report, Albuquerque, NM (United States: Sandia National Lab.(SNLNM); 2020. [27] Roustant O, Ginsbourger D, Deville Y. DiceKriging, DiceOptim: Two R packages for the analysis of computer experiments by Krigingbased metamodeling and optimization. J Stat Softw 2012;51:155. [28] Zhang Y, Tao S, Chen W, Apley D. A latent variable approach to Gaus sian process modeling with qualitative and quantitative factors. Technometrics 2020;62:291302. [29] Chang TH, Wild SM. ParMOO: A Python library for parallel multiobjective simulation optimization. J Open Source Softw 2023;8:4468. [30] GarridoMerch\u00e1n EC, Hern\u00e1ndezLobato D. Dealing with categorical and integervalued variables in Bayesian optimization with Gaussian processes. Neurocomputing 2020;380:2035. [31] Halstrup M. Blackbox optimization of mixed discretecontinuous optimization problems (Ph.D. thesis), TU Dortmund; 2016. [32] Roustant O, Padonou E, Deville Y, Cl\u00e9ment A, Perrin G, Giorla J, et al. Group kernels for gaussian process metamodels with categorical inputs. SIAM J Uncertain Quant 2020;8:775806. [33] Zhou Q, Qian PZG, Zhou S. A simple approach to emulation for computer models with qualitative and quantitative factors. Technometrics 2011;53:26673. [34] Saves P, Diouane Y, Bartoli N, Lefebvre T, Morlier J. A mixedcategorical correlation kernel for Gaussian process. Neurocomputing 2023;550:126472. [35] Pelamatti J, Brevault L, Balesdent M, Talbi EG, Guerin Y. Efficient global optimization of constrained mixed variable problems. J Global Optim 2019;73:583613. [36] Horn D, Stork J, ler NJS, Zaefferer M. Surrogates for hierarchical search spaces: The WedgeKernel and an automated analysis. In: Proceedings of the genetic and evolutionary computation conference. 2019. [37] Hung Y, Joseph VR, Melkote SN. Design and analysis of computer experiments with branching and nested factors. Technometrics 2009;51:35465. [38] Audet C, Hall\u00e9Hannan E, Le Digabel S. A general mathematical framework for constrained mixedvariable blackbox optimization problems with meta and categorical variables. Oper Res Forum 2023;4:137. [39] Saves P, Nguyen Van E, Bartoli N, Diouane Y, Lefebvre T, David C, Defoort S, Morlier J. Bayesian optimization for mixed variables using an adaptive dimension reduction process: applications to aircraft design. In: AIAA scitech 2022. 2022. [40] Conde Arenzana R, L\u00f3pezLopera A, Mouton S, Bartoli N, Lefebvre T. Multi fidelity Gaussian process model for CFD and wind tunnel data fusion. In: ECCOMAS aerobest. 2021. [41] Rufato RC, Diouane Y, Henry J, Ahlfeld R, Morlier J. A mixedcategorical datadriven approach for prediction and optimization of hybrid discontinuous composites performance. In: AIAA aviation 2022 forum. 2022. [42] Gorissen D, Crombecq K, Couckuyt I, Dhaene T, Demeester P. A surrogate modeling and adaptive sampling toolbox for computer based design. J Mach Learn Res 2010;11:20515. [43] Williams CK, Rasmussen CE. Gaussian processes for machine learning. MA: MIT press Cambridge; 2006. [44] Bouhlel MA, Bartoli N, Regis R, Otsmane A, Morlier J. Efficient Global Opti mization for highdimensional constrained problems by using the Kriging models combined with the Partial Least Squares method. Eng Optim 2018;50:203853. [45] Bouhlel MA, He S, Martins J. Scalable gradientenhanced artificial neural networks for airfoil shape design in the subsonic and transonic regimes. Struct Multidiscip Optim 2020;61:136376. [46] Kwan LS, Pitrou A, Seibert S. Numba: A LLVMbased python JIT compiler. In: Proceedings of the second workshop on the LLVM compiler infrastructure in HPC. 2015. [47] Zaefferer M, Horn D. A first analysis of kernels for Krigingbased optimization in hierarchical search spaces. 2018, arXiv. [48] Jin R, Chen W, Sudjianto A. An efficient algorithm for constructing optimal design of computer experiments. J Statist Plann Inference 2005;2:54554. [49] Garnett R, Osborne M, Hennig P. Active learning of linear embeddings for Gaussian processes. In: Uncertainty in artificial intelligence Proceedings of the 30th conference. 2013. [50] Jones D. A taxonomy of global optimization methods based on response surfaces. J Global Optim 2001;21:34583. [51] Lafage R. egobox, a Rust toolbox for efficient global optimization. J Open Source Softw 2022;7:4737. [52] Jones DR, Schonlau M, Welch WJ. Efficient global optimization of expensive blackbox functions. J Global Optim 1998;13:45592. [53] Deng X, Lin CD, Liu K, Rowe RK. Additive Gaussian process for computer models with qualitative and quantitative factors. Technometrics 2017;59:28392. [54] CuestaRamirez J, Le Riche R, Roustant O, Perrin G, Durantin C, Gliere A. A comparison of mixedvariables Bayesian optimization approaches. Adv Model Simul Eng Sci 2021;9:129. [55] Rebonato R, Jaeckel P. The most general methodology to create a valid correlation matrix for risk management and option pricing purposes. J Risk 2001;2:1727. [56] Rapisarda F, Brigo D, Mercurio F. Parameterizing correlations: a geometric interpretation. IMA J Manag Math 2007;18:5573. [57] Bouhlel MA, Bartoli N, Regis R, Otsmane A, Morlier J. An improved approach for estimating the hyperparameters of the Kriging model for high dimensional problems through the Partial Least Squares method. Math Probl Eng 2016;2016:6723410. [58] Cheng GH, Younis A, Hajikolaei KH, Wang GG. Trust region based mode pursuing sampling method for global optimization of high dimensional design problems. J Mech Des 2015;137:021407. [59] Karlsson R, Bliek L, Verwer S, de Weerdt M. Continuous surrogatebased optimization algorithms are wellsuited for expensive discrete problems. In: Artificial intelligence and machine learning. 2021. [60] Pelamatti J, Brevault L, Balesdent M, Talbi EG, Guerin Y. Bayesian optimization of variablesize design space problems. Opt Eng 2021;22:387447. [61] Hebbal A, Brevault L, Balesdent M, Talbi EG, Melab N. Bayesian optimization using deep Gaussian processes with applications to aerospace system design. Opt Eng 2021;22:32161. [62] Wildberger N. A rational approach to trigonometry. Math Horiz 2007;15:1620. [63] Cho H, Kim Y, Lee E, Choi D, Lee Y, Rhee W. Basic enhancement strategies when using bayesian optimization for hyperparameter tuning of deep neural networks. IEEE Access 2020;8:52588608. [64] Zuniga MM, Sinoquet D. Global optimization for mixed categoricalcontinuous variables based on Gaussian process models with a randomized categorical space exploration step. INFOR Inf Syst Oper Res 2020;58:31041. [65] Lindauer M, Eggensperger K, Feurer M, AB, Deng D, Benjamins C, et al. SMAC3: A versatile Bayesian optimization package for hyperparameter optimization. J Mach Learn Res 2022;23:19. [66] Picheny V, Berkeley J, Moss H, Stojic H, Granta U, Ober S, et al. Trieste: Efficiently exploring the depths of blackbox functions with TensorFlow. 2023, arXiv. [67] CowenRivers AI, Ly W, Wang Z, Tutunov R, Jianye H, Wang J, et al. HEBO: Heteroscedastic evolutionary Bayesian optimisation. 2020, arXiv. [68] Jiang H, Shen Y, Li Y, Zhang W, Zhang C, Cui B. OpenBox: A Python toolkit for generalized blackbox optimization. 2023, arXiv.",
    "Advances in Engineering Software 188 (2024) 103571 16 P. Saves et al. [69] Kandasamy K, Vysyaraju KR, Neiswanger W, Paria B, Collins C, Schneider J, et al. Tuning hyperparameters without grad students: Scalable and robust bayesian optimisation with dragonfly. J Mach Learn Res 2020;21:3098124. [70] Roy S, Crossley WA, Stanford BK, Moore KT, Gray JS. A mixed integer efficient global optimization algorithm with multiple infill strategy Applied to a wing topology optimization problem. In: AIAA scitech 2019 forum. 2019. [71] M\u00fcller J, Shoemaker CA, Pich\u00e9 R. SOMI: A surrogate model algorithm for computationally expensive nonlinear mixedinteger blackbox global optimization problems. Comput Oper Res 2013;40:1383400. [72] Tran T, Sinoquet D, Da Veiga S, Mongeau M. Derivativefree mixed binary necklace optimization for cyclicsymmetry optimal design problems. Opt Eng 2021. [73] Meliani M, Bartoli N, Lefebvre T, Bouhlel MA, Martins JRRA, Morlier J. Multi fidelity efficient global optimization: Methodology and application to airfoil shape design. In: AIAA aviation 2019 forum. 2019. [74] Lee H. Gaussian processes. Springer Berlin Heidelberg; 2011, p. 5757. [75] L\u00f3pezLopera AF, Idier D, Rohmer J, Bachoc F. Multioutput Gaussian processes with functional data: A study on coastal flood hazard assessment. Reliab Eng Syst Saf 2022;218:108139. [76] Berthelin G, Dubreuil S, Sala\u00fcn M, Bartoli N, Gogu C. Disciplinary proper orthogonal decomposition and interpolation for the resolution of parameterized multidisciplinary analysis. Internat J Numer Methods Engrg 2022;123:3594626. [77] Cardoso I, Dubreuil S, Bartoli N, Gogu C, Sala\u00fcn M, Lafage R. Disciplinary surrogates for gradientbased optimization of multidisciplinary systems. In: ECCOMAS Aerobest. 2023. [78] Platt J, Penny S, Smith T, Chen T, Abarbanel H. A systematic exploration of reservoir computing for forecasting complex spatiotemporal dynamics. Neural Netw 2022;153:53052. [79] Charayron R, Lefebvre T, Bartoli N, Morlier J. Multifidelity Bayesian optimiza tion strategy applied to overall drone design. In: AIAA scitech 2023 forum. 2023. [80] Charayron R, Lefebvre T, Bartoli N, Morlier J. Towards a multifidelity and multiobjective Bayesian optimization efficient algorithm. Aerosp Sci Technol 2023;142:108673. [81] Wold H. Soft modelling by latent variables: The nonlinear iterative partial least squares (NIPALS) approach. J Appl Probab 1975;12:11742. [82] Priem R, Diouane Y, Bartoli N, Dubreuil S, Saves P. Highdimensional efficient global optimization using both random and supervised embeddings. In: AIAA aviation 2023 forum. 2023. [83] Betz W, Papaioannou I, Straub D. Numerical methods for the discretization of random fields by means of the KarhunenLo\u00e8ve expansion. Comput Methods Appl Mech Engrg 2014;271:10929. [84] Menz M, Dubreuil S, Morio J, Gogu C, Bartoli N, Chiron M. Variance based sen sitivity analysis for Monte Carlo and importance sampling reliability assessment with Gaussian processes. Struct Saf 2021;93:102116. [85] Ginsbourger D, Le Riche R, Carraro L. Kriging is wellsuited to parallelize optimization. Springer Berlin Heidelberg; 2010, p. 13162. [86] Roux E, Tillier Y, Kraria S, Bouchard PO. An efficient parallel global opti mization strategy based on Kriging properties suitable for material parameters identification. Arch Mech Eng 2020;67."
]