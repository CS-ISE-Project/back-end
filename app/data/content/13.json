{
    "url": "dummy",
    "title": "SMT 2.0: A Surrogate Modeling Toolbox with a focus on hierarchical and mixed variables Gaussian processes",
    "authors": [
        "Paul Saves",
        "R\u00e9mi Lafage",
        "Nathalie Bartoli",
        "Youssef Diouane",
        "Jasper Bussemaker",
        "Thierry Lefebvre",
        "John T. Hwang",
        "Joseph Morlier",
        "Joaquim R.R.A. Martins"
    ],
    "institutes": [
        "ONERA/DTIS, Universit\u00e9 de Toulouse, Toulouse, France",
        "ISAESUPAERO, Universit\u00e9 de Toulouse, Toulouse, France",
        "Polytechnique Montr\u00e9al, Montreal, QC, Canada",
        "German Aerospace Center (DLR), Institute of System Architectures in Aeronautics, Hamburg, Germany",
        "University of California San Diego, Department of Mechanical and Aerospace Engineering, La Jolla, CA, USA",
        "ICA, Universit\u00e9 de Toulouse, ISAESUPAERO, INSA, CNRS, MINES ALBI, UPS, Toulouse, France",
        "University of Michigan, Department of Aerospace Engineering, Ann Arbor, MI, USA"
    ],
    "keywords": [
        "Surrogate modeling",
        "Gaussian process",
        "Kriging",
        "Hierarchical problems",
        "Hierarchical and mixedcategorical inputs",
        "Meta variables"
    ],
    "abstract": "The Surrogate Modeling Toolbox (SMT) is an opensource Python package that offers a collection of surrogate modeling methods, sampling techniques, and a set of sample problems. This paper presents SMT 2.0, a major new release of SMT that introduces significant upgrades and new features to the toolbox. This release adds the capability to handle mixedvariable surrogate models and hierarchical variables. These types of variables are becoming increasingly important in several surrogate modeling applications. SMT 2.0 also improves SMT by extending sampling methods, adding new surrogate models, and computing variance and kernel derivatives for Kriging. This release also includes new functions to handle noisy and use multifidelity data. To the best of our knowledge, SMT 2.0 is the first opensource surrogate library to propose surrogate models for hierarchical and mixed inputs. This opensource software is distributed under the New BSD license.",
    "content": "motivation and significance with the increasing complexity and accuracy of numerical it has become more challenging to run complex simulations and computer codes as a surrogate models have been recognized as a key tool for engineering tasks such as design space uncertainty and optimization in surrogate models are used to reduce the computational effort of these tasks by replacing expensive numerical simulations with closedform approximations to build such a we start by evaluating the original expensive simulation at a set of points through a design of experiments the corresponding evaluations are used to build the surrogate model according to the chosen such as quadratic or least squares the surrogate modeling toolbox is an opensource frame work that provides functions to efficiently build surrogate models advances in engineering software saves et table comparison of software packages for hierarchical and mixed kriging package botorch dakota dicekriging kergp lvgp parmoo spearmint smt reference this paper license mit epl gpl gpl gpl bsd gnu bsd language python c r r r python python python mixed gd kernel cr kernel hh kernel ehh kernel hierarchical and number of instantiated when design problems include both discrete variables and continuous they are said to have mixed when architectural choices lead to different sets of design we have hierarchical variables for consider ent aircraft propulsion architectures a conventional gas turbine would not require a variable to represent a choice in the electrical power while hybrid or pure electric propulsion would require such a the relationship between the choices and the sets of variables can be represented by a handling hierarchical and mixed variables requires specialized rogate modeling techniques to address these smt is offering researchers and practitioners a collection of tools to build surrogate models with mixed and hierarchical the main objective of this paper is to detail the new ments that have been added in this release compared to the original smt release there are two new major capabilities in smt the ability to build surrogate models involving mixed variables and the support for hierarchical variables within kriging to handle mixed variables in kriging existing libraries such as botorch dakota dicekriging lvgp parmoo and spearmint implement simple mixed models by using either continuous ation also known as encoding or a gower distance based correlation kernel kergp in ments more general kernels but there is no python toolbox that implements more general kernels to deal with mixed such as the homoscedastic hypersphere and exponential homoscedastic hypersphere such kernels require the tuning of a large number of hyperparameters but lead to more accurate kriging surrogates than simpler mixed kernels smt implements all these kernels and through a unified framework and to handle hierarchical no library in the literature can build peculiar surrogate models except smt which implements two kriging methods for these most softwares are compatible with a na\u00efve strategy called the imputation method but this method lacks depth and depends on arbitrary this is why hutter and osborne proposed a first called which in turn was generalized by horn et with a new kernel called the kernel none of these kernels are available in any modeling thanks to the framework introduced in audet et our proposed kernels are sufficiently general so that all existing hierarchical kernels are included within section describes the two kernels implemented in smt that are referred as smt and smt in kernel is a novel hierarchical kernel introduced in this table outlines the main features of the modeling software that can handle hierarchical and mixed smt introduces other such as additional pling new surrogate new kriging kernels their kriging variance and an adaptive criterion for smt adds applications of bayesian optimization with hierarchical and mixed variables or noisy kriging that have been successfully applied to aircraft design data fusion and structural design the smt interface is more and offers an improved and more detailed documentation for users and smt is hosted and can be directly imported within python it is released under the new bsd license and runs on and windows operating regression tests are run automatically for each operating system whenever a change is committed to the in smt builds on the strengths of the original smt package while adding new on one the emphasis on derivatives training and output is maintained and improved in smt on the other this new release includes support for hierarchical and mixed variables kriging based for the sake of an notebook is available that gathers all the methods and results presented on this the remainder of the paper is organized as we troduce the organization and the main implemented features of the release in section we describe the kriging model with an example in section we describe and provide an example for a kriging model in section the bayesian optimization models and applications are described in section we describe the other relevant contributions in section and conclude in section smt an improved surrogate modeling toolbox from a software point of smt maintains and improves the modularity and generality of the original smt version in this we describe the software as section describes the legacy of smt section describes the organization of the section shows the new capabilities implemented in the smt background on smt former smt smt is an collaborative work originally developed by nasa and the university of both polytechnique montr\u00e9al and the university of california san diego are also smt updates and tends the original smt repository capabilities among which the original publication focuses on different types of derivatives for surrogate models detailed advances in engineering software saves et table impact of using numba on training time of the hierarchical goldstein speedup is calculated excluding the jit compilation as this step is only needed once after smt training set without numba numba speedup jit overhead points s s s points s s s a python surrogate modeling framework with one of the original main motivations for smt was derivative in none of the existing packages for surrogate modeling such as in python sumo in matlab or gpml in matlab and octave focuses on three types of derivatives are prediction training and output smt also includes new models with derivatives such as kriging with partial least squares and regularized spline these developed derivatives were even used in a novel algorithm called kriging with partial least squares to use with adjoint for example software and automatic smt is nized along three main that implement a set of sampling techniques benchmarking functions and surrogate modeling techniques the toolbox is created using restructuredtext and a umentation generation package for with custom code snippets in the documentation pages are taken directly from actual tests in the source code and are automatically the output from these code snippets and tables of options are generated dynamically by custom sphinx this leads to documentation with minimal along with user developer documentation is also provided to explain how to contribute to this includes a list of api methods for the and problem that must be implemented to create a new surrogate modeling sampling or benchmarking when a developer submits a pull it is merged only after passing the automated tests and receiving approval from at least one the repository on is linked to continuous integration tests for linux and to a coverage test on and to a dependency version check for python with various parts of the source code have been accelerated using numba an compiler for python numba is applied to ventional python code using function thereby minimizing its impact on the development process and not requiring an additional build for a mixed kriging surrogate with training a speedup of up to is see table the jit compilation step only needs to be done once when installing or upgrading smt and adds an overhead of approximately s on a typical workstation in this all results are obtained using an cpu ghz core and gb of memory with a generation processor and a compute node of a peak power of organization of smt the main features of the repository smt are described in more sampling problems and surrogate models are kept from smt and two new sections models applications and interactive notebooks have been added to the architecture of the these sections are highlighted in blue and detailed on the new major features implemented in smt are highlighted in lavender whereas the legacy features that were already in present in the original publication for smt are in new features within smt the main objective of this new release is to enable kriging surrogate models for use with both hierarchical and mixed for each of these five described in section several improvements have been made between the original version and the smt hierarchical and mixed design a new design space definition class designspace has been added that implements hierarchical and mixed design variables can either be ous ordered or categorical the integer type resents a special case of the ordered specified by bounds rather than a list of possible the hierarchical ture of the design space can be defined using this function declares that a variable is a decreed variable that is activated when the associated meta variable takes one of a set of specified see section for the designspace class also implements mechanisms for sampling valid design vectors design vectors that adhere to the hierarchical structure of the design using any of the for recting and imputing design and for requesting which design variables are acting in a given design correction ensures that variables have valid values integers for discrete and imputation replaces variables by some default value for discrete between the bounds for continuous variables in smt smt implements three methods for the first one is a na\u00efve called random that draws uniformly points along every the second sampling method is called full torial and draws a point for every cross combination of to have an design of the last one is the latin hypercube sampling that draws a point in every latin square parameterized by a certain for a new criterion to manage the randomness has been implemented and the sampling method was adapted for and mixed or hierarchical more details about the new sampling techniques are given in section smt implements two new engineering a mixed variant of a cantilever beam described in section and a hierarchical neural network described in section surrogate in order to keep up with several leases done from the original version developed new options for the already existing in compared to the original publication smt adds neural networks and marginal gaussian process models to the list of available more details about the new models are given in section several applications have been added to the toolbox to demonstrate the surrogate models the most relevant plication is efficient global optimization a bayesian tion algorithm ego optimizes problems with a chosen surrogate model and a chosen optimization criterion the usage of ego with hierarchical and mixed variables is described in section advances in engineering software saves et functionalities of smt the new major features implemented in smt compared to smt are highlighted with the lavender interactive these tutorials introduce and explain how to use the toolbox for different surrogate models and every tutorial is available both as a file and directly on google in a hierarchical and mixed variables dedicated notebook is available to reproduce the results presented on this in the section details the kriging based surrogate models for mixed and section presents our new kriging surrogate for hierarchical section details the ego tion and the other new relevant features aforementioned are described succinctly in section surrogate models with mixed variables in smt as mentioned in section design variables can be either of ous or discrete and a problem with both types is a discrete variables can be ordinal or a discrete variable is ordinal if there is an order relation within the set of possible an example of an ordinal design variable is the number of engines in an a possible set of values in this case could be a discrete variable is categorical if no order relation is known between the possible choices the variable can one example of a categorical variable is the color of a a possible example of a set of choices could be the possible choices are called the levels of the several methods have been proposed to address the recent increase interest in mixed kriging based models the main difference from a continuous kriging model is in the estimation of advances in engineering software saves et table categorical kernels implemented in smt name of smt gd \ud835\udf03\ud835\udc56 smt cr \ud835\udc3f\ud835\udc56 smt ehh log \ud835\udf16 smt hh \ud835\udf19 the categorical correlation which is critical to determine the mean and variance as mentioned in section approaches such as cr continuous latent variables and gd use a method to estimate the correlation other methods estimate the correlation matrix by modeling the correlation entries directly such as hh and ehh the hh correlation kernel is of particular interest because it generalizes simpler kernels such as cr and gd in smt the correlation kernel is an option that can be set to either cr gd hh or ehh mixed gaussian processes the continuous and ordinal variables are both treated similarly in smt with a continuous where the ordinal values are converted to continuous through for categorical four models ehh and can be used in smt if specified by the this is why we developed a unified mathematical formulation that allows a unique implementation for any denote \ud835\udc59 the number of categorical for a given \ud835\udc56 the \ud835\udc56th categorical variable is denoted \ud835\udc50\ud835\udc56 and its number of levels is denoted the hyperparameter matrix peculiar to this variable \ud835\udc50\ud835\udc56 is \ud835\udee9\ud835\udc56 and the categorical parameters are defined as \ud835\udf03\ud835\udc50\ud835\udc4e\ud835\udc61 for two given inputs in the for the \ud835\udc5fth and \ud835\udc60th let \ud835\udc50\ud835\udc5f \ud835\udc56 and \ud835\udc50\ud835\udc60 \ud835\udc56 be the associated categorical variables taking respectively the \ud835\udcc1\ud835\udc56 \ud835\udc5f and the \ud835\udcc1\ud835\udc56 \ud835\udc60 level on the categorical variable the categorical correlation kernel is defined by \ud835\udc56 \ud835\udc56 \ud835\udc56 \ud835\udc56 \ud835\udc56 \ud835\udc56 \ud835\udc56 \ud835\udc56 where \ud835\udf05 is either a positive definite kernel or identity and is a symmetric positive definite function such that the matrix is spd if \ud835\udee9\ud835\udc56 is for an exponential table gives the parameterizations of \ud835\udef7 and \ud835\udf05 that correspond to and ehh the complexity of these different kernels depends on the number of hyperparameters that characterizes as defined by saves et for every categorical variable \ud835\udc56 the matrix is lower triangular and built using a hypersphere decomposition from the symmetric matrix \ud835\udee9\ud835\udc56 of the variable \ud835\udf16 is a small positive constant and the variable \ud835\udf03\ud835\udc56 denotes the only positive hyperparameter that is used for the gower distance another kriging based model that can use mixed variables is kriging with partial least squares kpls adapts kriging to high dimensional problems by using a reduced number of hyperparameters thanks to a projection into a smaller for a general not necessarily smt uses continuous relaxation to allow whatever model to handle mixed for we can use mixed variables with least squares or quadratic polynomial we now illustrate the abilities of the toolbox in terms of mixed modeling over an engineering test table results of the cantilever beam models table categorical kernel displacement error likelihood of smt gd smt cr smt ehh smt hh an engineering design a classic engineering problem commonly used for model validation is the beam bending problem this problem is illustrated on and consists of a cantilever beam in its linear range loaded at its free end with a force as in cheng et the young modulus is \ud835\udc38 gpa and the chosen load is \ud835\udc39 as in roustant et possible can be these sections consist of possible shapes that can be either thick or full as illustrated in to compare the mixed kriging models of smt we draw a point lhs as training set and the validation set is a grid of for the four implemented displacement error with a error number of hyperparameters and computational time for every model are shown in table for the continuous we use the square exponential more details are found in as the complex ehh and hh models lead to a lower displacement error and a higher likelihood but use more hyperparameters and increase the computational cost compared to gd and on this test the kernel ehh is easier to optimize than hh but in they are similar in terms of by default smt uses cr as it is known to be a good between complexity and performance surrogate models with hierarchical variables in smt to introduce the newly developed kriging model for hierarchical variables implemented in smt we present the general matical framework for hierarchical and mixed variables established by audet et in smt two variants of our new method are namely smt and smt in the smt is a novel correlation kernel introduced in this the hierarchical variables framework a problem structure is classified as hierarchical when the sets of active variables depend on architectural this occurs frequently in industrial design in hierarchical we can classify variables as meta known as or decreed known as conditionally as detailed in audet et neutral variables are the variables that are not affected by the hierarchy whereas the value assigned to meta variables determines which decreed variables are for a meta variable could be the number of if the number of engines the number of decreed bypass ratios that every engine should specify also advances in engineering software saves et cantilever beam problem figure variables classification as used in smt the wing aspect ratio being it is not affected by this problems involving hierarchical variables are generally dependant on discrete architectures and as such involve mixed in addition to their role meta or each variable also has a variable type amongst ordinal or for the sake of simplicity and because both continuous and ordinal variables are treated similarly we chose to regroup them as quantitative for the neutral variables \ud835\udc65neu may be partitioned into different variable such that \ud835\udc65neu \ud835\udc65qnt where \ud835\udc65cat neu represents the categorical variables and \ud835\udc65qnt neu are the quantitative the variable classification scheme in smt is detailed in to explain the framework and the new kriging we illustrate the inputs variables of the model using a classical machine ing problem related to the hyperparameters optimization of a connected perceptron on in table we detail the input variables of the model related to the mlp problem the hyperparameters of the neural together with their types and to keep things clear and the chosen problem is a simplification of the original problem developed by audet et regarding the mlp problem of and following the fication scheme of we start by separating the input variables according to their in changing the number of hidden layers modifies the number of inputs of hidden is a meta the number of neurons in the hidden layer number \ud835\udc58 is either included or for the of neurons in the would be excluded for an input that only has hidden of neurons hidden layer are decreed the and are not affected by the hierarchy they are neutral according to their the mlp input variables can be classified as the meta variable of hidden is an integer as is represented by the component \ud835\udc65qnt the decreed variables of neurons hidden layer are integers as are represented by the component \ud835\udc65qnt the and for the first two ery value between two categorical between three and integer between the and the are represented by the component \ud835\udc65cat the and the are represented by the component \ud835\udc65qnt to model hierarchical as proposed in we separate the input space as where advances in engineering software saves et the perceptron figure adapted from figure table a detailed description of the variables in the mlp mlp hyperparameters variable domain type role learning rate \ud835\udc5f float neutral momentum \ud835\udefc float neutral activation function \ud835\udc4e \ud835\udc47 enum neutral batch size \ud835\udc4f ord neutral of hidden layers \ud835\udc59 ord meta of neurons hidden layer \ud835\udc58 \ud835\udc5b\ud835\udc58 ord decreed for a given point \ud835\udc65 one has \ud835\udc65 where \ud835\udc65neu \ud835\udc65met and are defined as the components \ud835\udc65neu gather all neutral variables that are not impacted by the meta variables but for in the mlp gathers the possible learning activation functions and batch from table the components \ud835\udc65met gather the meta known as variables that determine the inclusion or exclusion of other for in the mlp represents the possible numbers of layers in the from table the components contain the decreed variables whose inclusion or exclusion is determined by the values of the meta components for in the mlp represents the number of neurons in the decreed from table and a kriging model for hierarchical variables in this a new method to build a kriging model with chical variables is introduced based on the framework the proposed methods are included in smt motivation and assuming that the decreed variables are hutter and osborne proposed several kernels for the hierarchical a classic called the imputation method leads to an efficient paradigm in practice that can be easily integrated into a more general framework as proposed by bussemaker et this kernel lacks depth and depends on arbitrary hutter and osborne also proposed a more general called and horn et generalized this kernel even more and proposed a new formulation called the kernel the drawbacks of these two methods are that they add some extra hyperparameters for every decreed dimension one extra hyperparameter for the and two eters for the and that they need a normalization according to the more pelamatti et developed a hierarchical kernel for bayesian our work is also more general thanks to the framework introduced earlier that considers formulation and is more flexible as we are allowing to be in the we describe our new method to build a correlation kernel for hierarchical in we introduce a new braic kernel called that behaves like the whilst correcting most of its in our kernel does not add any and the normalization is handled in a natural a new hierarchical correlation kernel for modeling we assume that the decreed space is let \ud835\udc62 be an input point partitioned as advances in engineering software saves et \ud835\udc62 \ud835\udc63 is another input such that \ud835\udc63 the new kernel \ud835\udc58 that we propose for hierarchical variables is given by where \ud835\udc58met and are as \ud835\udc58neu represents the neutral kernel that encompasses both ical and quantitative neutral \ud835\udc58neu can be posed into two parts \ud835\udc63cat \ud835\udc63qnt the categorical denoted could be any symmetric positive definite mixed kernel section for the quantitative or a based kernel is the chosen quantitative kernel always depends on a given distance for the exponential kernel gives \ud835\udc5b \ud835\udc56 \ud835\udc63qnt \ud835\udc56 \ud835\udc58met is the meta variables related it is also separated into two \ud835\udc63cat \ud835\udc63qnt where the quantitative kernel is ordered and not continuous because meta variables take value in a finite is an spd kernel that models the correlations between the meta levels the possible and the decreed in what comes we detailed this towards an algebraic kernel kernels like the imputation kernel or the were first proposed in and the kernels such as or were proven to be to guarantee this spd the same hyperparameters are used to model the correlations between the meta levels and between the decreed variables for this the includes additional continuous hyperparameters which makes the training of the gp models more expensive and introduces more numerical stability in this we are proposing a new algebraic kernel as that enjoys similar properties as but without using additional continuous hyperparameters nor in the smt we included and a simpler version of that do not relies on additional our proposed kernel is given by \ud835\udc58alg \ud835\udc58alg \ud835\udc58alg we could consider that there is only one meta variable whose levels correspond to every possible included let \ud835\udc3csub denotes the components indices of possible the subspaces parameterized by the meta component \ud835\udc62met are defined as \ud835\udc59 it follows that the fully extended continuous decreed space writes as and \ud835\udc3cdec is the set of the associated let \ud835\udc3c\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f denotes the set of components related to the space containing the variables in both and since the decreed variables are one has \ud835\udc58alg the construction of the quantitative kernel \ud835\udc58qnt depends on a given distance denoted the kernel \ud835\udc58alg met is an induced meta kernel that depends on the same distance \ud835\udc51alg to preserve the spd property of \ud835\udc58alg for every \ud835\udc56 if \ud835\udc56 \ud835\udc3c\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f the new algebraic distance is given by where \ud835\udf03\ud835\udc56 is a continuous if \ud835\udc56 \ud835\udc3cdec but \ud835\udc56 \ud835\udc3c\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f there should be a residual distance between the two different subspaces and to ensure the kernel spd to have a residual not depending on the decreed our model considers that there is a unit distance \ud835\udc3cdec \ud835\udc3c\ud835\udc56\ud835\udc5b\ud835\udc61\ud835\udc52\ud835\udc5f the induced meta kernel \ud835\udc58alg to preserve the spd property of \ud835\udc58alg is defined \ud835\udc58alg not only our kernel of uses less hyperparameters than the kernel we cut off its extra but it is also a more flexible kernel as it allows different kernels for meta and decreed another advantage of our kernel is that it is numerically more stable thanks to the new algebraic distance defined in our proposed distance also does not need any rescaling by the bounds to have values between and in what comes we will refer to the implementation of the kernels and by smt and smt we note also that the implementation of smt differs slightly from the original as we fixed some hyperparameters to in order to avoid adding extra perparameters and use the formulation of and rescaling of the illustration on the mlp problem in this we illustrate the hierarchical on the mlp for that we consider two design variables \ud835\udc62 and \ud835\udc63 such that \ud835\udc62 and \ud835\udc63 since the value of \ud835\udc62met the number of hidden differs from one point to another for \ud835\udc62 and for the associated variables have either or variables for the number of neurons in each layer and for and and for the point in our ters will have to be optimized where \ud835\udc58 is given by these hyperparameters can be described using our proposed framework as for the neutral we have \ud835\udc62neu and \ud835\udc63neu for a categorical matrix kernel and a square exponential quantitative \ud835\udc63cat \ud835\udc63qnt exp exp exp the values and need to be is the correlation between and for the meta we have \ud835\udc62met and \ud835\udc63met for a square exponential quantitative \ud835\udc63cat \ud835\udc63qnt exp the value needs to be advances in engineering software saves et for the we have and which gives \ud835\udc58alg \ud835\udc58alg \ud835\udc58alg the distance in for surrogate and in particular in smt the input data are with a unit normalization from to we would have we between and for a square exponential quantitative one gets \ud835\udc58alg exp exp exp where the meta induced component is \ud835\udc58alg exp because the decreed value in \ud835\udc63 has nothing to be compared with in \ud835\udc62 as in the values and need to be mized which complete the description of the we note that for the mlp models use hyperparameters whereas the would require hyperparameters without the meta kernel but with extra decreed hyperparameters and the would require for deep learning a more complex perceptron with up to hidden layers would require hyperparameters with smt models against for and for the next section illustrates the interest of our method to build a surrogate model for this neural network engineering a neural network using smt in this we apply our models to the hyperparameters mization of a mlp problem aforementioned of within smt an example illustrates this mlp for the sake of showing the kriging surrogate we implemented a dummy function with no significance to replace the real that would require training a whole neural network with big this function requires a number of variables that depends on the value of the meta the number of hidden to we have chosen only or hidden layers and we have decreed variables but deep neural networks could also be investigated as our model can tackle a few dozen a test case shows that our model smt interpolates the data checks that the data dimension is correct and also asserts that the inactive decreed variables have no influence over the in we illustrate the usage of kriging surrogates with hierarchical and mixed variables based on the implementation of smt for to compare the hierarchical models of smt and smt with the imputation method previously used on industrial application we draw a point lhs points by meta as a training set and the validation set is a lhs of for the the values are replaced by because the mean value is not an integer smt rounds to the floor for the three the precision with a square error rmse the likelihood and the computational time are shown in table for this we can see that smt kernel gives better performance than the imputation method or smt as all methods use the same number of they have similar time a direct application of table results on the neural network hierarchical method prediction error likelihood of smt smt our modeling method is bayesian optimization to perform quickly the hyperparameter optimization of a neural network bayesian optimization within smt efficient global optimization is a sequential bayesian timization algorithm designed to find the optimum of a function that may be expensive to evaluate ego starts by fitting a kriging model to an initial and then uses an acquisition function to select the next point to the most used acquisition function is the expected once a new point has been the kriging model is successive updates increase the model racy over this enrichment process repeats until a stopping criterion is because smt implements kriging models that handle mixed and hierarchical we can use ego to solve problems volving such design other bayesian optimization algorithms often adopt approaches based on solving subproblems with uous or this subproblem approach is less efficient and scales but it can only solve simple several bayesian optimization software packages can handle mixed or hierarchical variables with such a subproblem the ages include botorch smac trieste hebo openbox and dragonfly a mixed optimization problem compares the four ego methods implemented in smt smt smt smt ehh and smt the mixed test case that illustrates bayesian optimization is a toy test case detailed in pendix this test case has two one continuous and one categorical with to assess the performance of our we performed runs with different initial doe sampled by every doe consists of points and we chose a budget of infill plots the convergence curves for the four in the median is the solid and the first and third quantiles are plotted in dotted to visualize better the data the boxplots of the best solutions after evaluations are plotted in as the more a method is the better the both smt hh and smt ehh converged in around evaluations whereas smt cr and smt gd take around iterations as shown on the more complex the the closer the optimum is to the real value as shown on in we illustrate how to use ego with mixed variables based on the implementation of smt the illustrated problem is a mixed variant of the branin function note that a dedicated notebook is available to reproduce the results presented in this paper and the mixed integer notebook also includes an extra mechanical application with composite materials advances in engineering software saves et example of usage of hierarchical and mixed kriging advances in engineering software saves et optimization results for the toy function example of usage of mixed surrogates for bayesian advances in engineering software saves et optimization results for the hierarchical goldstein a hierarchical optimization problem the hierarchical test case considered in this paper to illustrate bayesian optimization is a modified goldstein function detailed in appendix the resulting optimization problem involves are are integer and are these variables consist in neutral dimensional variable and decreed depending on the meta variable different can be the optimization problem is given min \ud835\udc65qnt \ud835\udc65cat \ud835\udc5a \ud835\udc65qnt \ud835\udc65cat neu \ud835\udc65qnt neu \ud835\udc65cat \ud835\udc5a \ud835\udc65qnt dec compared to the model choice of pelamatti et we chose to model and as neutral variables even if \ud835\udc53 does not depend on when other modeling choices are for is a and not a categorical one we also keep the formulation of as a categorical variable but a better model would be to model it as a the resulting problem is described in appendix to assess the performance of our we performed runs with different initial doe sampled by every doe consists of \ud835\udc5b points and we chose a budget of infill to compare our method with a we also tested the random search method thanks to the new method described in section and we also optimized the goldstein function using ego with a classic kriging model based on imputation method this method replaces the excluded variables by their mean or respectively for and plots the convergence curves for the four in the median is the solid line and the first and third quantiles are plotted in dotted to visualize better the ing data the boxplots of the best solutions are plotted in the results in show that the hierarchical kriging models of smt lead to better results than the imputation method or the random search both in terms of final objective value and variance over the runs and in term of convergence more smt and smt kriging model gave the best ego results and managed to converge correctly as shown in more the sampled does led to similar performance and from one the method smt managed to find the true this result has not been reproduced in other runs and is therefore not statistically the variance between the runs is of similar magnitude regardless of the considered other relevant contributions in smt the new release smt introduces several improvements sides kriging for hierarchical and mixed this section details the most important new recall from section that five are present in the surrogate applications and contributions to sampling the latin hypercube sampling is a stochastic sampling technique to generate sampling it is among the most popular sampling method in computer experiments thanks to its simplicity and projection properties with the lhs method uses the pydoe package of experiments for five criteria for the construction of lhs are implemented in the first four criteria are the same as in the last criterion is implemented by the authors of smt in smt a new lhs method was developed for the nested design of experiments to use with fidelity a new mathematical method was developed in smt to increase the size of a design of iments while maintaining the ese we proposed a sampling method for mixed and the aforementioned lhs method was applied to hierarchical variables in contributions to surrogate models new kernels and their derivatives for kriging surrogates are based on hyperparameters and on a correlation four tion kernels are now implemented in smt in these correlation functions are absolute exponential gaussian matern and matern in the implementation of gradient and hessian for each kernel makes it possible to calculate both the first and second derivatives of the gp likelihood with respect to the hyperparameters variance derivatives for to perform uncertainty quantification for system analysis it could be interesting to know more about the variance derivatives of a model for that purpose and also to pursue the original publication about derivatives smt extends the derivative support to kriging variances and advances in engineering software saves et noisy in engineering and in big data contexts with real surrogate models for noisy data are of significant in there is a growing need for techniques like noisy kriging and noisy kriging for data fusion for that smt has been designed to accommodate kriging and mfk to noisy data including the option to incorporate heteroscedastic noise the and to account for different noise levels for each data source kriging with partial least beside for the toolbox implements kriging with partial least squares and its extension kplsk the pls information is computed by projecting the data into a smaller space spanned by the principal by integrating this pls information into the kriging correlation the number of inputs can be scaled thereby reducing the number of hyperparameters the ing number of hyperparameters \ud835\udc51\ud835\udc52 is indeed much smaller than the original problem dimension in smt we extended the kpls method for kriging and we also proposed an automatic criterion to choose automatically the reduced dimension \ud835\udc51\ud835\udc52 based on r criterion this criterion has been applied to aircraft optimization with ego where the number \ud835\udc51\ud835\udc52 in the for the model is automatically selected at every iteration special efforts have been made to accommodate kpls for and mixed integer data marginal gaussian smt implements marginal gaussian process surrogate models for high dimensional problems mgp are gaussian processes taking into account hyperparameters certainty defined as a density probability especially we pose that the function to model \ud835\udc53 \ud835\udefa where \ud835\udefa r\ud835\udc51 and \ud835\udc51 is the number of design lies in a linear embedding such as \ud835\udc65 \ud835\udc34 and with r and \ud835\udc51\ud835\udc52 we must use a kernel whose each component of the transfer matrix \ud835\udc34 is an thus we have \ud835\udc51\ud835\udc52 \ud835\udc51 hyperparameters to note that \ud835\udc51\ud835\udc52 is defined as in the code neural the new release smt ments neural network models neural networks are fully connected layer perceptrons whose training process was modified to account for gradient the model is trained to minimize not only the prediction error of the response but also the prediction error of the partial the chief benefit of gradient enhancement is better accuracy with fewer training note that genn applies to regression or but not classification since there is no gradient in that the implementation is fully vectorized and uses adam and for genn can be used to learn airfoil geometries from a this usage is documented in smt contributions to applications kriging trajectory and sampling a gp with high resolution is usually expensive due to the large dimension of the associated covariance several methods are proposed to draw samples of a gp on a given set of to sample a conditioned the classic method consists in using a cholesky decomposition of the conditioned covariance matrix of the process but some numerical computational errors can lead to non spd a more recent approach based on decomposition of the covariance kernel with the nystr\u00f6m method has been proposed in where the paths can be sampled by generating independent standard normal distributed the different methods are documented in the tutorial gaussian process trajectory sampling parallel bayesian due to the recent progress made in hardware it has been of high interest to perform parallel a parallel criterion called qei was developed to perform efficient global optimization the goal is to be able to run batch at each iteration of the multiple new sampling points are extracted from the known these new sampling points are then evaluated using a parallel computing five criteria are implemented in smt kriging believer kriging believer upper bound kriging believer lower bound kriging believer random bound and constant liar conclusion smt introduces significant upgrades to the surrogate modeling this new release adds support for hierarchical and mixed variables and improves the surrogate models with a particular focus on kriging smt is distributed through an license and is freely available we provide documentation that caters to both users and potential smt enables users and developers collaborating on the same project to have a common surrogate modeling tool that facilitates the exchange of methods and reproducibility of smt has been widely used in aerospace and mechanical modeling the toolbox is general and can be useful for anyone who needs to use or develop surrogate modeling regardless of the targeted smt is currently the only source toolbox that can build hierarchical and mixed surrogate declaration of competing interest the authors declare that they have no known competing cial interests or personal relationships that could have appeared to influence the work reported in this data availability data will be made available on results can be reproduced freely online at acknowledgments we want to thank all those who contribute to this carreira conde roux and this work is part of the activities of onera isae enac joint research we also acknowledge the partners nasa institut cl\u00e9ment ader the university of polytechnique montr\u00e9al and the university of california san the research presented in this paper has been performed in the framework of the agile project rative aircraft funded by the european union horizon research and innovation framework programme under grant agreement and in the colossus project system of systems exploration of aviation services and advances in engineering software saves et business funded by the european union horizon europe search and innovation framework programme under grant agreement we also are grateful to from polytechnique tr\u00e9al for the hierarchical variables appendix toy test function this appendix gives the detail of the toy function of section we recall the optimization min \ud835\udc65cat \ud835\udc65qnt the toy function \ud835\udc53 is defined as \ud835\udc65 \ud835\udc65 \ud835\udc65 \ud835\udc65 \ud835\udc65 \ud835\udc65 \ud835\udc65 \ud835\udc65 appendix hierarchical goldstein test function this appendix gives the detail of the hierarchical goldstein problem of section we recall the optimization min \ud835\udc65qnt \ud835\udc65cat \ud835\udc5a \ud835\udc65qnt \ud835\udc65cat neu \ud835\udc65qnt neu \ud835\udc65cat \ud835\udc5a \ud835\udc65qnt dec the hierarchical and mixed function \ud835\udc53 is defined as a hierarchical function that depends on and \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont as describes in the the functions and are defined as mixed variants of \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont as such to finish the function \ud835\udc3a\ud835\udc5c\ud835\udc59\ud835\udc51cont is given by cos appendix supplementary data more at supplementary material related to this article can be found online at",
    "references": [
        "[1] Mader CA, Martins JRRA, Alonso JJ, van der Weide E. ADjoint: An approach for the rapid development of discrete adjoint solvers. AIAA J 2008;46:86373. ",
        "[2] Kennedy M, OHagan A. Bayesian calibration of computer models. J R Stat Soc Ser B Stat Methodol 2001;63:42564. ",
        "[3] Hwang JT, Martins JRRA. A fastprediction surrogate model for large datasets. Aerosp Sci Technol 2018;75:7487. ",
        "[4] Martins JRRA, Ning A. Engineering design optimization. Cambridge University Press; 2021. ",
        "[5] Bouhlel MA, Hwang JT, Bartoli N, Lafage R, Morlier J, Martins JRA. A Python surrogate modeling framework with derivatives. Adv Eng Softw 2019;135:102662. ",
        "[6] Bouhlel MA, Martins J. Gradientenhanced kriging for highdimensional problems. Eng Comput 2019;35:15773. ",
        "[7] Pedregosa F, Varoquaux G, Gramfort A, Thirion VMB, Grisel O, et al. Scikitlearn: Machine learning in Python. J Mach Learn Res 2011;12:282530.",
        "[8] Lataniotis C, Marelli S, Sudret B. Uqlab 2.0 and uqcloud: opensource vs. cloudbased uncertainty quantification. In: SIAM conference on uncertainty quantification. 2022. ",
        "[9] Faraci A, Beaurepaire P, Gayton N. Review on Python toolboxes for Kriging surrogate modelling. In: ESREL. 2022. ",
        "[10] Kr\u00fcgener M, Zapata Usandivaras J, Bauerheim M, Urbano A. Coaxialinjector surrogate modeling based on Reynoldsaveraged NavierStokes simulations using deep learning. J Propuls Power 2022;38:78398. ",
        "[11] Ming D, Williamson D, Guillas S. Deep Gaussian process emulation using stochastic imputation. Technometrics 2022;112. ",
        "[12] Eli\u00e1\u0161 J, Vo\u0159echovsky M, Sad\u00edlekv V. Periodic version of the minimax distance criterion for Monte Carlo integration. Adv Eng Softw 2020;149:102900. ",
        "[13] Drouet V, Balesdent M, Brevault L, Dubreuil S, Morio J. Multifidelity algo rithm for the sensitivity analysis of multidisciplinary problems. J Mech Des 2023;145:122. ",
        "[14] Karban P, P\u00e1nek D, Orosz T, Petr\u00e1\u0161ov\u00e1 I, Dole\u017eel I. FEM based robust design optimization with Agros and Artap. Comput Math Appl 2021;81:61833. ",
        "[15] Kudela J, Matousek R. Recent advances and applications of surrogate models for finite element method computations: a review. Soft Comput 2022;26:1370933. ",
        "[16] Chen Y, Dababneh F, Zhang B, Kassaee S, Smith BT, Liu K, et al. Surrogate mod eling for capacity planning of charging station equipped with photovoltaic panel and hydropneumatic energy storage. J Energy Res Technol 2020;142:050907. ",
        "[17] Jasa J, Bortolotti P, Zalkind D, Barter G. Effectively using multifidelity optimization for wind turbine design. Wind Energy Sci 2022;7:9911006. ",
        "[18] Wang W, Tao G, Ke D, Luo J, Cui J. Transpiration cooling of high pres sure turbine vane with optimized porosity distribution. Appl Therm Eng 2023;223:119831. ",
        "[19] Savage T, AlmeidaTrasvina HF, del R\u00edoChanona EA, Smith R, Zhang D. An adaptive datadriven modelling and optimization framework for complex chemical process design. Comput Aided Chem Eng 2020;48:738. ",
        "[20] Chan A, Pires AF, Polacsek T. Trying to elicit and assign goals to the right actors. In: Conceptual modeling: 41st international conference. 2022. ",
        "[21] Hutter F, Osborne MA. A kernel for hierarchical parameter spaces. 2013, arXiv. ",
        "[22] Bussemaker JH, Ciampa PD, Nagel B. System architecture design space explo ration: An approach to modeling and optimization. In: AIAA aviation 2020 forum. 2020. ",
        "[23] Fouda MEA, Adler EJ, Bussemaker J, Martins JRRA, Kurtulus DF, Boggero L, et al. Automated hybrid propulsion model construction for conceptual aircraft design and optimization. In: 33rd congress of the international council of the aeronautical sciences. 2022. ",
        "[24] Bussemaker JH, Bartoli N, Lefebvre T, Ciampa PD, Nagel B. Effectiveness of surrogatebased optimization algorithms for system architecture optimization. In: AIAA aviation 2021 forum. 2021. ",
        "[25] Balandat M, Karrer B, Jiang D, Daulton S, Letham B, Wilson A, et al. BoTorch: A framework for efficient MonteCarlo Bayesian optimization. Adv Neural Inf Process Syst 2020;33:2152438. ",
        "[26] Adams B, Bohnhoff W, Dalbey K, Ebeida M, Eddy J, Eldred M, et al. Dakota, a multilevel parallel objectoriented framework for design optimization, pa rameter estimation, uncertainty quantification, and sensitivity analysis: Version 6.13 users manual. Technical report, Albuquerque, NM (United States: Sandia National Lab.(SNLNM); 2020. ",
        "[27] Roustant O, Ginsbourger D, Deville Y. DiceKriging, DiceOptim: Two R packages for the analysis of computer experiments by Krigingbased metamodeling and optimization. J Stat Softw 2012;51:155. ",
        "[28] Zhang Y, Tao S, Chen W, Apley D. A latent variable approach to Gaus sian process modeling with qualitative and quantitative factors. Technometrics 2020;62:291302. ",
        "[29] Chang TH, Wild SM. ParMOO: A Python library for parallel multiobjective simulation optimization. J Open Source Softw 2023;8:4468. ",
        "[30] GarridoMerch\u00e1n EC, Hern\u00e1ndezLobato D. Dealing with categorical and integervalued variables in Bayesian optimization with Gaussian processes. Neurocomputing 2020;380:2035. ",
        "[31] Halstrup M. Blackbox optimization of mixed discretecontinuous optimization problems (Ph.D. thesis), TU Dortmund; 2016. ",
        "[32] Roustant O, Padonou E, Deville Y, Cl\u00e9ment A, Perrin G, Giorla J, et al. Group kernels for gaussian process metamodels with categorical inputs. SIAM J Uncertain Quant 2020;8:775806. ",
        "[33] Zhou Q, Qian PZG, Zhou S. A simple approach to emulation for computer models with qualitative and quantitative factors. Technometrics 2011;53:26673. ",
        "[34] Saves P, Diouane Y, Bartoli N, Lefebvre T, Morlier J. A mixedcategorical correlation kernel for Gaussian process. Neurocomputing 2023;550:126472. ",
        "[35] Pelamatti J, Brevault L, Balesdent M, Talbi EG, Guerin Y. Efficient global optimization of constrained mixed variable problems. J Global Optim 2019;73:583613. ",
        "[36] Horn D, Stork J, ler NJS, Zaefferer M. Surrogates for hierarchical search spaces: The WedgeKernel and an automated analysis. In: Proceedings of the genetic and evolutionary computation conference. 2019. ",
        "[37] Hung Y, Joseph VR, Melkote SN. Design and analysis of computer experiments with branching and nested factors. Technometrics 2009;51:35465. ",
        "[38] Audet C, Hall\u00e9Hannan E, Le Digabel S. A general mathematical framework for constrained mixedvariable blackbox optimization problems with meta and categorical variables. Oper Res Forum 2023;4:137. ",
        "[39] Saves P, Nguyen Van E, Bartoli N, Diouane Y, Lefebvre T, David C, Defoort S, Morlier J. Bayesian optimization for mixed variables using an adaptive dimension reduction process: applications to aircraft design. In: AIAA scitech 2022. 2022. ",
        "[40] Conde Arenzana R, L\u00f3pezLopera A, Mouton S, Bartoli N, Lefebvre T. Multi fidelity Gaussian process model for CFD and wind tunnel data fusion. In: ECCOMAS aerobest. 2021. ",
        "[41] Rufato RC, Diouane Y, Henry J, Ahlfeld R, Morlier J. A mixedcategorical datadriven approach for prediction and optimization of hybrid discontinuous composites performance. In: AIAA aviation 2022 forum. 2022. ",
        "[42] Gorissen D, Crombecq K, Couckuyt I, Dhaene T, Demeester P. A surrogate modeling and adaptive sampling toolbox for computer based design. J Mach Learn Res 2010;11:20515. ",
        "[43] Williams CK, Rasmussen CE. Gaussian processes for machine learning. MA: MIT press Cambridge; 2006. ",
        "[44] Bouhlel MA, Bartoli N, Regis R, Otsmane A, Morlier J. Efficient Global Opti mization for highdimensional constrained problems by using the Kriging models combined with the Partial Least Squares method. Eng Optim 2018;50:203853. ",
        "[45] Bouhlel MA, He S, Martins J. Scalable gradientenhanced artificial neural networks for airfoil shape design in the subsonic and transonic regimes. Struct Multidiscip Optim 2020;61:136376. ",
        "[46] Kwan LS, Pitrou A, Seibert S. Numba: A LLVMbased python JIT compiler. In: Proceedings of the second workshop on the LLVM compiler infrastructure in HPC. 2015. ",
        "[47] Zaefferer M, Horn D. A first analysis of kernels for Krigingbased optimization in hierarchical search spaces. 2018, arXiv. ",
        "[48] Jin R, Chen W, Sudjianto A. An efficient algorithm for constructing optimal design of computer experiments. J Statist Plann Inference 2005;2:54554. ",
        "[49] Garnett R, Osborne M, Hennig P. Active learning of linear embeddings for Gaussian processes. In: Uncertainty in artificial intelligence Proceedings of the 30th conference. 2013. ",
        "[50] Jones D. A taxonomy of global optimization methods based on response surfaces. J Global Optim 2001;21:34583. ",
        "[51] Lafage R. egobox, a Rust toolbox for efficient global optimization. J Open Source Softw 2022;7:4737. ",
        "[52] Jones DR, Schonlau M, Welch WJ. Efficient global optimization of expensive blackbox functions. J Global Optim 1998;13:45592. ",
        "[53] Deng X, Lin CD, Liu K, Rowe RK. Additive Gaussian process for computer models with qualitative and quantitative factors. Technometrics 2017;59:28392. ",
        "[54] CuestaRamirez J, Le Riche R, Roustant O, Perrin G, Durantin C, Gliere A. A comparison of mixedvariables Bayesian optimization approaches. Adv Model Simul Eng Sci 2021;9:129. ",
        "[55] Rebonato R, Jaeckel P. The most general methodology to create a valid correlation matrix for risk management and option pricing purposes. J Risk 2001;2:1727. ",
        "[56] Rapisarda F, Brigo D, Mercurio F. Parameterizing correlations: a geometric interpretation. IMA J Manag Math 2007;18:5573. ",
        "[57] Bouhlel MA, Bartoli N, Regis R, Otsmane A, Morlier J. An improved approach for estimating the hyperparameters of the Kriging model for high dimensional problems through the Partial Least Squares method. Math Probl Eng 2016;2016:6723410. ",
        "[58] Cheng GH, Younis A, Hajikolaei KH, Wang GG. Trust region based mode pursuing sampling method for global optimization of high dimensional design problems. J Mech Des 2015;137:021407. ",
        "[59] Karlsson R, Bliek L, Verwer S, de Weerdt M. Continuous surrogatebased optimization algorithms are wellsuited for expensive discrete problems. In: Artificial intelligence and machine learning. 2021. ",
        "[60] Pelamatti J, Brevault L, Balesdent M, Talbi EG, Guerin Y. Bayesian optimization of variablesize design space problems. Opt Eng 2021;22:387447. ",
        "[61] Hebbal A, Brevault L, Balesdent M, Talbi EG, Melab N. Bayesian optimization using deep Gaussian processes with applications to aerospace system design. Opt Eng 2021;22:32161. ",
        "[62] Wildberger N. A rational approach to trigonometry. Math Horiz 2007;15:1620. ",
        "[63] Cho H, Kim Y, Lee E, Choi D, Lee Y, Rhee W. Basic enhancement strategies when using bayesian optimization for hyperparameter tuning of deep neural networks. IEEE Access 2020;8:52588608. ",
        "[64] Zuniga MM, Sinoquet D. Global optimization for mixed categoricalcontinuous variables based on Gaussian process models with a randomized categorical space exploration step. INFOR Inf Syst Oper Res 2020;58:31041. ",
        "[65] Lindauer M, Eggensperger K, Feurer M, AB, Deng D, Benjamins C, et al. SMAC3: A versatile Bayesian optimization package for hyperparameter optimization. J Mach Learn Res 2022;23:19. ",
        "[66] Picheny V, Berkeley J, Moss H, Stojic H, Granta U, Ober S, et al. Trieste: Efficiently exploring the depths of blackbox functions with TensorFlow. 2023, arXiv. ",
        "[67] CowenRivers AI, Ly W, Wang Z, Tutunov R, Jianye H, Wang J, et al. HEBO: Heteroscedastic evolutionary Bayesian optimisation. 2020, arXiv. ",
        "[68] Jiang H, Shen Y, Li Y, Zhang W, Zhang C, Cui B. OpenBox: A Python toolkit for generalized blackbox optimization. 2023, arXiv.",
        "[69] Kandasamy K, Vysyaraju KR, Neiswanger W, Paria B, Collins C, Schneider J, et al. Tuning hyperparameters without grad students: Scalable and robust bayesian optimisation with dragonfly. J Mach Learn Res 2020;21:3098124. ",
        "[70] Roy S, Crossley WA, Stanford BK, Moore KT, Gray JS. A mixed integer efficient global optimization algorithm with multiple infill strategy Applied to a wing topology optimization problem. In: AIAA scitech 2019 forum. 2019. ",
        "[71] M\u00fcller J, Shoemaker CA, Pich\u00e9 R. SOMI: A surrogate model algorithm for computationally expensive nonlinear mixedinteger blackbox global optimization problems. Comput Oper Res 2013;40:1383400. ",
        "[72] Tran T, Sinoquet D, Da Veiga S, Mongeau M. Derivativefree mixed binary necklace optimization for cyclicsymmetry optimal design problems. Opt Eng 2021. ",
        "[73] Meliani M, Bartoli N, Lefebvre T, Bouhlel MA, Martins JRRA, Morlier J. Multi fidelity efficient global optimization: Methodology and application to airfoil shape design. In: AIAA aviation 2019 forum. 2019. ",
        "[74] Lee H. Gaussian processes. Springer Berlin Heidelberg; 2011, p. 5757. ",
        "[75] L\u00f3pezLopera AF, Idier D, Rohmer J, Bachoc F. Multioutput Gaussian processes with functional data: A study on coastal flood hazard assessment. Reliab Eng Syst Saf 2022;218:108139. ",
        "[76] Berthelin G, Dubreuil S, Sala\u00fcn M, Bartoli N, Gogu C. Disciplinary proper orthogonal decomposition and interpolation for the resolution of parameterized multidisciplinary analysis. Internat J Numer Methods Engrg 2022;123:3594626. ",
        "[77] Cardoso I, Dubreuil S, Bartoli N, Gogu C, Sala\u00fcn M, Lafage R. Disciplinary surrogates for gradientbased optimization of multidisciplinary systems. In: ECCOMAS Aerobest. 2023. ",
        "[78] Platt J, Penny S, Smith T, Chen T, Abarbanel H. A systematic exploration of reservoir computing for forecasting complex spatiotemporal dynamics. Neural Netw 2022;153:53052. ",
        "[79] Charayron R, Lefebvre T, Bartoli N, Morlier J. Multifidelity Bayesian optimiza tion strategy applied to overall drone design. In: AIAA scitech 2023 forum. 2023. ",
        "[80] Charayron R, Lefebvre T, Bartoli N, Morlier J. Towards a multifidelity and multiobjective Bayesian optimization efficient algorithm. Aerosp Sci Technol 2023;142:108673. ",
        "[81] Wold H. Soft modelling by latent variables: The nonlinear iterative partial least squares (NIPALS) approach. J Appl Probab 1975;12:11742. ",
        "[82] Priem R, Diouane Y, Bartoli N, Dubreuil S, Saves P. Highdimensional efficient global optimization using both random and supervised embeddings. In: AIAA aviation 2023 forum. 2023. ",
        "[83] Betz W, Papaioannou I, Straub D. Numerical methods for the discretization of random fields by means of the KarhunenLo\u00e8ve expansion. Comput Methods Appl Mech Engrg 2014;271:10929. ",
        "[84] Menz M, Dubreuil S, Morio J, Gogu C, Bartoli N, Chiron M. Variance based sen sitivity analysis for Monte Carlo and importance sampling reliability assessment with Gaussian processes. Struct Saf 2021;93:102116. ",
        "[85] Ginsbourger D, Le Riche R, Carraro L. Kriging is wellsuited to parallelize optimization. Springer Berlin Heidelberg; 2010, p. 13162. ",
        "[86] Roux E, Tillier Y, Kraria S, Bouchard PO. An efficient parallel global opti mization strategy based on Kriging properties suitable for material parameters identification. Arch Mech Eng 2020;67."
    ]
}