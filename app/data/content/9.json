{
    "info": {
        "title": "Generating Diverse Code Explanations using the GPT3 Large Language Model",
        "authors": [
            "Stephen MacNeil",
            "Andrew Tran",
            "Dan Mogil",
            "Seth Bernstein",
            "Erin Ross",
            "Ziheng Huang"
        ],
        "institutes": [
            "Temple University Philadelphia, PA, USA",
            "University of CaliforniaSan Diego La Jolla, CA, USA"
        ],
        "keywords": [
            "large language models",
            "natural language processing",
            "code explanations",
            "computer science education"
        ],
        "abstract": "Good explanations are essential to efficiently learning introductory programming concepts [10]. To provide highquality explanations at scale, numerous systems automate the process by tracing the execution of code [8, 12], defining terms [9], giving hints [16], and providing errorspecific feedback [10, 16]. However, these ap proaches often require manual effort to configure and only explain a single aspect of a given code segment. Large language models (LLMs) are also changing how students interact with code [7]. For example, Githubs Copilot can generate code for programmers [4], leading researchers to raise concerns about cheating [7]. Instead, our work focuses on LLMs potential to support learning by explain ing numerous aspects of a given code snippet. This poster features a systematic analysis of the diverse natural language explanations that GPT3 can generate automatically for a given code snippet. We present a subset of three use cases from our evolving design space of AI Explanations of Code."
    },
    "sections": {
        "1. Introduction": "Good explanations are essential to efficiently learning introductory programming concepts [10]. To provide highquality explanations at scale, numerous systems automate the process by tracing the execution of code [8, 12], defining terms [9], giving hints [16], and providing errorspecific feedback [10, 16]. However, these ap proaches often require manual effort to configure and only explain a single aspect of a given code segment. Large language models (LLMs) are also changing how students interact with code [7]. For example, Githubs Copilot can generate code for programmers [4], leading researchers to raise concerns about cheating [7]. Instead, our work focuses on LLMs potential to support learning by explain ing numerous aspects of a given code snippet. This poster features a systematic analysis of the diverse natural language explanations that GPT3 can generate automatically for a given code snippet. We present a subset of three use cases from our evolving design space of AI Explanations of Code.",
        "2. Use Cases": "To understand the types of explanations GPT3 [2] can generate, we issued over 700 prompts across numerous code snippets. An example prompt and resulting explanation is shown in Figure 1. We discovered eight explanation types and Figure 2 includes three explanation types to illustrate the explanatory power of GPT3. The additional types include: 1) tracing the execution of code, 2) fixing bugs and explaining how they were fixed, 3) generating analogies to real world settings, 4) listing relevant programming concepts, and 5) predicting the console output. Figure 1: A prompt and explanation based on analogy.",
        "2.1 Analyzing and explaining time complexity": "Instructors rate time complexity as the most difficult programming topic [17]. However, understanding time complexity is important [6, 13] because it facilitates decisionmaking so students choose an appropriate algorithm for a given problem. This use case shows GPT3 can identify and explain time complexity.",
        "2.2 Identifying common mistakes made by beginner programmers": "Commonality exists in how students solve programming prob lems [15] and the mistakes they make [1, 11]. Pedagogical tech niques, such as the muddiest point highlight these common and most confusing concepts [3, 14]. GPT3 can automatically create a checklist of common mistakes students might make regarding a given code snippet. ICER 2022, August 711, 2022, Lugano and Virtual Event, Switzerland MacNeil et al. Figure 2: Three example explanations automatically generated by GPT3 for an anonymized Binary Search code snippet.",
        "2.3 Summarizing code at multiple levels of": "abstraction Before understanding how a code snippet executes, it is often useful to understand the purpose of the code [5]. The summary gener ated by GPT3 and shown in Figure 2 defines the goal, traces the execution, and highlights relevant CS concepts such as arrays. 3 DISCUSSION Our three use cases demonstrate the potential for GPT3 to explain code for intro CS students. Our poster presentation will feature all eight explanation types as a design space of explanations to convey the diversity of explanations that can be generated by LLMs. We will highlight best practices for generating effective explanations and pitfalls that lead to less effective explanations. We are evaluating the usefulness of these explanations in a series of summer classes."
    },
    "references": [
        "[1] Amjad Altadmri and Neil CC Brown. 2015. 37 million compilations: Investigating novice programming mistakes in largescale student data. In Proceedings of the 46th ACM Technical Symposium on Computer Science Education. 522527. ",
        "[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are fewshot learners. Advances in Neural Information Processing Systems 33 (2020), 18771901. ",
        "[3] Adam Carberry, Stephen Krause, Casey Ankeny, and Cynthia Waters. 2013. Unmuddying course content using muddiest point reflections. In 2013 IEEE Frontiers in Education Conference (FIE). IEEE, 937942. ",
        "[4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021). ",
        "[5] Kathryn Cunningham, Yike Qiao, Alex Feng, and Eleanor ORourke. 2022. Bring ing \"HighLevel\" Down to Earth: Gaining Clarity in Conversational Program mer Learning Goals. In Proceedings of the 53rd ACM Technical Symposium on Computer Science Education V. 1 (Providence, RI, USA) (SIGCSE 2022). As sociation for Computing Machinery, New York, NY, USA, 551557. https: //doi.org/10.1145/3478431.3499370 ",
        "[6] Elvina Elvina and Oscar Karnalim. 2017. Complexitor: An educational tool for learning algorithm time complexity in practical manner. ComTech: Computer, Mathematics and Engineering Applications 8, 1 (2017), 2127. ",
        "[7] James FinnieAnsley, Paul Denny, Brett A. Becker, Andrew LuxtonReilly, and James Prather. 2022. The Robots Are Coming: Exploring the Implications of Ope nAI Codex on Introductory Programming. In Australasian Computing Education Conference (Virtual Event, Australia) (ACE 22). ACM, New York, NY, USA, 1019. https://doi.org/10.1145/3511861.3511863 ",
        "[8] Philip J Guo. 2013. Online python tutor: embeddable webbased program visual ization for cs education. In Proceeding of the 44th ACM technical symposium on Computer science education. 579584. ",
        "[9] Andrew Head, Codanda Appachu, Marti A Hearst, and Bj\u00f6rn Hartmann. 2015. Tutorons: Generating contextrelevant, ondemand explanations and demonstra tions of online code. In 2015 IEEE Symposium on Visual Languages and Human Centric Computing (VL/HCC). IEEE, 312. ",
        "[10] Samiha Marwan, Ge Gao, Susan Fisk, Thomas W. Price, and Tiffany Barnes. 2020. Adaptive Immediate Feedback Can Improve Novice Programming Engagement and Intention to Persist in Computer Science. In Proceedings of the 2020 ACM Conference on International Computing Education Research (Virtual Event, New Zealand) (ICER 20). Association for Computing Machinery, New York, NY, USA, 194203. https://doi.org/10.1145/3372782.3406264 ",
        "[11] Davin McCall and Michael K\u00f6lling. 2014. Meaningful categorisation of novice pro grammer errors. In 2014 IEEE Frontiers in Education Conference (FIE) Proceedings. IEEE, 18. ",
        "[12] Greg L Nelson, Benjamin Xie, and Amy J Ko. 2017. Comprehension first: eval uating a novel pedagogy and tutoring system for program tracing in CS1. In Proceedings of the 2017 ACM conference on international computing education research. 211. ",
        "[13] Miranda Parker and Colleen Lewis. 2014. What makes bigO analysis difficult: understanding how students understand runtime analysis. Journal of Computing Sciences in Colleges 29, 4 (2014), 164174. ",
        "[14] Daniel Perez, Leila Zahedi, Monique Ross, Jia Zhu, Tiffany VinciCannava, Laird Kramer, and Maria Charters. 2020. WIP: An exploration into the muddiest points 38",
        "[15] Chris Piech, Mehran Sahami, Jonathan Huang, and Leonidas Guibas. 2015. Au tonomously generating hints by inferring problem solving policies. In Proceedings of the second (2015) acm conference on learning@ scale. 195204. ",
        "[16] Thomas W Price, Yihuan Dong, and Dragan Lipovac. 2017. iSnap: towards intelligent tutoring in novice programming environments. In Proceedings of the 2017 ACM SIGCSE Technical Symposium on computer science education. 483488. ",
        "[17] Carsten Schulte and Jens Bennedsen. 2006. What do teachers teach in introductory programming?. In Proceedings of the second international workshop on Computing education research. 1728. 39"
    ]
}