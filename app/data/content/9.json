{
    "url": "",
    "publication_date": "07-08-2022",
    "title": "Generating Diverse Code Explanations using the GPT-3 Large Language Model",
    "authors": [
        "Stephen MacNeil",
        "Andrew Tran",
        "Dan Mogil",
        "Seth Bernstein",
        "Erin Ross",
        "Ziheng Huang"
    ],
    "institutes": [
        "Temple University Philadelphia, PA, USA",
        "University of CaliforniaSan Diego La Jolla, CA, USA"
    ],
    "keywords": [
        "large language models",
        "natural language processing",
        "code explanations",
        "computer science education"
    ],
    "abstract": "Good explanations are essential to efficiently learning introductory programming concepts [10]. To provide high-quality explanations at scale, numerous systems automate the process by tracing the execution of code [8, 12], defining terms [9], giving hints [16], and providing error-specific feedback [10, 16]. However, these approaches often require manual effort to configure and only explain a single aspect of a given code segment. Large language models (LLMs) are also changing how students interact with code [7]. For example, Githubs Copilot can generate code for programmers [4], leading researchers to raise concerns about cheating [7]. Instead, our work focuses on LLMs potential to support learning by explain- ing numerous aspects of a given code snippet. This poster features a systematic analysis of the diverse natural language explanations that GPT-3 can generate automatically for a given code snippet. We present a subset of three use cases from our evolving design space of AI Explanations of Code.",
    "content": "introduction good explanations are essential to efficiently learning introductory programming concepts to provide explanations at numerous systems automate the process by tracing the execution of code defining terms giving hints and providing feedback these approaches often require manual effort to configure and only explain a single aspect of a given code large language models are also changing how students interact with code for githubs copilot can generate code for programmers leading researchers to raise concerns about cheating our work focuses on llms potential to support learning by ing numerous aspects of a given code this poster features a systematic analysis of the diverse natural language explanations that can generate automatically for a given code we present a subset of three use cases from our evolving design space of ai explanations of use cases to understand the types of explanations can we issued over prompts across numerous code an example prompt and resulting explanation is shown in figure we discovered eight explanation types and figure includes three explanation types to illustrate the explanatory power of the additional types tracing the execution of fixing bugs and explaining how they were generating analogies to real world listing relevant programming and predicting the console figure a prompt and explanation based on analyzing and explaining time complexity instructors rate time complexity as the most difficult programming topic understanding time complexity is important because it facilitates so students choose an appropriate algorithm for a given this use case shows can identify and explain time identifying common mistakes made by beginner programmers commonality exists in how students solve programming lems and the mistakes they make pedagogical such as the muddiest point highlight these common and most confusing concepts can automatically create a checklist of common mistakes students might make regarding a given code icer august lugano and virtual switzerland macneil et figure three example explanations automatically generated by for an binary search code summarizing code at multiple levels of abstraction before understanding how a code snippet it is often useful to understand the purpose of the code the summary ated by and shown in figure defines the traces the and highlights relevant cs concepts such as discussion our three use cases demonstrate the potential for to explain code for intro cs our poster presentation will feature all eight explanation types as a design space of explanations to convey the diversity of explanations that can be generated by we will highlight best practices for generating effective explanations and pitfalls that lead to less effective we are evaluating the usefulness of these explanations in a series of summer",
    "references": [
        "[1] Amjad Altadmri and Neil CC Brown. 2015. 37 million compilations: Investigating novice programming mistakes in large-scale student data. In Proceedings of the 46th ACM Technical Symposium on Computer Science Education. 522\u2013527. ",
        "[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in Neural Information Processing Systems 33 (2020), 1877\u20131901. ",
        "[3] Adam Carberry, Stephen Krause, Casey Ankeny, and Cynthia Waters. 2013. Unmuddying course content using muddiest point reflections. In 2013 IEEE Frontiers in Education Conference (FIE). IEEE, 937\u2013942. ",
        "[4] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021). ",
        "[5] Kathryn Cunningham, Yike Qiao, Alex Feng, and Eleanor ORourke. 2022. Bring- ing \"High-Level\" Down to Earth: Gaining Clarity in Conversational Program- mer Learning Goals. In Proceedings of the 53rd ACM Technical Symposium on Computer Science Education V. 1 (Providence, RI, USA) (SIGCSE 2022). As- sociation for Computing Machinery, New York, NY, USA, 551\u2013557. https: //doi.org/10.1145/3478431.3499370 ",
        "[6] Elvina Elvina and Oscar Karnalim. 2017. Complexitor: An educational tool for learning algorithm time complexity in practical manner. ComTech: Computer, Mathematics and Engineering Applications 8, 1 (2017), 21\u201327. ",
        "[7] James Finnie-Ansley, Paul Denny, Brett A. Becker, Andrew Luxton-Reilly, and James Prather. 2022. The Robots Are Coming: Exploring the Implications of Ope- nAI Codex on Introductory Programming. In Australasian Computing Education Conference (Virtual Event, Australia) (ACE 22). ACM, New York, NY, USA, 10\u201319. https://doi.org/10.1145/3511861.3511863 ",
        "[8] Philip J Guo. 2013. Online python tutor: embeddable web-based program visual- ization for cs education. In Proceeding of the 44th ACM technical symposium on Computer science education. 579\u2013584. ",
        "[9] Andrew Head, Codanda Appachu, Marti A Hearst, and Bj\u00f6rn Hartmann. 2015. Tutorons: Generating context-relevant, on-demand explanations and demonstra- tions of online code. In 2015 IEEE Symposium on Visual Languages and Human- Centric Computing (VL/HCC). IEEE, 3\u201312. ",
        "[10] Samiha Marwan, Ge Gao, Susan Fisk, Thomas W. Price, and Tiffany Barnes. 2020. Adaptive Immediate Feedback Can Improve Novice Programming Engagement and Intention to Persist in Computer Science. In Proceedings of the 2020 ACM Conference on International Computing Education Research (Virtual Event, New Zealand) (ICER 20). Association for Computing Machinery, New York, NY, USA, 194\u2013203. https://doi.org/10.1145/3372782.3406264 ",
        "[11] Davin McCall and Michael K\u00f6lling. 2014. Meaningful categorisation of novice pro- grammer errors. In 2014 IEEE Frontiers in Education Conference (FIE) Proceedings. IEEE, 1\u20138. ",
        "[12] Greg L Nelson, Benjamin Xie, and Amy J Ko. 2017. Comprehension first: eval- uating a novel pedagogy and tutoring system for program tracing in CS1. In Proceedings of the 2017 ACM conference on international computing education research. 2\u201311. ",
        "[13] Miranda Parker and Colleen Lewis. 2014. What makes big-O analysis difficult: understanding how students understand runtime analysis. Journal of Computing Sciences in Colleges 29, 4 (2014), 164\u2013174. ",
        "[14] Daniel Perez, Leila Zahedi, Monique Ross, Jia Zhu, Tiffany Vinci-Cannava, Laird Kramer, and Maria Charters. 2020. WIP: An exploration into the muddiest points 38",
        "[15] Chris Piech, Mehran Sahami, Jonathan Huang, and Leonidas Guibas. 2015. Au- tonomously generating hints by inferring problem solving policies. In Proceedings of the second (2015) acm conference on learning@ scale. 195\u2013204. ",
        "[16] Thomas W Price, Yihuan Dong, and Dragan Lipovac. 2017. iSnap: towards intelligent tutoring in novice programming environments. In Proceedings of the 2017 ACM SIGCSE Technical Symposium on computer science education. 483\u2013488. ",
        "[17] Carsten Schulte and Jens Bennedsen. 2006. What do teachers teach in introductory programming?. In Proceedings of the second international workshop on Computing education research. 17\u201328. 39"
    ]
}