{
    "info": {
        "title": "Large Language Model Augmented Narrative Driven Recommendations",
        "authors": [
            "Sheshera Mysore",
            "Andrew McCallum",
            "Hamed Zamani"
        ],
        "institutes": [
            "University of Massachusetts Amherst USA",
            "University of Massachusetts Amherst USA",
            "University of Massachusetts Amherst USA"
        ],
        "keywords": [],
        "abstract": "Narrativedriven recommendation (NDR) presents an information access problem where users solicit recommendations with verbose descriptions of their preferences and context, for example, travelers soliciting recommendations for points of interest while describ ing their likes/dislikes and travel circumstances. These requests are increasingly important with the rise of natural languagebased conversational interfaces for search and recommendation systems. However, NDR lacks abundant training data for models, and current platforms commonly do not support these requests. Fortunately, classical useritem interaction datasets contain rich textual data, e.g., reviews, which often describe user preferences and context this may be used to bootstrap training for NDR models. In this work, we explore using large language models (LLMs) for data augmentation to train NDR models. We use LLMs for authoring synthetic narrative queries from useritem interactions with few shot prompting and train retrieval models for NDR on synthetic queries and useritem interaction data. Our experiments demon strate that this is an effective strategy for training smallparameter retrieval models that outperform other retrieval and LLM baselines for narrativedriven recommendation."
    },
    "sections": {
        "1. Our crossencoders are trained with": "a crossentropy loss: L\ud835\udc36\ud835\udc5f = \ud835\udc62 \ud835\udc40 \ud835\udc56=1 log( \ud835\udc52\ud835\udc60 \ud835\udc51 \ud835\udc52\ud835\udc60 ). For training, 4 negative example items \ud835\udc51 are randomly sampled from ranks 100 300 from our trained biencoder. At test time, we retrieve the top 200 items with our trained biencoder and rerank them with the crossencoder we evaluate both these components in experiments and refer to them as BiEncMint and CrEncMint. 4 EXPERIMENTS AND RESULTS Next, we evaluate Mint on a publicly available test collection for NDR and present a series of ablations. 4.1 Experimental Setup 4.1.1 Datasets. We perform evaluations on an NDR dataset for pointofinterest (POI) recommendation Pointrec [1]. Pointrec contains 112 realistic narrative queries (130 words long) obtained from discussion forums on Reddit and items pooled from baseline rankers. The items are annotated on a graded relevance scale by crowdworkers and/or discussion forum members and further vali dated by the dataset authors. The item collection C in Pointrec contains 700k POIs with metadata (category, city) and noisy text snippets describing the POI obtained from the Bing search engine. For test time ranking, we only rank the candidate items in the city and request category (e.g., Restaurants) of the query available in Pointrec this follows prior practice to exclude clearly irrelevant items [1, 26]. We use useritem interaction datasets from Yelp to generate synthetic queries for training.5 Note also that we limit our evaluations to Pointrec since it presents the only publicly avail able, manually annotated, and candidate pooled test collection for NDR, to our knowledge. Other datasets for NDR use document col lections that are no longer publicly accessible [24], contain sparse and noisy relevance judgments due to them being determined with automatic rules applied to discussion threads [18, 24], lack pooling to gather candidates for judging relevance [18, 24], or lack realistic narrative queries [21]. We leave the development of more robust test collections and evaluation methods for NDR to future work. 4.1.2 Implementation Details. Next, we describe important details for Mint and leave finer details of the model and training to our code release. To sample user interactions for generating synthetic queries from the Yelp dataset, we exclude POIs and users with fewer than ten reviews to ensure that users were regular users of the site with well represented interests. This follows common prior practice in preparing useritem interaction datasets for use [27]. Then we retain users who deliver an average rating greater than 3/5 and with 1030 aboveaverage reviews. This desirably biases our data to users who commonly describe their likings (rather than 5https://www.yelp.com/dataset dislikes). It also retains the users whose interests are summarizable by QGen. In the Yelp dataset, this results in 45,193 retained users. Now, 10,000 randomly selected users are chosen for generating syn thetic narrative queries. For these users, a single randomly selected sentence from 10 of their reviews is included in the prompt (Figure 2) to QGen, i.e., \ud835\udc41\ud835\udc62 = 10. After generating synthetic queries, some items are filtered out (3.2.2). Here, we exclude 40% of the items for a user. This results in about 60,000 training samples for training BiEncMint and CrEncMint. These decisions were made manu ally by examining the resulting datasets and the cost of authoring queries. The expense of generating \ud835\udc5e\ud835\udc62 was about USD 230. 4.1.3 Baselines. We compare BiEncMint and CrEncMint mod els against several standard and performant retrieval model base lines. These span zeroshot/unsupervised rankers, supervised bi encoders, unsupervised crossencoders, and LLM baselines. BM25: A standard unsupervised sparse retrieval baseline based on term overlap between query and document, with strong generalization performance across tasks and domains [38]. Contriver: A BERTbase biencoder model pretrained for zeroshot retrieval with weakly su pervised querydocument pairs [22]. MPNet1B: A strong Sentence Bert biencoder model initialized with MPNetbase and trained on 1 billion supervised querydocument pairs aggregated from numer ous domains [37]. BERTMSM: A BERTbase biencoder finetuned on supervised questionpassage pairs from MSMarco. UPR: A two stage approach that retrieves items with a Contriver biencoder and reranks the top 200 items with a querylikelihood model using a FlanT5 model with 3B parameters [14, 40]. This may be seen as an unsupervised crossencoder model. Grounded LLM: A re cently proposed twostage approach which autoregressively gener ates ten pseudorelevant items using an LLM (175B InstructGPT) prompted with the narrative query and generates recommenda tions grounded in C by retrieving the nearest neighbors for each generated item using a biencoder [19]. We include one fewshot example of a narrative query and recommended items in the prompt to the LLM. We run this baseline three times and report average performance across runs. We report NDCG at 5 and 10, MAP, MRR, and Recall at 100 and 200. Finally, our reported results should be considered lower bounds on realistic performance due to the un judged documents (about 70% at \ud835\udc58 = 10) in our test collections [10]. 4.2 Results Table 1 presents the performance of the proposed method compared against baselines. Here, bold numbers indicate the bestperforming model, and superscripts indicate statistical significance computed with twosided ttests at \ud835\udc5d 0.05. Here, we first note the performance of baseline approaches. We see BM25 outperformed by Contriver, a transformer biencoder model trained for zeroshot retrieval; this mirrors prior work [22]. Next, we see supervised biencoder models trained on similar pas sage (MPNet1B) and questionanswer (BERTMSM) pairs outper form a weakly supervised model (Contriver) by smaller margins. Finally, the Grounded LLM outperforms all biencoder baselines, in dicating strong fewshot generalization and mirroring prior results [19]. Examining the Mint models, we first note that the BiEnc Mint sees statistically significant improvement compared to BM25 780 Large Language Model Augmented Narrative Driven Recommendations RecSys 23, September 1822, 2023, Singapore, Singapore Table 1: Performance of the proposed method, Mint, for pointofinterest recommendation on Pointrec. The superscripts denote statistically significant improvements compared to specific baseline models. Pointrec Model Parameters NDCG@5 NDCG@10 MAP MRR Recall@100 Recall@200 1BM25 0.2682 0.2464 0.1182 0.2685 0.4194 0.5429 2Contriver 110M 0.2924 0.2776 0.1660 0.3355 0.4455 0.5552 3MPNet1B 110M 0.3038 0.2842 0.1621 0.3566 0.4439 0.5657 4BERTMSM 110M 0.3117 0.2886 0.1528 0.3320 0.4679 0.5816 5Grounded LLM 175B+110M 0.3558 0.3251 0.1808 0.3861 0.4797 0.5797 6UPR 110M+3B 0.3586 0.3242 0.1712 0.4013 0.4489 0.5552 BiEncMint 110M 0.34891 0.32631 0.18901 0.39821 0.49141 0.6221 CrEncMint 2110M 0.372512 0.348912 0.219214 0.43171 0.5448123 0.6221 and outperforms the best biencoder baselines by 1113% on preci sion measures and 57% on recall measures. Specifically, we see a model trained for questionanswering (BERTMSM) underperform BiEncMint, indicating the challenge of the NDR task. Further, BiEncMint, trained on 5 orders of magnitude lesser data than MPNet1B, sees improved performance indicating the quality of data obtained from Mint. Furthermore, BiEncMint also performs at par with a 175B LLM while offering the inference efficiency of a smallparameter biencoder. Next, we see CrEncMint outperform the baseline biencoders, BiEncMint, UPR, and Grounded LLM by 421% on precision measures and 713% on recall measures demonstrating the value of Mint for training NDR models. 4.3 Ablations In Table 2, we ablate various design choices in Mint. Different choices result in different training sets for the BiEnc and CrEnc models. Also, note that in reporting ablation performance for CrEnc, we still use the performant BiEncMint model for obtaining nega tive examples for training and firststage ranking. Without high quality negative examples, we found CrEnc to result in much poorer performance. No item filtering. Since synthetic queries are unlikely to rep resent all the items of a user, Mint excludes user items {\ud835\udc51\ud835\udc56}\ud835\udc41\ud835\udc62 \ud835\udc56=1 which have a low likelihood of being generated from the document (3.2.2). Without this step, we expect the training set for training retrieval models to be larger and noisier. In Table 2, we see that excluding this step leads to a lower performance for BiEnc and CrEnc, indicating that the quality of data obtained is important for performance. 6B LLM for QGen. Mint relies on using an expensive 175B pa rameter InstructGPT model for QGen. Here, we investigate the efficacy for generating\ud835\udc5e\ud835\udc62 for {\ud835\udc51\ud835\udc56}\ud835\udc41\ud835\udc62 \ud835\udc56=1 with a 6B parameter Instruct GPT model (textcurie001). We use an identical setup to the 175B LLM for this. In Table 2, we see that training on the synthetic narrative queries of the smaller LLM results in worse models of ten underperforming the baselines in Table",
        "1. This indicates the": "inability of a smaller model to generate complex narrative queries while conditioning on a set of user items. This necessity of a larger LLM for generating queries in complex retrieval tasks has been observed in prior work [15, 23]. 6B LLM for Item Queries. We find a smaller 6B LLM to result in poor quality data when used to generate narrative queries con ditioned on {\ud835\udc51\ud835\udc56}\ud835\udc41\ud835\udc62 \ud835\udc56=",
        "1. Here we simplify the text generation task": "using a 6B LLM to generate queries for individual items \ud835\udc51\ud835\udc56. This experiment also mirrors the setup for generating synthetic queries for search tasks [7, 15]. Here, we use 3few shot examples and sam ple one item per user for generating \ud835\udc5e\ud835\udc62. Given the lower cost of using a smaller LLM, we use all 45,193 users in our Yelp dataset rather than a smaller random sample. From Table 2, we see that this results in higher quality queries than using smaller LLMs for gen erating narrative queries from {\ud835\udc51\ud835\udc56}\ud835\udc41\ud835\udc62 \ud835\udc56=",
        "1. The resulting BiEnc model": "underperforms the BiEncMint, indicating the value of generating complex queries conditioned on multiple items as in Mint for NDR. We see that CrEnc approaches the performance of CrEncMint note, however, that this approach uses the performant BiEncMint for sampling negatives and first stage ranking. We leave further exploration of using small parameter LLMs for data augmentation for NDR models to future work. 5 CONCLUSIONS In this paper, we present Mint, a data augmentation method for the narrativedriven recommendation (NDR) task. Mint repurposes historical useritem interaction datasets for NDR by using a 175B pa rameter large language model to author longform narrative queries while conditioning on the text of items liked by users. We evaluate biencoder and crossencoder models trained on data from Mint on the publicly available Pointrec test collection for narrativedriven point of interest recommendation. We demonstrate that the result ing models outperform several strong baselines and ablated models and match or outperform a 175B LLM directly used for NDR in a 1shot setup. However, Mint also presents some limitations. Given our use of historical interaction datasets for generating synthetic training data and the prevalence of popular interests in these datasets longer, tailed interests are unlikely to be present in the generated syn thetic datasets. In turn, causing retrieval models to likely see poorer performance on these requests. Our use of LLMs to generate syn thetic queries also causes the queries to be repetitive in structure, likely causing novel longertail queries to be poorly served. These limitations may be addressed in future work. 781 RecSys 23, September 1822, 2023, Singapore, Singapore Mysore, McCallum, Zamani Table 2: Mint ablated for different design choices on Pointrec. Pointrec Ablation NDCG@5 NDCG@10 MAP MRR Recall@100 Recall@200 BiEncMint 0.3489 0.3263 0.1890 0.3982 0.5263 0.6221 No item filtering 0.2949 0.2766 0.1634 0.3505 0.4979 0.5951 6B LLM for QGen 0.2336 0.2293 0.1125 0.2287 0.426 0.5435 6B LLM for Item Queries 0.3012 0.2875 0.1721 0.3384 0.4800 0.5909 CrEncMint 0.3725 0.3489 0.2192 0.4317 0.5448 0.6221 No item filtering 0.3570 0.3379 0.2071 0.4063 0.5366 0.6221 6B LLM for QGen 0.2618 0.2421 0.1341 0.3118 0.4841 0.6221 6B LLM for Item Queries 0.3792 0.3451 0.2128 0.4098 0.5546 0.6221 Besides this, other avenues also present rich future work. While Mint leverages a 175B LLM for generating synthetic queries, smaller parameter LLMs may be explored for this purpose perhaps by training dedicated QGen models. Mint may also be expanded to explore more active strategies for sampling items and users for whom narrative queries are authored this may allow more effi cient use of large parameter LLMs while ensuring higher quality training datasets. Next, the generation of synthetic queries from sets of documents may be explored for a broader range of retrieval tasks beyond NDR given its promise to generate larger training sets a currently underexplored direction. Finally, given the lack of largerscale test collections for NDR and the effectiveness of LLMs for authoring narrative queries from useritem interaction, fruitful future work may also explore the creation of largerscale datasets in a mixedinitiative setup to robustly evaluate models for NDR. ACKNOWLEDGMENTS We thank anonymous reviewers for their invaluable feedback. This work was partly supported by the Center for Intelligent Informa tion Retrieval, NSF grants IIS1922090 and 2143434, the Office of Naval Research contract number N000142212688, an Amazon Alexa Prize grant, and the Chan Zuckerberg Initiative under the project Scientific Knowledge Base Construction. Any opinions, findings and conclusions or recommendations expressed here are those of the authors and do not necessarily reflect those of the sponsors."
    },
    "references": [
        "[1] Jafar Afzali, Aleksander Mark Drzewiecki, and Krisztian Balog. 2021. POINTREC: A Test Collection for NarrativeDriven Point of Interest Recommendation. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, Canada) (SIGIR 21). As sociation for Computing Machinery, New York, NY, USA, 24782484. https: //doi.org/10.1145/3404835.3463243 ",
        "[2] Jaime Arguello, Adam Ferguson, Emery Fine, Bhaskar Mitra, Hamed Zamani, and Fernando Diaz. 2021. Tip of the Tongue KnownItem Retrieval: A Case Study in Movie Identification. In Proceedings of the 6th international ACM SIGIR Conference on Human Information Interaction and Retrieval. ACM. https://dlnext.acm.org/ doi/10.1145/3406522.3446021 ",
        "[3] Toine Bogers, Maria G\u00e4de, Marijn Koolen, Vivien Petras, and Mette Skov. 2018. What was this Movie About this Chick? A Comparative Study of Relevance Aspects in Book and Movie Discovery. In Transforming Digital Worlds: 13th Inter national Conference, iConference 2018, Sheffield, UK, March 2528, 2018, Proceedings 13. Springer, 323334. ",
        "[4] Toine Bogers, Maria G\u00e4de, Marijn Koolen, Vivien Petras, and Mette Skov. 2019. Looking for an amazing game I can relax and sink hours into...: A Study of Relevance Aspects in Video Game Discovery. In Information in Contemporary Society: 14th International Conference, iConference 2019, Washington, DC, USA, March 31April 3, 2019, Proceedings 14. Springer, 503515. ",
        "[5] Toine Bogers and Marijn Koolen. 2017. Defining and Supporting NarrativeDriven Recommendation. In Proceedings of the Eleventh ACM Conference on Recommender Systems (Como, Italy) (RecSys 17). Association for Computing Machinery, New York, NY, USA, 238242. https://doi.org/10.1145/3109859.3109893 ",
        "[6] Toine Bogers and Marijn Koolen. 2018. Im looking for something like...: Combining Narratives and Example Items for Narrativedriven Book Recommen dation. In Knowledgeaware and Conversational Recommender Systems Workshop. CEUR Workshop Proceedings. ",
        "[7] Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022. InPars: Unsupervised Dataset Generation for Information Retrieval. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (Madrid, Spain) (SIGIR 22). Association for Computing Machinery, New York, NY, USA, 23872392. https://doi.org/10.1145/3477495. 3531863 ",
        "[8] Leonid Boytsov, Preksha Patel, Vivek Sourabh, Riddhi Nisar, Sayani Kundu, Ramya Ramanathan, and Eric Nyberg. 2023. InParsLight: CostEffective Unsu pervised Training of Efficient Rankers. arXiv:2301.02998 ",
        "[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel HerbertVoss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are FewShot Learners. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ran zato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 18771901. https://proceedings.neurips.cc/paper_files/paper/2020/file/ 1457c0d6bfcb4967418bfb8ac142f64aPaper.pdf ",
        "[10] Chris Buckley and Ellen M. Voorhees. 2004. Retrieval Evaluation with Incomplete Information. In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (Sheffield, United Kingdom) (SIGIR 04). Association for Computing Machinery, New York, NY, USA, 2532. https://doi.org/10.1145/1008992.1009000 ",
        "[11] DongKyu Chae, Jihoo Kim, Duen Horng Chau, and SangWook Kim. 2020. AR CF: Augmenting Virtual Users and Items in Collaborative Filtering for Addressing ColdStart Problems. In Proceedings of the 43rd International ACM SIGIR Con ference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR 20). Association for Computing Machinery, New York, NY, USA, 12511260. https://doi.org/10.1145/3397271.3401038 ",
        "[12] Lei Chen, Le Wu, Kun Zhang, Richang Hong, Defu Lian, Zhiqiang Zhang, Jun Zhou, and Meng Wang. 2023. Improving Recommendation Fairness via Data Augmentation. In Proceedings of the ACM Web Conference 2023 (Austin, TX, USA) (WWW 23). Association for Computing Machinery, New York, NY, USA, 10121020. https://doi.org/10.1145/3543507.3583341 ",
        "[13] Li Chen, Zhirun Zhang, Xinzhi Zhang, and Lehong Zhao. 2022. A Pilot Study for Understanding Users Attitudes Towards a Conversational Agent for News Recommendation. In Proceedings of the 4th Conference on Conversational User Interfaces (Glasgow, United Kingdom) (CUI 22). Association for Computing Machinery, New York, NY, USA, Article 36, 6 pages. https://doi.org/10.1145/ 3543829.3544530 ",
        "[14] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instructionfinetuned language models. arXiv preprint arXiv:2210.11416 (2022). ",
        "[15] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov, Kelvin Guu, Keith Hall, and MingWei Chang. 2023. Promptagator: Fewshot 782",
        "[16] Abhinandan S. Das, Mayur Datar, Ashutosh Garg, and Shyam Rajaram. 2007. Google News Personalization: Scalable Online Collaborative Filtering. In Pro ceedings of the 16th International Conference on World Wide Web (Banff, Alberta, Canada) (WWW 07). Association for Computing Machinery, New York, NY, USA, 271280. https://doi.org/10.1145/1242572.1242610 ",
        "[17] James Davidson, Benjamin Liebald, Junning Liu, Palash Nandy, Taylor Van Vleet, Ullas Gargi, Sujoy Gupta, Yu He, Mike Lambert, Blake Livingston, and Dasarathi Sampath. 2010. The YouTube Video Recommendation System. In Proceedings of the Fourth ACM Conference on Recommender Systems (Barcelona, Spain) (RecSys 10). Association for Computing Machinery, New York, NY, USA, 293296. https: //doi.org/10.1145/1864708.1864770 ",
        "[18] Lukas Eberhard, Simon Walk, Lisa Posch, and Denis Helic. 2019. Evaluating NarrativeDriven Movie Recommendations on Reddit. In Proceedings of the 24th International Conference on Intelligent User Interfaces (Marina del Ray, California) (IUI 19). Association for Computing Machinery, New York, NY, USA, 111. https: //doi.org/10.1145/3301275.3302287 ",
        "[19] Luyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022. Precise ZeroShot Dense Retrieval without Relevance Labels. arXiv preprint arXiv:2212.10496 (2022). ",
        "[20] Negar Hariri, Bamshad Mobasher, and Robin Burke. 2013. QueryDriven Context Aware Recommendation. In Proceedings of the 7th ACM Conference on Recom mender Systems (Hong Kong, China) (RecSys 13). Association for Computing Machinery, New York, NY, USA, 916. https://doi.org/10.1145/2507157.2507187 ",
        "[21] Seyyed Hadi Hashemi, Jaap Kamps, Julia Kiseleva, Charles LA Clarke, and Ellen M Voorhees. 2016. Overview of the TREC 2016 Contextual Suggestion Track.. In TREC. ",
        "[22] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo janowski, Armand Joulin, and Edouard Grave. 2022. Unsupervised Dense Infor mation Retrieval with Contrastive Learning. Transactions on Machine Learning Research (2022). https://openreview.net/forum?id=jKN1pXi7b0 ",
        "[23] Vitor Jeronymo, Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, Roberto Lotufo, Jakub Zavrel, and Rodrigo Nogueira. 2023. InParsv2: Large Language Models as Efficient Dataset Generators for Information Retrieval. arXiv:2301.01820 ",
        "[24] Marijn Koolen, Toine Bogers, Maria G\u00e4de, Mark Hall, Iris Hendrickx, Hugo Huurdeman, Jaap Kamps, Mette Skov, Suzan Verberne, and David Walsh. 2016. Overview of the CLEF 2016 Social Book Search Lab. In Experimental IR Meets Mul tilinguality, Multimodality, and Interaction, Norbert Fuhr, Paulo Quaresma, Teresa Gon\u00e7alves, Birger Larsen, Krisztian Balog, Craig Macdonald, Linda Cappellato, and Nicola Ferro (Eds.). Springer International Publishing, Cham, 351370. ",
        "[25] Megan Leszczynski, Ravi Ganti, Shu Zhang, Krisztian Balog, Filip Radlinski, Fernando Pereira, and Arun Tejasvi Chaganty. 2023. Generating Synthetic Data for Conversational Music Recommendation Using Random Walks and Language Models. arXiv:2301.11489 ",
        "[26] Xin Liu, Yong Liu, Karl Aberer, and Chunyan Miao. 2013. Personalized Pointof Interest Recommendation by Mining Users Preference Transition. In Proceedings of the 22nd ACM International Conference on Information & Knowledge Manage ment (San Francisco, California, USA) (CIKM 13). Association for Computing Ma chinery, New York, NY, USA, 733738. https://doi.org/10.1145/2505515.2505639 ",
        "[27] Yiding Liu, TuanAnh Nguyen Pham, Gao Cong, and Quan Yuan. 2017. An Experimental Evaluation of PointofInterest Recommendation in LocationBased Social Networks. Proc. VLDB Endow. 10, 10 (jun 2017), 10101021. https://doi. org/10.14778/3115404.3115407 ",
        "[28] Federico L\u00f3pez, Martin Scholz, Jessica Yung, Marie Pellat, Michael Strube, and Lucas Dixon. 2021. Augmenting the useritem graph with textual similarity models. arXiv preprint arXiv:2109.09358 (2021). ",
        "[29] Xing Han Lu, Siva Reddy, and Harm de Vries. 2023. The StatCan Dialogue Dataset: Retrieving Data Tables through Conversations with Genuine Intents. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics. Association for Computational Linguistics, Dubrovnik, Croatia, 27992829. https://aclanthology.org/2023.eaclmain.206 ",
        "[30] Kai Luo, Scott Sanner, Ga Wu, Hanze Li, and Hojin Yang. 2020. Latent Linear Critiquing for Conversational Recommender Systems. In The Web Conference. ",
        "[31] Ji Ma, Ivan Korotkov, Yinfei Yang, Keith Hall, and Ryan McDonald. 2021. Zeroshot Neural Passage Retrieval via Domaintargeted Synthetic Question Generation. In Proceedings of the 16th Conference of the European Chapter of the Associa tion for Computational Linguistics: Main Volume. Association for Computational Linguistics, Online, 10751088. https://doi.org/10.18653/v1/2021.eaclmain.92 ",
        "[32] Sheshera Mysore, Tim OGorman, Andrew McCallum, and Hamed Zamani. 2021. CSFCube A Test Collection of Computer Science Research Articles for Faceted Query by Example. In Thirtyfifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). https://doi.org/10.48550/arXiv. 2103.12906 ",
        "[33] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Eds.), Vol. 35. Curran Associates, Inc., 2773027744. https://proceedings.neurips.cc/paper_files/paper/2022/file/ b1efde53be364a73914f58805a001731PaperConference.pdf ",
        "[34] Andrea Papenmeier, Dagmar Kern, Daniel Hienert, Alfred Sliwa, Ahmet Aker, and Norbert Fuhr. 2021. Starting Conversations with Search Engines Interfaces That Elicit Natural Language Queries. In Proceedings of the 2021 Conference on Human Information Interaction and Retrieval (Canberra ACT, Australia) (CHIIR 21). Association for Computing Machinery, New York, NY, USA, 261265. https: //doi.org/10.1145/3406522.3446035 ",
        "[35] Gustavo Penha, Enrico Palumbo, Maryam Aziz, Alice Wang, and Hugues Bouchard. 2023. Improving Content Retrievability in Search with Controllable Query Generation. In Proceedings of the ACM Web Conference 2023 (Austin, TX, USA) (WWW 23). Association for Computing Machinery, New York, NY, USA, 31823192. https://doi.org/10.1145/3543507.3583261 ",
        "[36] Filip Radlinski, Krisztian Balog, Fernando Diaz, Lucas Dixon, and Ben Wedin. 2022. On Natural Language User Profiles for Transparent and Scrutable Rec ommendation. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval (Madrid, Spain) (SIGIR 22). Association for Computing Machinery, New York, NY, USA, 28632874. https://doi.org/10.1145/3477495.3531873 ",
        "[37] Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence Embeddings using Siamese BERTNetworks. In Proceedings of the 2019 Conference on Em pirical Methods in Natural Language Processing. Association for Computational Linguistics. https://arxiv.org/abs/1908.10084 ",
        "[38] Stephen Robertson and Hugo Zaragoza. 2009. The Probabilistic Relevance Framework: BM25 and Beyond. Found. Trends Inf. Retr. 3, 4 (apr 2009), 333389. https://doi.org/10.1561/1500000019 ",
        "[39] Jon SaadFalcon, Omar Khattab, Keshav Santhanam, Radu Florian, Martin Franz, Salim Roukos, Avirup Sil, Md Arafat Sultan, and Christopher Potts. 2023. UDAPDR: Unsupervised Domain Adaptation via LLM Prompting and Distillation of Rerankers. arXiv:2303.00807 [cs.IR] ",
        "[40] Devendra Sachan, Mike Lewis, Mandar Joshi, Armen Aghajanyan, Wentau Yih, Joelle Pineau, and Luke Zettlemoyer. 2022. Improving Passage Retrieval with ZeroShot Question Generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 37813797. https://aclanthology. org/2022.emnlpmain.249 ",
        "[41] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu. 2020. MPNet: Masked and Permuted Pretraining for Language Understanding. In Advances in Neural Information Processing Systems, Vol. 33. https://proceedings.neurips.cc/paper_ files/paper/2020/file/c3a690be93aa602ee2dc0ccab5b7b67ePaper.pdf ",
        "[42] Jaime Teevan, Susan T. Dumais, and Eric Horvitz. 2005. Personalizing Search via Automated Analysis of Interests and Activities. In Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (Salvador, Brazil) (SIGIR 05). Association for Computing Machinery, New York, NY, USA, 449456. https://doi.org/10.1145/1076034.1076111 ",
        "[43] Mengting Wan and Julian McAuley. 2018. Item Recommendation on Monotonic Behavior Chains. In Proceedings of the 12th ACM Conference on Recommender Systems (Vancouver, British Columbia, Canada) (RecSys 18). Association for Computing Machinery, New York, NY, USA, 8694. https://doi.org/10.1145/ 3240323.3240369 ",
        "[44] Haonan Wang, Chang Zhou, Carl Yang, Hongxia Yang, and Jingrui He. 2021. Controllable Gradient Item Retrieval. In Web Conference. ",
        "[45] Qinyong Wang, Hongzhi Yin, Hao Wang, Quoc Viet Hung Nguyen, Zi Huang, and Lizhen Cui. 2019. Enhancing Collaborative Filtering with Generative Aug mentation. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (Anchorage, AK, USA) (KDD 19). As sociation for Computing Machinery, New York, NY, USA, 548556. https: //doi.org/10.1145/3292500.3330873 ",
        "[46] Jiajing Xu, Andrew Zhai, and Charles Rosenberg. 2022. Rethinking Personalized Ranking at Pinterest: An EndtoEnd Approach. In Proceedings of the 16th ACM Conference on Recommender Systems (Seattle, WA, USA) (RecSys 22). Association for Computing Machinery, New York, NY, USA, 502505. https://doi.org/10. 1145/3523227.3547394 ",
        "[47] Yuxin Ying, Fuzhen Zhuang, Yongchun Zhu, Deqing Wang, and Hongwei Zheng. 2023. CAMUS: AttributeAware Counterfactual Augmentation for Minority Users in Recommendation. In Proceedings of the ACM Web Conference 2023 (Austin, TX, USA) (WWW 23). Association for Computing Machinery, New York, NY, USA, 13961404. https://doi.org/10.1145/3543507.3583538 ",
        "[48] Hamed Zamani, Johanne R Trippas, Jeff Dalton, and Filip Radlinski. 2022. Con versational information seeking. arXiv preprint arXiv:2201.08808 (2022). ",
        "[49] Jie Zou, Yifan Chen, and Evangelos Kanoulas. 2020. Towards QuestionBased Recommender Systems. In Proceedings of the 43rd International ACM SIGIR Confer ence on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR 20). Association for Computing Machinery, New York, NY, USA, 881890. https://doi.org/10.1145/3397271.3401180 783"
    ]
}