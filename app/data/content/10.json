{
    "info": {
        "title": "The Programmers Assistant: Conversational Interaction with a Large Language Model for Software Development",
        "authors": [
            "Steven I. Ross",
            "Fernando Martinez",
            "Stephanie Houde",
            "Michael Muller",
            "Justin D. Weisz"
        ],
        "institutes": [
            "IBM Research AI Cambridge, MA, USA",
            "IBM Argentina La Plata, Buenos Aires, Argentina",
            "IBM Research AI Yorktown Heights, NY, USA"
        ],
        "keywords": [
            "codefluent large language models",
            "foundation models",
            "conversational interaction",
            "humancentered AI"
        ],
        "abstract": "Large language models (LLMs) have recently been applied in soft ware engineering to perform tasks such as translating code between programming languages, generating code from natural language, and autocompleting code as it is being written. When used within development tools, these systems typically treat each model in vocation independently from all previous invocations, and only a specific limited functionality is exposed within the user interface. This approach to user interaction misses an opportunity for users to more deeply engage with the model by having the context of their previous interactions, as well as the context of their code, inform the models responses. We developed a prototype system the Programmers Assistant in order to explore the utility of conversational interactions grounded in code, as well as software engineers receptiveness to the idea of conversing with, rather than invoking, a codefluent LLM. Through an evaluation with 42 partici pants with varied levels of programming experience, we found that our system was capable of conducting extended, multiturn discus sions, and that it enabled additional knowledge and capabilities beyond code generation to emerge from the LLM. Despite skepti cal initial expectations for conversational programming assistance, participants were impressed by the breadth of the assistants capa bilities, the quality of its responses, and its potential for improving their productivity. Our work demonstrates the unique potential of conversational interactions with LLMs for cocreative processes like software development."
    },
    "sections": {
        "1. This conversation begins with the assistant": "greeting the user (line 1). Next, the user asks a general Python programming question (line 4), to which the assistant responds with a nonauthoritative remark (I think...) and a code snippet (line 9). The user next asks a followup question that depends on their previous question and the assistants response (line 11), to which the assistant provides another code snippet (line 15), satisfying the users request. The user then switches topics and asks the assistant to write a Fibonacci function (line 17), and the assistant again responds with a nonauthoritative remark (I will give it a try, line 20) and a block of code. The user then asks how the function works (line 30) and the assistant provides an adequate description (line 32). 495 IUI 23, March 2731, 2023, Sydney, NSW, Australia Ross et al. Figure 2: Code Preview. Long code responses ( 10 lines) are presented in a popup window (A), with a button to copy the code and close the window. This code can be reaccessed from the chat transcript by clicking the proxy icon (B). Next, the user asks the assistant to reimplement the function in a different way (line 37), again leveraging the ability to ask followup questions. The assistant produces an alternative implementation that conforms to the users request (line 41). The user follows up with a question that depends on multiple past utterances and re sponses in the chat transcript (line 47), and the assistant produces a relevant response (line 49). The conversation closes with the user thanking the assistant (line 53) and the assistant acknowledging their gratitude (line 55). Listing 1: A conversation with the Programmers Assistant. Code presented by the assistant is listed in bold face. 1 Socrates: Hello! My name is Socrates your programming 2 assistant. What can I help you with today? 3 4 User: How do I get a list of the keys in a 5 Python dictionary? 6 7 Socrates: I think you would do something like this: 8 9 list(my_dict.keys()) 10 11 User: What if I just wanted to iterate over them? 12 13 Socrates: I think you would do something like this: 14 15 for key in my_dict: 16 17 User: Write a function that returns the nth value 18 in the Fibonacci sequence. 19 20 Socrates: I will give it a try. 21 22 def fib(n): 23 if n == 0: 24 return 0 25 elif n == 1: 26 return 1 27 else: 28 return fib(n 1) + fib(n 2) 29 30 User: How does this work? 31 32 Socrates: The Fibonacci sequence is defined by the 33 recurrence relation F(n) = F(n1) + F(n2). 34 This function implements that recurrence 35 relation. 36 37 User: Can this be done without recursion? 38 496 The Programmers Assistant IUI 23, March 2731, 2023, Sydney, NSW, Australia 39 Socrates: I think so. How about this? 40 41 def fib(n): 42 a,b = 0,1 43 for i in range(n): 44 a,b = b,a+b 45 return 46 47 User: Which version is better? 48 49 Socrates: I think the recursive version is more 50 elegant , but the iterative version is more 51 efficient. 52 53 User: Thanks! 54 55 Socrates: You 're welcome! 4 EMPIRICAL STUDY OF CONVERSATIONAL PROGRAMMING ASSISTANCE We conducted an empirical user study of the Programmers Assis tant to assess whether conversational assistance provides value in a software engineering context11. Our inquiry focused on the user experience and value of conversational interactions grounded in code. We therefore designed a qualitative study to investigate atti tudes toward a conversational programming assistant: do people enjoy interacting conversationally, what kinds of questions do they ask, and how does the experience compare to other forms of pro gramming support such as searching the web? We note that prior studies (e.g. [103, 105, 109]) conducted quantitative examinations of the use of LLMs in code work; our study is akin to Weisz et al.s qualitative examination of software engineers attitudes toward working with models that may fail to produce working code [102]. To address our questions, we deployed the Programmers Assis tant within our organization a global technology company and invited people to try it out and give us feedback on their experience. We invited people with varying levels of programming skill in order to obtain a wide range of feedback on the kinds of use cases for which the tool could provide assistance. 4.1 Tasks We set up the Programmers Assistant as a playground environment that participants could try out with a few sample programming problems. We created a tutorial to orient participants to the assis tant, its capabilities, and how to interact with it. We also created four programming challenges focused on writing code, document ing code, and writing tests for code. We designed these challenges to expose participants to a broad range of the assistants capabilities. For each of these challenges, we explicitly did not evaluate metrics such as the participants productivity, the quality of their solutions, or the time taken to produce them, as the focus of our study was to understand the utility of conversational interaction. We selected Python as the language used for the tutorial and challenges because of its general popularity [21] and the fact that it was wellsupported by our underlying LLM [24]. 4.1.1 Tutorial. All participants were first introduced to the Pro grammers Assistant through a tutorial. The tutorial walked each 11For historical context, we note that our study was completed before the public release of ChatGPT [60], which has subsequently demonstrated the application of conversational assistance for programming tasks [34]. participant through 10 sample interactions to give them a feeling for what the assistant could do and how to interact with it. The tutorial demonstrated how to ask questions, how to request code to be generated, and how to evaluate existing code. It did not specifi cally cover how to generate documentation or unit tests. Tutorial instructions were provided within the code editor. We include the specific text used for the tutorial in Appendix B. 4.1.2 Programming Challenges. After completing the tutorial, par ticipants unlocked four programming challenges. Two of the chal lenges involved coding problems (writing a queue class and writing code to create a scatterplot of data in a CSV file), one involved doc umenting a given function (an implementation of a graph search algorithm), and one involved writing unit tests for a given func tion (computing the greatest common divisor of two arguments). Although the Programmers Assistant was visible and available for use, we provided no specific requirement that it actually be used to complete the challenges. After participants completed their solution to a challenge, they submitted it by clicking a button in the UI. The code editor used in the Programmers Assistant was not a fullyfunctional IDE and did not provide syntax checking or the ability to run, test, or debug code. Due to these limitations, participants were asked to submit their solutions when they felt they had completed the challenge to their own satisfaction. 4.2 Participants To recruit participants for our study, we posted internal advertise ments in various communications channels focused on software engineering. Our advertisements stated that we were evaluating a conversational programming assistant, but were kept deliberately vague in order to minimize the impact on peoples expectations of the experience. Our advertisement yielded a pool of 140 potential participants. In order to recruit a diverse sample, we used a screening survey that asked about their job role, their familiarity with and recency of use of Python, and their availability to participate in our study. We accepted participants into the study on a rolling basis, selecting participants to capture a range of programming experiences and ensure balanced gender representation. We conducted periodic re views to determine whether we were learning something new from each participant or if we had reached the point of saturation [7]. We stopped collecting data after running 42 participants as we were no longer observing any new behaviors or gleaning any new insights. The Programmers Assistant implementation and configuration were held constant over the course of the study; no changes to the UI design or LLM prompt were made. Our participants had the following selfidentified characteristics: Job role: 19 Software Engineers, 12 Researcher/Scientists, 3 Software Architects, 2 Data Scientists, 1 Machine Learning Engineer, 1 Systems Test Engineer, 1 Business Analyst, 1 Manager, 1 Marketer, and 1 Consultant. Gender: 21 Female, 19 Male, 1 Gender Variant / Noncon forming, and 1 Preferred not to say. Python Experience: 17 participants had 3+ years of Python experience, 11 had 13 years, 11 had less than 1 year, and 3 were not familiar with Python. 497 IUI 23, March 2731, 2023, Sydney, NSW, Australia Ross et al. Recency of Python Use: 29 participants had written Python code within the past month, 4 within the past year, 5 within the past 5 years, and 4 had not written Python code within the past 5 years. We provide full demographic information for individual partici pants in Appendix E. 4.3 Procedure Participants completed the study on their own time, independently and without moderation. Each participant was provided with a web link to a prestudy survey that described the nature of the study and the tasks that they would be expected to perform. They were then directed to the Programmers Assistant to complete the tutorial and the four programming challenges. When participants indicated they were finished with the challenges12, they were directed to a final poststudy survey. Complete sessions generally required about an hour of effort, though some participants spread their effort across a longer period of time and across multiple sessions. Participants were compensated for their time at a rate equivalent to US $15/hr. 4.4 Measures We collected a variety of data in our study from three sources: (1) Surveys. We employed three surveys in the study: a pre study survey to collect demographic information, a pretask survey to gauge expectations of the conversational user ex perience, and a posttask survey to assess actual user expe rience. We describe these survey questions in the relevant context of our results, and we provide a complete listing of all survey instruments in Appendix A. (2) Event logs. The Programmers Assistant was instrumented to collect data on participants usage. The event logs pro vided timestamped records of interaction events, including conversational exchanges, hiding/showing the assistant, use of the try again and start over features, and use of copy/ paste. (3) Conversation logs. From the event logs, we extracted con versational transcripts between each participant and the Programmers Assistant. 5 RESULTS 5.1 Data & Analysis We collected a wealth of data in our study: 126 survey responses from three surveys per participant, containing 296 written com ments in openended survey questions, and 4,877 instances of 23 different types of UI events, including 1,699 conversational ex changes13 in the event logs. We also compute, for each participant, counts or durations for 21 different metrics from the event logs. In our analysis, we deliberately exclude the portion of our data collected during the tutorial exercise. We exclude this data because that activity was guided by the tutorial instructions, not by our participants own initiative. Thus, our final sample consists of 3,172 12We did not enforce that participants actually complete all of the challenges. Never theless, all participants but one did submit solutions to all of the challenges. 13We refer to a participants utterance, followed by the assistants response, as a conversational exchange. events, including 968 conversational exchanges in the event logs; no survey data was excluded. Our primary analysis of this data is qualitative, as our partici pants provided us with a rich source of interesting feedback and thoughtprovoking insights in their comments. Where applicable, we supplement this data with quantitative data from the survey and the event logs, as well as chat transcript data from the con versation logs. In this way, we triangulate [47] across our three data sources, using the openended survey data as a foundation. When we quote participants, either from their qualitative survey responses or the conversational transcripts, we reproduce their words exactly as typed, including typos, misspellings, grammati cal errors, capitalization, and potential trigger words, and we only make minor clarifying edits where needed, delineated by square brackets. In order to set the context for our analysis, we first describe how we used reflexive thematic analysis to analyze participants responses to the openended survey questions. We then describe our analysis of the conversation logs and our development of a coding guide based on Conversation Analysis [76], and specifically, Moore and Arars Natural Conversation Framework [50]. 5.1.1 Thematic Analysis of Qualitative Survey Responses. We con ducted a reflexive thematic analysis to analyze the responses to our seven openended survey questions. We followed the process described by Braun and Clarke [16] in which researchers immerse themselves in the data, generate codes for material that seems interesting, and then iteratively group and refine codes through collaborative discussion in order to identify higherlevel themes. Initially, four authors performed opencoding on the openended survey responses. Through discussion, these codes were grouped and consolidated into a single set, which were then reapplied to the data by two authors. After another round of discussion, these authors identified a set of 12 higherlevel themes. Some themes had clear parallels to quantitative survey questions or event log data, and thus represented clear instances where we were able to trian gulate across data sources. Other themes surprised us. We structure our presentation of the results based on these 12 themes, grouped into three different aspects of the user experience: expectations and experience, utility of conversational assistance, and patterns of interaction and mental models. 5.1.2 Conversation Analysis via the Natural Conversation Frame work. In order to understand the content and structure of the conversations that took place between our participants and the Programmers Assistant, we turned to the Natural Conversation Framework [50] (NCF). We developed a codebook for the event logs, beginning with 21 different categories of utterances from the NCF. Nine NCF categories Acknowledgment, Apology, Confirma tion, Expression of Gratitude, Farewell, Greeting, SelfIdentification, Welfare Check, and Welfare Report appeared twice in our code book to distinguish cases in which the utterance was made by the human participant vs. the assistant. Other NCF categories were split to provide nuanced detail about the interaction; for example, we distinguished three different kinds of NCF requests, depending upon whether they were stated as Requests for Action (e.g. Would you...), Commands of Action (e.g. Write a function that...), or Expressions of Desire (e.g. I want...). We also added 18 additional 498 The Programmers Assistant IUI 23, March 2731, 2023, Sydney, NSW, Australia Interlocutor Orientation Codes Human Social Acknowledgment, Apology, Criticism, Expression of Gratitude, Farewell, Greeting, Politeness, Praise, Self Identification, Small Talk, Welfare Check, Welfare Report Task Asks Question, Asserts Information, Capability Check, Command of Action, Expression of Desire, Identi fies Error, Request for Action, Requests Elaboration, Requests Explanation Meta / UI Chat Context Required, Confirmation, Copy, Copy (Extraneous), Erroneous Input, Includes Selection, Includes Extraneous Selection, Missing Selection, Paste, Paste (Extraneous), Pasted Code in Chat, Spelling Error, Start Over, Try Again Assistant Appears Fixated, Claims Ignorance, Didnt Understand, Grants Request (Complete), Grants Request (Incomplete), Offers Help, Provided Wrong Answer, Requests Details, Requests Paraphrase, Response Includes Code, Spews Garbage Table 1: Event log codebook. Our codebook contained 46 unique codes, applied separately to participant utterances (Human) and assistant responses (Assistant). Codes in bold were applied to both participant and assistant responses. Human codes were classified as demonstrating either a social or task orientation to the assistant. codes to identify metainformation such as utterances that included code, utterances that referenced selected code, utterances that im plicitly or explicitly referenced earlier portions of the conversation, or nonverbal UI activities such as copies, pastes, and invocations of try again and start over. Finally, we classified a subset of the humanapplied codes based on whether they represented a par ticipants task or social orientation toward the assistant. We list our codes in Table 1, but note that not all of them ended up being relevant to our analysis. When coding conversational data, we applied individual codes at the level of each conversational utterance. We allowed multiple codes to be applied to each utterance to account for utterances that performed multiple functions (e.g. greeting and selfidentification). In order to ensure consistency in how our codebook was applied, two authors coded a 10% sample of the 968 conversational ex changes, achieving a satisfactory level of interrater reliability (Krip pendorfs \ud835\udefc = 0.77, where agreement was conservatively defined as having all of the same codes applied to both utterances in a conversational exchange). 5.2 Expectations and Experience Pilot testing of the Programmers Assistant suggested that software engineers would be skeptical of a conversational programming assistant and its ability to provide useful assistance. Our study revealed that, for most participants, their actual experience after using the tool was better than they had anticipated. Participants were surprised at the quality of the assistants responses and they appreciated how its integration with the code editor reduced the amount of context switching they needed to do in the UI. Some participants struggled with the code selection feature, although others appreciated the ability to ask questions related to selected code. 5.2.1 Usage. All of our participants engaged with the Program mers Assistant while working on the challenges, despite there being no requirement to do so. Fortyone participants submitted solutions to all four challenges, and one participant, P14, only sub mitted solutions for one of the four challenges. Participants spent an average of 68 minutes engaged with the assistant, as measured by the amount of time the Programmers Assistant window was in focus. Participants made an average of 23.0 utterances (SD = 15.1 ut terances) to the assistant. On average, 6.2 of their utterances (SD = 4.3 utterances) contained a code selection. The average latency per request14 was 6.7 seconds (SD = 3.1 seconds). We saw a 66.3% rate of acceptance of generated code, where we considered code to be accepted if the participant performed a copy immediately after the code was generated. This acceptance rate is much higher than the 27% acceptance rate reported for Copilot [109]. We believe one reason we observed a higher acceptance rate is because Copilots completion suggestions are generated proactively, whereas the Programmers Assistants suggestions are generated upon request. When copying generated code from the assistant, participants most often copied the entirety of the generated code, and only in 5.8% of cases did they copy a smaller portion of it. 5.2.2 User Experience Expectations & Changed Attitudes. Prior to running our study, we had reason to believe that participants would be skeptical of a conversational programming assistant. Before de veloping the Programmers Assistant, we showed potential users mockups of a program editor with an integrated chatbot feature. These prototypes elicited uniformly negative reactions. People told us about their frustrating experiences with conventional chatbots and raised doubts about the knowledge, capabilities, and value of a conversational programming assistant. This skepticism moti vated us to develop the Programmers Assistant in order to evaluate whether the conversational experience, as powered by a stateof theart codefluent LLM, would be better than people had antici pated. During pilot testing, we received feedback that the Program mers Assistant provided a much better conversational experience compared to testers previous experiences with chatbots. Thus, in designing our study, we felt it important to first gauge participants expectations of a conversational interaction around code, and then measure their experience after the fact. 14This time includes additional time added by our proxy server to ensure our confor mance to the API rate limitation. 499 IUI 23, March 2731, 2023, Sydney, NSW, Australia Ross et al. We developed a short inventory of six scale items to measure user experience of code work15. The scale was administered twice: once before participants were exposed to the Programmers Assistant (but after they had been briefed that they would interact with an AI chatbot), and once after completing the programming challenges. The items were presented with the appropriate tense: Do you expect (Did you find that) the Programmers Assistant: (a) will be (was) easy to use; (b) will understand (understood) your requests; (c) will provide (provided) high quality responses; (d) will help (helped) you to write better code; (e) will help (helped) you to write code more quickly; (f) will be (was) enjoyable to use. Each item was rated on a 4point scale of extent: Not at all (1), A little (2), Somewhat (3), A great deal (4). A factor analysis revealed the items on this scale measured a single construct, which we identify as user experience (Cronbachs \ud835\udefc = 0.87). Thus, we computed two scores of user experience (UX) for each participant: a pretask UX score computed as the average of their six pretask expectation scale responses, and a posttask UX score computed as the average of their six posttask experience scale responses. We found that participants had lower initial expectations for their experience with a conversational programming assistant (pre task UX M (SD) = 3.0 (0.62) of 4) than their experience actually was (posttask UX M (SD) = 3.6 (0.32) of 4). A paired sample ttest shows that this difference was significant, \ud835\udc61(41) = 5.94, \ud835\udc5d .001, Cohens \ud835\udc51 = 0.92 (large). Measured another way, 32 participants (76.2%) had posttask UX ratings that were higher than their pre task expectations, demonstrating a significant shift in attitudes toward conversational programming assistance. However, the UX ratings alone fail to capture participants nu anced expectations of the assistant and the reasons for their shifted attitudes after using it. Participants expressed a variety of expecta tions of the assistant before using it, including that it would be easy to use (P30) and produce correct responses (P30), understand the problem and what is being asked of it (P8, P9, P11), not interfere with their flow state (P5), produce imperfect or questionable out puts (P6, P21), improve with feedback (P31), provide generic and unhelpful answers (P17) or only answer basic questions (P40), and produce responses quickly (P40). P17 expected to be frustrated very quickly and that what Id think would be relatively common questions would be responded to with generic, unhelpful answers. P6 explained, I didnt have very good experiences with chatbots. I think Ill need to spend more time in reviewing and fixing the suggestions than in writing the code myself from scratch. P11 had a more balanced view, that Itll do some tasks really well, but others will not be as reliable. After interacting with the Programmers Assistant, many par ticipants commented on how the experience was better than they anticipated, because it seemed to be able to handle complex issues (P10) and was a great help (P8). P20 felt it was incredible! P6 and P17, who were both initially skeptical, reported having a positive experience. For P6, It absolutely exceeded all my expectations, in 15Our scale items were modeled from scales published in Weisz et al. [103, Table 9 AI Support] that measured constructs including ease of use (item 3), response quality (item 1), the production of higherquality code (item 5), and the ability to write code more rapidly (item 4). We added additional items to cover the constructs of request understanding and enjoyment, and we cast all items on a 4point scale of extent. all aspects that I could have imagined and more! P17 provided a more quantitative assessment: Initial expectations: 3 Actual: 9.5. P38 was emphatic in their evaluation: I was blown away how well it allowing me to structure how I want the code to look and work and just giving me the thing I asked for. Many participants described a sense of surprise in their experi ences. P9 was surprised by how well it understood their requests: I was surprised at how well the Programmer Assistant was able to understand my requests and generate good code/documentation/tests. It understood major concepts and was able to explain it to me in a clear way, and it was also able to understand and write functional code. It even was able to help me review my answer. I was also surprised at how well it could understand the context of what I was asking in followup questions when I did not specify exactly what I was talking about, but rather referencing our prior conversation (such as, what does that mean). (P9) Similarly, P6 was surprised that they liked the conversational interaction when they expected that they wouldnt: I though[t] I wouldnt like the chatbot interaction and that I would prefer something like the tool Ive seen in those demos [of Copilot]. But surprisingly, after using the chatbot (and seeing the results: easy to use, it un derstands well, I felt it like a partner) I like this kind of help. (P6) 5.2.3 Quality of Assistants Responses. In order to gauge the quality of responses produced by the Programmers Assistant, we examined the 910 taskoriented requests made by participants in the study. For the vast majority (80.2%), the assistant produced a correct re sponse (Grants Request (Complete)); in other cases, the assistants response was incorrect (9.6%; Provided Wrong Answer), correct but incomplete (4.4%; Grants Request (Incomplete)), or the assistant didnt understand (3.4%; Didnt Understand), claimed ignorance of the subject (1.5%; Claims Ignorance), or produced another type of response (0.9%; Appears Fixated, Spews Garbage). Participants also reported experiencing this variability in the quality of the assistants responses. Some participants described how the assistant provided detailed answers (P17) and high qual ity outputs (P18) that were surprisingly good (P2). P6 felt it was incredible to see the quality of the responses, and P3 even explored the assistants capabilities outside the scope of the challenges and found that it could handle those as well: It was surprising the quality of the code and the ability to answer all my questions correctly. Although I think the challenges may be biased towards what the Assistant is able to do, it was a great experience because I asked many other things and it was able to answer correctly. (P3) Of course, the Programmers Assistant wasnt perfect, and some participants did run into issues. For P35, The documentation gener ation did not perform very well. P16 questioned the accuracy of the knowledge encoded in the model: Does the model need to be up dated? It said latest python version is 3.7 but google says its 3.10. In some instances, participants needed to ask their question multiple 500 The Programmers Assistant IUI 23, March 2731, 2023, Sydney, NSW, Australia times to get a good response: you need to ask many times if you want to get an answer and also a detailed explanation (P3). P27 felt, it was annoying when I asked it to try again and it would give me the same response. P22 struggled because, It didnt seem to handle multiple sentences well. P28 perhaps offered the most scathing criticism, that, It makes mistakes often enough to be not very practical. However, despite the production of poorerquality responses, other participants felt that the assistant was still helpful. P36 reported that, Only minor tweaks were normally needed to correct any issues. Similarly, P38 described how the assistant wasnt able to completely solve their problem, but provided a useful start: There was only one hickup I noticed where when I asked it to memoize fibonacci it couldnt, but it dropped the building blocks on my lap for me to finish so that was fine, that was like minutes of effort on my part. (P38) 5.2.4 UI Design & Affordances. Participants made many comments on our specific UI design and the affordances provided (or not provided) in our chataugmented editor. Overall, the integration between the chat pane and the code editor was very good (P23), with a nice interface between the code pane and the assistant pane (P17) that makes it really convenient (P35). Prior research by Brandt et al. [15] has shown how keeping developers focused in their IDE improves productivity, and our participants expressed similar sentiments. P40 remarked, It allows me to stay in one browser window/tab! and P12 hinted at how the interface might preserve their flow state by prevent[ing] me from getting distracted when looking into an issue in another tab. Some aspects of our user interface were confusing to participants, such as the mechanism for selecting code to be included in the conversational context. P7 remarked, Its was a little confusing doing the selection part for it to tell me what a function does, but... it gave me code that was insanely easy to copy and paste. Other participants appreciated the code selection mechanism, such as P11: I enjoyed the code selection feature, and found that very easy to use. In the event logs, we identified 20 instances in which a participant unintentionally included selected code in the conversation when it wasnt needed (Includes Extraneous Selection), 12 instances in which a code selection was omitted when it was needed to provide context for the question (Missing Selection), and 16 instances in which a participant copy/pasted code directly into the chat rather than selecting it in the editor (Pasted Code in Chat). Although these cases represent a small fraction of the 227 instances in which a code selection was required and included in the conversation (Includes Selection), their presence does indicate that more attention is needed to the interaction design of code selection. Another issue regarded the awareness of the try again and start over features. The try again feature was only used by 14 participants, who used it a total of 63 times over the course of the study. Some participants used it specifically when they got an answer which they saw as clearly wrong, while others used it to get a variety of possible answers before proceeding. The start over feature was used even less, by 5 participants who used it a total of 6 times. Despite our effort to surface these conversational features in the UI via shortcut buttons, they may not have been sufficiently noticeable or salient: The try again button is not so reachable, often times I forgot it exists (P23). By contrast, at least one participant was successful with these features: at some point it had issue with challenge 3 and I had to start over. Just asking try again was not enough and I was getting always the same (wrong and not related) answer. starting again solved the issue! (P20) 5.3 Utility of Conversational Assistance Our next set of themes concerns the utility provided by conversa tional programming assistance. Participants felt the assistant was highly valuable and desired to use it in their own work. They felt it would be most helpful for smaller or narrowlyscoped tasks, but able to provide a wide variety of types of assistance. The fact that the interaction model was conversational and grounded in code were valuable aspects, as was the ability for the assistant to bolster users learning about programming topics through that interac tion. Participants did question whether they could trust and rely upon the assistants responses, echoing a similar theme discussed in Weisz et al. [102]. 5.3.1 Value & Appropriate Tasks. Participants rated the value of the Programmers Assistant highly (M (SD) = 8.6 (1.4) of 10). Many participants asked questions such as, Can I have it in my editor please? (P15), or made comments that, I would enjoy using it in the future (P36), I would love to be able to... have access to it for my coding (P37), and Id love to use this tool as part of my usual programming workflow if I could! (P39). Some of the reasons why participants found it valuable are because it help[s] me remember how to do things in certain languages that normally I would just Google (P9) and It helps me to avoid silly syntax errors and can when I cannot remember exact function/method names and required arguments (P40). We did not observe any differences in value ratings based on participants familiarity with or recency of using Python. Participants described a wide variety of tasks for which they felt the assistant would be useful. These tasks included ordinary (P23), simpler (P2), and small, repetitive (P4) tasks such as quick lookups (P25) for short chunks of code (P11) or for narrowed questions (P26). Participants also felt the assistant was useful for small containable novel algorithms (P38) and little coding problems (P4). Several kinds of task assistance were reported as being valuable, such as explaining code (P31), implementing business logic in a UI (P38), understanding what code does (P19, P37), and recalling language syntax, method names, and arguments (P12, P15, P20, P40, P42). P27 felt that the assistant was More helpful when recognizing a specific well known algorithm but not things you make yourself. Participants also made recommendations for how to increase the value of the Programmers Assistant. P38 suggested, What would blow me away though is if its able to help with what I do most often which is to integrate, refactor and iterate on an existing system. P16, P26, and P38 all desired more information on the data sources used to produce the assistants responses. P9 requested to Have the Programmers Assistant examine your code and make proactive suggestions for improving it in the chat. P36 requested the same, 501 IUI 23, March 2731, 2023, Sydney, NSW, Australia Ross et al. but cautioned that, Care would need to be taken to avoid becoming an annoyance or disrupting the flow of a coding session. In the posttask survey, we probed participants on how certain changes to the Programmers Assistant would either decrease, in crease, or result in no change to its value. Over 75% of participants felt that the assistant would be more valuable if it operated in a proactive manner, either by making improvement suggestions in the chat or as comments directly in the code. Similarly, 78.6% of participants felt that having more buttons in the UI for common features such as explaining or documenting code would make the tool more valuable. 5.3.2 Conversational Interactions Grounded in Code. One of the challenges in interpreting participants comments about the utility of the Programmers Assistant was in disentangling the extent to which value was derived from the quality of the underlying model versus the integration of conversation in a code context. Indeed, participants felt that the chat interaction was valuable: 69.0% of participants felt that eliminating the conversational interaction and making the assistant behave more like web search would de crease its value. Further, our analysis of the conversation transcripts revealed that 42% of the 910 taskoriented utterances from partic ipants required historical conversational context (Chat Context Required) in order to be correctly interpreted. Thus, we observe that behaviorally, participants did rely on conversational context in their interactions. In the posttask survey, 83% of participants rated the importance of the ability to ask followup questions as being somewhat or a great deal. Several participants specifically commented on the value of this conversational context. P39 remarked, I absolutely loved how you can straight up ask followup questions to the Programmers Assistant without having to reiterate the original topic/question. P15 expressed a similar sentiment, saying, I think the conversational context was someone helpful, just in communicating that its a running conversation where my context is remembered. P9 provided a similar analysis: This tool was so helpful at answering questions I had about the code in the context of the code I am working on... I was also impressed with how well it was able to remember the context of our conversation, especially when I asked vague followup questions. (P9) In addition, some participants identified how a conversational interaction grounded in code was useful, because I think to un derstand the dev context could be VERY important (P31). In fact, 24.9% of taskoriented utterances included a relevant code selection (Includes Selection), showing that participants valued this ability. Contrasting with these participants, P18 felt that interacting with the assistant conversationally was tedious, and they employed a more direct approach: I really like the PA. But, I didnt converse with it like a chat bot. I often told it what to do (Document this code.) as opposed to asking it what to do (How do I document this code?). Talking to it the way that was suggested in the tutorial seemed overly verbose/tedious. (P18) Despite these individual differences in interaction preferences, P39 envisioned that both interaction styles could be supported in the tool: I think both options should exist: people should be able to input their queries like a search bar AND also give their question as if in conversation. (P39) 5.3.3 Learning Effects. One specific benefit of the Programmers Assistant identified by participants is its ability to help people improve their programming skills and reinforce knowledge gaps. For example, it can help users remember how to do things in certain languages... such as, when I am using a language I havent used in a while (P9). The assistant can also serve as an memory aid, such as when I use a lot of libraries that I dont always remember all of the functions (P15). Similarly, P31 said, No matter how good youre as a developer, you cant (humanly) remember all the API of hundreds of libs or new languages... Id learn new dev lang and new lib/frameworks faster. P39 felt the assistant is perfect for programmers of all levels, and P1 felt it could help them rapidly improve their Python skills: I have wanted to learn python... The main concern how much time spent learning is needed before I could actually get some value out of learning python. I have a feeling this would cut that time down from weeks to a day or so. (P1) P39 also identified the fact that, because the interactions with the assistant are conversational, it forces people to learn how to communicate to others about their code: The conversation aspect promotes proper communi cation, which would really stand to benefit budding programmers if they want to learn how to explain con cepts more fluently in the future to their colleagues. (P39) Conversely, P36 suggested that overreliance on programming assistance might have a detrimental effect to ones learning: Its definitely a huge time saver, but overreliance on it may cause new developers to skip learning the reference material themselves and discovering new things and sparking new ideas. (P36) 5.3.4 Trust. Many participants raised questions about whether they could trust the responses provided by the Programmers Assis tant. P21 asked this question most directly: will the code be correct, safe, efficient? Other participants raised similar questions, such as, Im wondering how it validates its answers, if it can be trusted to always give a working answer (P10), and Sometimes lack of source and context may raise doubts in the mind of the programmer (P16). These issues of trust were exacerbated by the fact that the Pro grammers Assistant did not allow participants to actually run their code. Because of this limitation, participants had to rely on their own knowledge to judge the correctness of the assistants responses. P19 asserted, There is no way to evaluate if the Programmers as sistant is giving you the right advise or not other than your own knowledge, and P9 concurred: I had to trust that it was correct (and use my own prior knowledge). 502 The Programmers Assistant IUI 23, March 2731, 2023, Sydney, NSW, Australia P18 described the potential consequences of allowing the assis tant to write code for them: The only thing that made me nervous was that it could have introduced a bug that wasnt immediately appar ent. And given I didnt write the code, I could have easily glossed over a mistake when reviewing it. Especially if it is also the one writing the test cases. (P18) Despite our efforts to make the Programmers Assistant respond in nonauthoritative ways, we did observe participants sometimes uncritically accept generated results that were clearly wrong or incomplete. Thus, we did find behavioral evidence for overreliance. Listing 2: Building trust through explanations and justifica tions 1 P9: what is a unit test? 2 3 Socrates: A unit test is a test of a single function 4 or method. It is typically written by the 5 programmer who wrote the code. 6 7 P9: 8 def euclid(a, b): 9 while b != 0: 10 (a, b) = (b, a % b) 11 return a 12 13 how would I write a unit test for this code? 14 15 [...] 16 17 Socrates: I think you would do something like this: 18 19 def test_euclid(): 20 assert euclid(2, 4) == 2 21 assert euclid(6, 9) == 3 22 assert euclid(17, 12) == 1 23 assert euclid(4, 4) == 4 24 25 P9: what is a euclid function 26 27 Socrates: The euclid function is a function that 28 computes the greatest common divisor of two 29 numbers. 30 31 P9: is this a good unit test for a euclid 32 function 33 34 Socrates: I believe that it is a good unit test for 35 a euclid function. 36 37 P9: why 38 39 Socrates: It seems to me that it tests the edge 40 cases of a and b being equal , a being 41 greater than b, and b being greater than a. 42 43 [...] 44 45 P9: thanks Socrates this was helpful 46 47 Socrates: Always happy to help. One way to address trust issues is for the assistant to provide further explanations and justifications that can calibrate a users confidence in the assistants responses. Such explanations could be requested conversationally, though most participants did not attempt to do so. One participant (P9) did ask for such explanations, and we show a summary of their transcript in Listing",
        "2. In this": "instance, P9 asked for a definition of a unit test (line 1), an expla nation of the code being tested (line 25), and justifications of the quality of the unit test (lines 31& 37). Thus, we observe that the assistant is capable of producing explanations and justifications when asked. 5.4 Patterns of Interaction and Mental Models Participants interacted with the assistant in a variety of ways with two main patterns of usage standing out: (1) invoking the assis tant to solve the entire programming challenge, and (2) breaking the challenge down into a set of smaller tasks and invoking the assistants help for each. There were no clear differences in how participants with differing Python experience approached the tasks. Participants mental models of the assistant also varied. Although participants strongly saw the role of the assistant as being a tool, their behaviors revealed that in many cases, they actually treated it as a social agent. In addition, participants ascribed various mental capacities to the assistant, such as having the ability to understand, compute, and learn. Participants felt the assistant changed the nature of their work process. For some participants, it enabled them to focus on the higherlevel aspects of development because the assistant handled lowerlevel details or provided partial solutions for them to build upon. Many participants felt the assistant sped up their work and helped them remain focused on their tasks. Finally, participants drew comparisons between the Program mers Assistant with other forms of programming support such as Copilot and web search. They felt that the conversational style of interaction enabled them to discover new, emergent behaviors from the model that were unavailable from Copilots focus on code autocompletion. They also felt that the examples provided by the assistant were more readily usable within their own code compared to browsing for answers within search results, speeding up the coding process. However, some participants advocated for a bal anced approach to the design of programming assistance tools by incorporating multiple modes of interaction rather than fixating on a single one. 5.4.1 Interaction Styles and Assistant Role. We observed that par ticipants interacted with the Programmers Assistant in strikingly different ways. Some participants would present the entire chal lenge description to the assistant and then work with the results it produced. Other participants approached the programming chal lenges in a piecemeal fashion, breaking them apart into a set of smaller tasks, then invoking the assistant to aid with each one. Experience with Python was not a determinant of how partici pants approached the programming challenges, but it did seem to impact how participants interacted with the assistant. Less experi enced participants tended to ask the assistant basic questions such as, What is a unit test (P29, not familiar with Python) and how do I document a function? (P27, 1 year of experience). More expe rienced participants made detailed requests about specific Python libraries or algorithms, such as, given a pandas dataframe with two columns Date and Sales please use matplotlib to draw me a scatterplot (P38, 3+ years of experience) and implement a runge kutta algorithm for solving an ODE with adaptive time steps (P37, 3+ years of experience). Another difference we observed in how people interacted with the assistant stemmed from their view on the role it played in their 503 IUI 23, March 2731, 2023, Sydney, NSW, Australia Ross et al. collaborative process. Some participants, such as P18, treated it more as a tool by issuing commands rather than asking questions. As quoted earlier, they said, I didnt converse with it like a chat bot. P5 described their interaction style similarly: I found myself wanting to type search queries into Socrates, not treating it as a person but as a search tool. In anticipation that participants would have different orienta tions to the assistant and its role, we asked a question on the post task survey about the different kinds of roles the assistant might take. These roles generally fell into one of two categories: a tool orientation (a tool, a reference guide, a content generator, a problem solver), and a social orientation (a collaborator, a colleague, a coach, an advisor, a reviewer). Participants rated the extent to which they viewed the Programmers Assistant in each of these roles on a 4 point scale of extent: Not at all (1), A little (2), Somewhat (3), or A great deal (4). 0 25 50 75 100 Tool Content generator Reference guide Collaborator Problem solver Advisor Coach Reviewer Colleague Role Percentage of participants Rating Not at all A little Somewhat A great deal Figure 3: Role orientations. Participants overwhelmingly felt that the assistants role was of a tool orientation rather than a social orientation. The chart shows ratings distributions across 9 different roles the assistant might take, sorted by the percentage of participants who rated the extent of that role as A great deal. The leftmost role is of a tool, with 80.9% of participants rating it as a great deal. Following tool are content generator (69.0%), reference guide (64.3%), collaborator (40.5%), problem solver (35.7%), advisor (21.4%), coach (19.0%), reviewer (16.7%), and colleague (14.3%). We show participants ratings of the assistants role in Figure",
        "3. Despite the fact that their attitudes toward the assistant overwhelm": "ingly reflected a tool orientation, their behaviors reveal that many participants actually treated the assistant as a social agent. P6 de scribed how I felt it like a partner, and P4 told the assistant, I could not have solved [the challenge] without your help, to which the assistant responded, Im glad I could help. The literature on Computers as Social Agents (CASA) helps us interpret this result as it demonstrates how computers are often treated like people [56, 67]. LLMbased conversational agents can exacerbate this tendency; as they likely have been trained on ex amples of social interaction, they can also respond as social agents. In the conversation logs, we identified participants who inter acted with the assistant in a sociallyoriented fashion (the social orientation codes in Table 1). Twenty participants (47.6%) made at least one sociallyoriented utterance. An extreme form of this inter action style can be seen in a snippet from P6s transcript (Listing 3). The 20 participants with a behaviorallydemonstrated social orientation did not generally differ in their role ratings from other participants, except that they rated the assistant as more likely to be an advisor (Fishers exact test, twotailed \ud835\udc5d = .02) or a reviewer (Fishers exact test, twotailed \ud835\udc5d = .03). However, they did not differ in their overwhelminglystrong ratings of the tool orientations. Thus, at least for some participants, there seems to be a dissonance in their view of the assistants role orientation. Listing 3: Excerpt from P6s interaction with the Program mers Assistant, in which P6 offers their thanks and congrat ulations. 1 P6: thank you! it's been really nice to code 2 with you! or to have you coding! LOL 3 Socrates: You 're welcome! 4 P6: I hope to see you on air soon , Socrates! 5 Socrates: I hope so too. 6 P6: congrats to you research team! you 've made 7 a good job! 8 Socrates: I'm glad you think so. 9 P6: sorry ... they 've made a good job (well ... 10 you too ... ;) ) 11 Socrates: Thank you. 12 P6: see you next time! bye!! 13 Socrates: Goodbye. 5.4.2 Mental Capacities. Participants made a number of inferences about the Programmers Assistant and its capacities for thought. Many participants talked about how the assistant possessed a level of understanding (P6, P8, P11, P18, P32) of the context (P9, P21) as well as major concepts (P9) and knowledge (P33). P24 was amazed by the assistants ability to take a plain english request and interpret it properly. P7 ascribed intelligence to the assistant, saying, It was a lot smarter and trained tha[n] I thought it was. One participant assumed that the assistant Keeps improving through (user) feedback (P31). Another felt that the assistant was capable of computation: It understands the problem... It can calculate the results of a function back (P8). However, not all participants were convinced of the assistants ability to understand. P37 questioned the assistants limitations: I wonder how far beyond boilerplate it can go and if it works for truly original problems. 5.4.3 Impact of Conversational Assistance on Work Practices. Many participants discussed how the Programmers Assistant shaped their work practices on the programming challenges. Overall, par ticipants felt that the assistant saves time (P10), helps me code faster (P34), and would speed up my productivity (P19) because I could focus on validating and improving the code it generated instead of having to write it all from scratch (P18). P37 remarked that, It opens a whole new door for fast develpment. P4 discussed how the assistant was helpful in staying focused on the code, although for P14, it took [me] time to get into tempo with the tool. P31 pointed out how the assistant would change the nature of their work: My job could focus more on higher level aspects and therefore achieving better (quality) results, besides the 504 The Programmers Assistant IUI 23, March 2731, 2023, Sydney, NSW, Australia timetovalue... Data science (and dev) becomes a more creativehigher level experience. (P31) Other participants discussed a work process in which the assis tant provided incomplete solutions the building blocks (P38) or initial draft of code (P11) upon which they could build. P5 aptly described this process: Its nice to copy well formulated challenges in natural language and have the code generator take its best stab at it, then edit to our hearts content. (P5) Participants felt that human review of the assistants responses was necessary because The answers provided are generally not novel solutions, often look clunky and nonelegant. There may be some unnecessary code. Basically the code would need to be reviewed (P16). P35 also pointed out how The code generator was good but you still have to really check it. P19 discussed how they would turn to the assistant as a first source for support, and only if it wasnt able to help would they then turn to other support tools: The way I will use it is, I will first us[e] the Program mers assistant for most of my cases. Only in certain cases where Programmers assistant cant answer things I will turn up to official documentation or stack overflow. (P19) However, latency was a factor for interactive use of the assistant and participants noticed when the assistant took a long time to respond. P19 remarked, Sometimes it took lot of time, like more than 5 seconds. P40 also felt the response [was] a little slow sometimes... in chat mode I expect faster responses. As discussed in Section 5.2.1, the assistant took an average of 6.7 seconds (SD = 3.1 seconds) to respond to a request, and participants did appreciate when the assistant produced rapid responses: I loved how quick it was able to pull up answers to questions I had (P38). 5.4.4 Conversational Interaction vs. Other Interaction Models. Al though our study was not intended to make comparative evalu ations with the Copilot tool, we nonetheless asked participants whether they were familiar with Copilot, and if so, to comment on how the two tools compared. We also asked a similar question to compare the assistant with another popular form of programming assistance, searching the web (via a search engine like Google, or a Q&A site like Stack Overflow). In discussing the differences be tween these three tools, we note that the primary differentiator is their interaction model. The interaction model for the Programmers Assistant is clearly conversational: users ask questions in natural language and are provided with a response in natural language and/or code. The interaction model of Copilot is reminiscent of direct manipulation interfaces [37], in which the users actions in the user interface directly manipulate an object on the screen. Copilot automatically makes autocompletion suggestions as the user types. This auto completed code is directly placed in the source editor; thus, the users work is contained entirely within the scope of the object on which they are working (i.e. the source code), which is how direct manipulation interfaces operate. In web search, users enter a separate search context (e.g. a search engine accessed within a web browser), type in a natural language query, and then forage amongst search results to identify relevant items of interest [12, 62]. When a desirable item is found, users must translate it into their code environment (e.g. via copy/paste) and possibly edit it to fit their existing code. We also note that the Programmers Assistant and Copilot both utilize the same underlying AI model, Codex [24], which means that the only difference between these tools is the user experience. The extent to which Codex was trained on data from programming related Q&A web sites is less clear, but for the purposes of our analysis, we focus our discussion solely on the differences in their interaction models16. Participants reported various benefits and drawbacks of a con versational interaction over a direct manipulation interaction. Fore most, conversation felt very natural (P21) and feels much more natural using Natural Language with the AI (P39). In addition, P39 felt that the use cases of Programmers Assistant seem more open ended. Many participants were surprised at the variety of tasks the assistant was capable of performing, from writing unit tests (P19, P36, P37) and documentation (P12, P19, P36, P37) to explaining what code did (P31, P38) and even answering generalknowledge questions (P31). Again, we note that the Programmers Assistant utilizes the same underlying model as Copilot, yet the conversa tional interface was able to expose a wider variety of emergent behaviors from the model. Multiple participants explored the limits of the assistants knowledge and abilities beyond our programming challenges. For example, P37 asked it questions about physics and ordinary differential equations (ODe as written by P37), and was surprised by the versatility of what it could answer. I asked it some physics and ODe question and the answers, though not complete, included the key parts needed to write that code. (P37) P31 probed the assistant on its knowledge of geography and was surprised when the assistant produced a correct answer. I asked something out of SW engineering domain (ge ography) and it replied correctly, also by correctly an swering on my nationality. (P31) For some participants, the ability to assess the assistants re sponse before committing to it (i.e. by inserting assistantgenerated code into their editor) was a boon. P15 described how the copy/ paste boundary provided them with a bit more control to ask specific questions about what I wanted and to assess before putting it in my code. Other participants felt that the copy/paste boundary was more inefficient: I think the main difference is the ability of Copilot to suggest code while you type, what make it faster and easier to use. While using the Programmers Assistant, you need to go to the chat, ask the question, copy the 16As an aside, our comparison of direct manipulation, search, and conversational inter action models is reminiscent of historical comparisons of textbased and graphical user interfaces [54, 63]. Each modality was shown to have advantages and disadvantages. For example, textonly interfaces can provide accessibility [54, 78] and productivity [53] advantages, whereas graphical user interfaces provide greater discoverability [10, 88]. Some researchers explored ways to bridge the two interaction modes, such as by developing GUI wrappers for command line programs [53, 94] or by developing tools that converted GUI activities into procedural descriptions [55]. Our view is that similar bridges can be constructed between direct manipulation, search, and conversational models of interaction; a users interaction with an LLM need not be constrained to a single interaction model. 505 IUI 23, March 2731, 2023, Sydney, NSW, Australia Ross et al. code (or rephrase the question if it was not understood by the agent), and edit it to match your code. (P3) A large number of participants felt that the conversational inter action was faster than web search (P1, P6, P7, P10, P11, P12, P16, P17, P18, P20, P24, P29, P30, P33, P36, P37, P42) because of its ability to provide realtime responses (P32) that can be applied exactly to your code (P33) without having to parse through lots of text... to get what you need (P15). In addition, the assistant provided MUCH faster, better responses (P17) that were much more relevant to the problems (P34) and simple [and] succinct (P9), without having to sort through answers on your own or read documentation (P9) or look at many posts before finding the relevant one (P18). Despite these benefits, some participants felt that the assistant might not work well for more specific and difficult problems on a bigger scale as compared to web search. P9 felt that the data [of the Programmers Assistant] wasnt as rich as the web. Other participants felt that the assistant lacked the multiple answers (P9) and rich social commentary (P19) that accompanies answers on Q&A sites: I like to see the different versions proposed on stack overflow and the commentary of what makes one solu tion better than another in a given situation. (P27) Some participants promoted a more balanced view that there isnt a single mode of interaction superior to all others. P19 felt that web search would be a fallback when the assistant failed to answer a question. P39 described how search could be integrated with the conversational interaction: I think both options should exist: people should be able to input their queries like a search bar AND also give their question as if in conversation. (P39) 6 DISCUSSION 6.1 Value of Conversational Interaction We began our research by asking the question of whether con temporary developments in codefluent LLMs could sufficiently support a conversational programming assistant. We believe that our work has demonstrated that they can. Clearly, the Program mers Assistant was viewed by our participants as a useful tool that provided real value so much so that many participants explic itly requested or expressed the desire to use it in their own work. However, how much of this value was derived from the model itself and its ability to produce highquality responses to programming questions, versus from participants ability to conduct extended conversational interactions grounded in their actual source code? We believe that both of these constituent aspects were valuable. Indeed, many participants commented on their surprise and satis faction with the quality of the assistants responses (Section 5.2.3). However, participants also valued the conversational interactions that they had with the assistant. In the event logs, we saw evi dence that participants were leveraging conversational context to ask followup questions as well as leveraging code context by ask ing about their code selections (Section 5.3.2). Many participants reported that they would find the tool less valuable if the conver sational interaction were removed (Section 5.3.2). Further, conver sation seemed to provide unique value beyond other interaction models (direct manipulation and search) because of its embedded ness in the UI and its ability to surface emergent behaviors of the model (Section 5.4.4). We do not believe that these different interaction models are in competition and we agree with P39s assessment that assistive tools can be built using a plethora of different interaction models. For use cases in which a model is known to produce highquality results (e.g. code autocompletion for Codex), a direct manipulation interface seems wholly appropriate as it would provide a discov erable and predictable way of invoking the model to produce a known type of result. However, direct manipulation interfaces may be less ideal for surfacing the emergent behaviors of a foundation model [14], and thus natural language interaction may be more suit able. Many popular texttoimage models, such as DALLE 2 [66] and Stable Diffusion [72], operate in a oneshot fashion, in which the user specifies a prompt, clicks a button, and gets results. Our study demonstrates how the additional contextual layers of con versational history and the artifactunderdevelopment provide additional value to the cocreative process. 6.2 Toward HumanAI Synergy The aim of humancentered AI is to enable[] people to see, think, create, and act in extraordinary ways, by combining potent user experiences with embedded AI methods to support services that users want [82]. Building upon this definition, Rezwana and Maher [69] posit that, In a creative collaboration, interaction dynamics, such as turntaking, contribution type, and communication, are the driving forces of the cocreative process. Therefore the interaction model is a critical and essential component for effective cocreative systems. [69]. They go on to note that, There is relatively little research about interaction design in the cocreativity field, which is reflected in a lack of focus on interaction design in many existing cocreative systems. Our study begins to address this gap. While many cocreative systems examine casual tasks or experimental activities (e.g., Spoto and Oleynik [87]), our focus was on the cocreative practice of programming. Our goal was to understand peoples attitudes to ward a conversational programming assistant, akin to Wang et al.s examination of data scientists attitudes toward automated data science technologies [99]. We found that, despite an initial level of skepticism, participants felt that a conversational assistant would provide value by improving their productivity (Section 5.4.3). How ever, further work is needed to assess the extent to which this type of assistance provides measurable productivity increases. Campero et al. [19] conducted a survey of papers published in 2021 that examined humanAI synergy, the notion that a humanAI team can accomplish more by working together than either party could accomplish working alone. They found mixed results, with no clear consensus emerging on how to design humancentered AI systems that can guarantee positive synergy. Summarizing from their discussion, Perhaps achieving substantial synergies among people and computers is harder than many people think. Per haps it requires... new ways of configuring groups that include people and computers. And perhaps it needs 506 The Programmers Assistant IUI 23, March 2731, 2023, Sydney, NSW, Australia more systematic, focused attention from researchers than it has, so far, received. [19, p.9] We believe such evaluations of humanAI synergy should go beyond oneshot performance measures. As implied by many of the uses cases listed by Seeber et al. [80], humancentered AI systems are often deployed in socioorganizational contexts that require longitudinal use [20, 41, 43], such as product design [93], game de sign [4], and engineering [20, Section 3.2.2]. Thus, we would expect that over time and through interaction with each other, humanAI teams would improve their performance through a mutual learning process. Evidence for this process surfaced in our study when participants described how they could improve their programming skills by interacting with the assistant (Section 5.3.3). We assert that the learning should operate in both directions: not only should people improve their programming skills, but the model itself can also improve based on peoples interactions with it. For example, when the assistant provides a code example to the user, and the user takes that example and edits it, those edits constitute feedback that can be used to further finetune the model. In addition, through longitudinal use, we believe that human and AI partners can create reciprocal representations of one another i.e., the human is likely to create a mental model of the AI, and the AI may be engineered to develop a user model for each of its human users [30, 48, 79]. Such a pair of models is often described as Mutual Theory of Mind [29, 100]. This type of capability raises the possibility of personalizing and adapting an assistant to the strengths and needs of individual users. With such models, an assistant that knows a user is learning a programming language could provide natural language explana tions alongside code outputs, whereas an assistant that knows a user is strongly skilled in a programming language might shorten or omit those explanations. Similarly, users are likely to update their mental models of the AI with more experience. We believe the space for exploring how these reciprocal models impact humanAI synergy is rich, and we encourage additional work in this area. Humancentered AI systems that are designed to combine and synergize the distinct skills of humans and AI models cannot suc ceed if they diminish the human skills upon which they depend. Welldesigned humancentered AI systems develop new and com plementary skills for both the human and AI constituents [82, 83], and we believe that mutual learning may address concerns that the wide deployment and use of AI systems will result in a deskilling of the workforce [77, 108]. Ultimately, the design decisions that go into an interactive AI system have ethical implications. Our design attempts to augment the users knowledge and skills by presenting help on demand, couched in nonauthoritative suggestions, which leaves the user firmly in control and ultimately responsible for the work product. 6.3 Opportunities for Future Research Our work highlights many interesting avenues for future enhance ments that could be made to LLMbased conversational assistants such as our Programmers Assistant, as well as future human centered research on LLMbased conversational assistance. Our work employed a codefluent model that was not specifi cally designed to handle conversational interaction. Finetuning the underlying LLM for conversational interaction, such as what has been done with Lamda [91], is one opportunity to improve the assistants performance. Another opportunity is to align the lan guage model to follow the desiderata proposed by Askell et al. [11] and described by Ouyang et al. as, helpful (they should help the user solve their task), honest (they shouldnt fabricate information or mislead the user), and harmless (they should not cause physical, psychological, or social harm to people or the environment) [61, p.2]. Glaese et al. [33] propose a slightly different desiderata of correct instead of honest, which may be more applicable to the software engineering domain, as the ability to produce correct code and correct answers about code are both important properties of a conversational programming assistant. Combining LLMs with searchbased approaches to establish ad ditional context for the model, such as AlphaCode [44] has done, may also result in more capable systems. These searches need not be limited to textual sources, but could be conducted over appro priate semantic stores (e.g. a knowledge graph) and take advantage of explicit semantic reasoning services, resulting in an integration of symbolic and neural approaches. Further, allowing for internal deliberation of the type shown in Nye et al. [59] could result in betterreasoned results, as well as better explanations and justifica tions. Another avenue for improvement involves the prompt used to configure the assistant (Appendix D). Just as the prompt for each successive interaction is modified by the growth of the conversa tional transcript, there is no requirement that the initial prompt be static. It too can be specialized to incorporate aspects of a user model, enabling the realization of a Mutual Theory of Mind [29, 100]. Pro viding better UX affordances for visualizing and manipulating the active contexts code and conversation could provide users with more control over which information contributes to the generation of the assistants response. Our participants clearly indicated that they were interested in having an assistant that behaved more proactively, in contrast to our deliberate design of an assistant that never takes conversational initiative. A more proactive assistant would be able to interrupt or remind a user when necessary [23], yet this characteristic raises many challenging issues. How can we calibrate the threshold for such interruptions? How can users tune the assistant to deliver only those interruptions that the they would find useful (e.g., [28, 81])? How can we help users to regain their prior context after dealing with an interruption (e.g. [89])? Should an assistant be used to persuade or nudge the user (e.g. [35])? Who should determine the topic, frequency, and insistence of such persuasion attempts (e.g. [52, 85])? Should users have the ability to moderate or defeat attempted persuasions, or should those decisions be left to the organization? Finally, we explored the different kinds of role orientations our participants had toward the assistant and found that participants varied in their views of it as a tool versus a social agent (e.g. collabo rator or colleague). We posit that peoples effectiveness in working with an AI system may be influenced by their role orientation, and we encourage future research in this area. 507 IUI 23, March 2731, 2023, Sydney, NSW, Australia Ross et al. 7 CONCLUSION We developed a prototype system, the Programmers Assistant, in order to assess the utility of a conversational assistant in a software engineering context. The assistant was implemented using a state oftheart codefluent large language model, Codex [24], and was capable of generating both code and natural language responses to user inquiries. We further used the prompting mechanism of the model to set up a conversational interaction in which the model uses the conversational history, plus the users current utterance, in order to generate a response. In this way, users are able to ask followup questions in the chat that reference prior utterances and responses. We incorporated the conversational assistant into a code editing environment, enabling the conversation to be grounded in the context of the users source code. We evaluated this system with 42 participants with varied levels of programming skill, and their quantitative and qualitative feed back, coupled with their usage of the system, demonstrated the varied, and sometimes emergent, types of assistance it was able to provide. Many participants noted the high quality of the conversa tional responses, including the assistants ability to produce code, explain code, answer general programming questions, and even answer general knowledge questions. Participants felt this type of assistance would aid their productivity, and they drew meaningful contrasts between the conversational style of interaction with other tools that employ a direct manipulation or searchbased interaction model. Our study motivates the use of conversational styles of inter action with large language models by showing how they enable emergent behaviors in a cocreative context. The Programmers Assistant did not always generate perfect code or correct answers; nonetheless, participants in our study had an overall positive ex perience working with it on a variety of programming challenges. We believe that our work takes us one step closer to realizing the vision of humancentered AI: learning how to design systems that maximize the synergy in humanAI collaborations. ACKNOWLEDGMENTS We would like to thank Socrates for his tireless assistance during the user study, as well as for suggesting the title of this paper based on its abstract."
    },
    "references": [
        "[1] Rabe Abdalkareem, Emad Shihab, and Juergen Rilling. 2017. What Do Developers Use the Crowd For? A Study Using Stack Overflow. IEEE Software 34, 2 (2017), 5360. https://doi.org/10.1109/MS.2017.31 ",
        "[2] Eleni Adamopoulou and Lefteris Moussiades. 2020. Chatbots: History, technol ogy, and applications. Machine Learning with Applications 2 (2020), 100006. ",
        "[3] Daniel Adiwardana, MinhThang Luong, David R. So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, and Quoc V. Le. 2020. Towards a Humanlike OpenDomain Chatbot. ",
        "[4] Safinah Ali, Nisha Elizabeth Devasia, and Cynthia Breazeal. 2022. Escape! Bot: Social Robots as Creative ProblemSolving Partners. In Creativity and Cognition. 275283. ",
        "[5] Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. 2018. A survey of machine learning for big code and naturalness. ACM Computing Surveys (CSUR) 51, 4 (2018), 137. ",
        "[6] Irene Alvarado, Idan Gazit, and Amelia Wattenberger. 2022. GitHub Next GitHub Copilot Labs. https://githubnext.com/projects/copilotlabs/ ",
        "[7] Hikari Ando, Rosanna Cousins, and Carolyn Young. 2014. Achieving saturation in thematic analysis: Development and refinement of a codebook. Comprehensive Psychology 3 (2014), 03CP. ",
        "[8] Craig Anslow, Stuart Marshall, James Noble, and Robert Biddle. 2013. Sourcevis: Collaborative software visualization for colocated environments. In 2013 First IEEE Working Conference on Software Visualization (VISSOFT). IEEE, 110. ",
        "[9] Zahra Ashktorab, Michael Desmond, Josh Andres, Michael Muller, Naren dra Nath Joshi, Michelle Brachman, Aabhas Sharma, Kristina Brimijoin, Qian Pan, Christine T Wolf, et al. 2021. AIAssisted Human Labeling: Batching for Efficiency without Overreliance. Proceedings of the ACM on HumanComputer Interaction 5, CSCW1 (2021), 127. ",
        "[10] Catherine A Ashworth. 1996. GUI Users have trouble using graphic conventions on novel tasks. In Conference Companion on Human Factors in Computing Systems. 7576. ",
        "[11] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. 2021. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861 (2021). ",
        "[12] Leif Azzopardi, Paul Thomas, and Nick Craswell. 2018. Measuring the utility of search engine result pages: an information foraging based measure. In The 41st International ACM SIGIR conference on research & development in information retrieval. 605614. ",
        "[13] Shraddha Barke, Michael B James, and Nadia Polikarpova. 2022. Grounded Copilot: How Programmers Interact with CodeGenerating Models. arXiv preprint arXiv:2206.15000 (2022). ",
        "[14] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2021. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 (2021). ",
        "[15] Joel Brandt, Mira Dontcheva, Marcos Weskamp, and Scott R Klemmer. 2010. Examplecentric programming: integrating web search into the development environment. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. 513522. ",
        "[16] Virginia Braun and Victoria Clarke. 2022. Common challenges in Thematic Analysis and how to avoid them. Retrieved August 11 2022 from https://youtu. be/tpWLsckpM78 ",
        "[17] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Ka plan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel HerbertVoss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are FewShot Learners. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 18771901. https://proceedings.neurips.cc/paper/2020/ file/1457c0d6bfcb4967418bfb8ac142f64aPaper.pdf ",
        "[18] Sallyann Bryant, Pablo Romero, and Benedict\" du Boulay. 2006. The Collabora tive Nature of Pair Programming. In Extreme Programming and Agile Processes in Software Engineering, Pekka Abrahamsson, Michele Marchesi, and Giancarlo Succi (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 5364. ",
        "[19] Andres Campero, Michelle Vaccaro, Jaeyoon Song, Haoran Wen, Abdullah Almaatouq, and Thomas W Malone. 2022. A Test for Evaluating Performance in HumanComputer Systems. arXiv preprint arXiv:2206.12390 (2022). ",
        "[20] Gaetano Cascini, Yukari Nagai, Georgi V Georgiev, Jader Zelaya, Niccol\u00f2 Be cattini, JeanFran\u00e7ois Boujut, Hernan Casakin, Nathan Crilly, Elies Dekoninck, John Gero, et al. 2022. Perspectives on design creativity and innovation research: 10 years later. , 30 pages. ",
        "[21] Stephen Cass. 2022. Top Programming Languages 2022. IEEE Spectrum (23 Aug 2022). https://spectrum.ieee.org/topprogramminglanguages2022 ",
        "[22] Cristina Catalan Aguirre, Nuria Gonzalez Castro, Carlos Delgado Kloos, Carlos AlarioHoyos, and Pedro Jos\u00e9 Mu\u00f1oz Merino. 2021. Conversational agent for supporting learners on a MOOC on programming with Java. (2021). ",
        "[23] Ana Paula Chaves and Marco Aurelio Gerosa. 2021. How should my chatbot interact? A survey on social characteristics in humanchatbot interaction design. International Journal of HumanComputer Interaction 37, 8 (2021), 729758. ",
        "[24] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas try, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, David W. Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel HerbertVoss, William H. Guss, Alex Nichol, Igor Babuschkin, S. Arun Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evalu ating a Large Language Models Trained on Code. ",
        "[25] LiTe Cheng, R.B. De Souza, Susanne Hupfer, John Patter son, and Steven Ross. 2003. Building Collaboration into IDEs: EditCompileRunDebugCollaborate? Queue 1, 9 (2003). 508",
        "[26] Carl Cook, Warwick Irwin, and Neville Churcher. 2005. A user evaluation of synchronous collaborative software engineering tools. In 12th AsiaPacific Software Engineering Conference (APSEC05). IEEE, 6pp. ",
        "[27] Claudio Le\u00f3n de la Barra, Broderick Crawford, Ricardo Soto, Sanjay Misra, and Eric Monfroy. 2013. Agile Software Development: It Is about Knowledge Management and Creativity. In Computational Science and Its Applications ICCSA 2013, Beniamino Murgante, Sanjay Misra, Maurizio Carlini, Carmelo M. Torre, HongQuang Nguyen, David Taniar, Bernady O. Apduhan, and Osvaldo Gervasi (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 98113. ",
        "[28] Uri Dekel and Steven Ross. 2004. Eclipse as a platform for research on in terruption management in software development. In Proceedings of the 2004 OOPSLA workshop on Eclipse Technology eXchange (Vancouver, British Columbia, Canada), Michael G. Burke (Ed.). ACM, 1216. ",
        "[29] Bobbie Eicher, Kathryn Cunningham, Sydni Peterson Marissa Gonzales, and Ashok Goel. 2017. Toward mutual theory of mind as a foundation for cocreation. In International Conference on Computational Creativity, CoCreation Workshop. ",
        "[30] Stephen M Fiore, Eduardo Salas, and Janis A CannonBowers. 2001. Group dynamics and shared mental model development. How people evaluate others in organizations 234 (2001). ",
        "[31] Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides. 1995. Design patterns: elements of reusable objectoriented software. AddisonWesley. ",
        "[32] GitHub, Inc. 2022. GitHub copilot your AI pair programmer. Retrieved August 5, 2022 from https://github.com/features/copilot/ ",
        "[33] Amelia Glaese, Nat McAleese, Maja Tr\u0119bacz, John Aslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker, Lucy CampbellGillingham, Jonathan Uesato, PoSen Huang, Ramona Comanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, Charlie Chen, Doug Fritz, Jaume Sanchez Elias, Richard Green, So\u0148a Mokr\u00e1, Nicholas Fernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac, John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, and Geoffrey Irving. 2022. Improving alignment of dialogue agents via targeted human judgements. https://arxiv.org/abs/2209.14375 ",
        "[34] Stephanie Glen. 2022. ChatGPT writes code, but wont replace devel opers. TechTarget (14 12 2022). Retrieved 20Jan2023 from https: //www.techtarget.com/searchsoftwarequality/news/252528379/ChatGPT writescodebutwontreplacedevelopers ",
        "[35] Samuel Holmes, Anne Moorhead, Raymond Bond, Huiru Zheng, Vivien Coates, and Mike McTear. 2018. WeightMentor: a new automated chatbot for weight loss maintenance. In Proceedings of the 32nd International BCS Human Computer Interaction Conference 32. 15. ",
        "[36] Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2020. Deep code comment generation with hybrid lexical and syntactical information. Empirical Software Engineering 25, 3 (2020), 21792217. ",
        "[37] Edwin L Hutchins, James D Hollan, and Donald A Norman. 1985. Direct manip ulation interfaces. Humancomputer interaction 1, 4 (1985), 311338. ",
        "[38] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016. Summarizing source code using a neural attention model. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 20732083. ",
        "[39] Andreas Jedlitschka and Markus Nick. 2003. Software Engineering Knowledge Repositories. Springer Berlin Heidelberg, Berlin, Heidelberg, 5580. ",
        "[40] Eirini Kalliamvakou. 2022. Research: Quantifying github copilots impact on developer productivity and happiness. https://github.blog/20220907 researchquantifyinggithubcopilotsimpactondeveloperproductivity andhappiness/ ",
        "[41] Anna Kantosalo et al. 2019. HumanComputer CoCreativity: Designing, Evalu ating and Modelling Computational Collaborators for Poetry Writing. (2019). ",
        "[42] Sandeep Kaur Kuttal, Bali Ong, Kate Kwasny, and Peter Robe. 2021. Trade Offs for Substituting a Human with an Agent in a Pair Programming Context: The Good, the Bad, and the Ugly. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems (Yokohama, Japan) (CHI 21). Association for Computing Machinery, New York, NY, USA, Article 243, 20 pages. ",
        "[43] Lauramaria Laine. 2021. Exploring Advertising Creatives Attitudes Towards HumanAI Collaboration. (2021). ",
        "[44] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, and et al. 2022. Competitionlevel code generation with AlphaCode. https: //arxiv.org/abs/2203.07814 ",
        "[45] Yaosheng Lou and Qi Sun. 2021. Overreliance on database: A case study of using web of science. Human Behavior and Emerging Technologies 3, 3 (2021), 454459. ",
        "[46] David Lyell and Enrico Coiera. 2017. Automation bias and verification complex ity: a systematic review. Journal of the American Medical Informatics Association 24, 2 (2017), 423431. ",
        "[47] Wendy E Mackay and AnneLaure Fayard. 1997. HCI, natural science and design: a framework for triangulation across disciplines. In Proceedings of the 2nd conference on Designing interactive systems: processes, practices, methods, and techniques. 223234. ",
        "[48] John E Mathieu, Tonia S Heffner, Gerald F Goodwin, Eduardo Salas, and Janis A CannonBowers. 2000. The influence of shared mental models on team process and performance. Journal of applied psychology 85, 2 (2000), 273. ",
        "[49] Cade Metz. 2022. Meet GPT3. It Has Learned to Code (and Blog and Ar gue). (Published 2020). https://www.nytimes.com/2020/11/24/science/artificial intelligenceaigpt3.html ",
        "[50] Robert J. Moore and Raphael Arar. 2019. Conversational UX Design: A Practi tioners Guide to the Natural Conversation Framework. Association for Computing Machinery, New York, NY, USA. ",
        "[51] Ekaterina A Moroz, Vladimir O Grizkevich, and Igor M Novozhilov. 2022. The Potential of Artificial Intelligence as a Method of Software Developers Produc tivity Improvement. In 2022 Conference of Russian Young Researchers in Electrical and Electronic Engineering (ElConRus). IEEE, 386390. ",
        "[52] Michael Muller, Stevean Ross, Stephanie Houde, Mayank Agarwal, Fernando Martinez, John Richards, Kartik Talamadupula, and Justin D Weisz. 2022. Drink ing Chai with Your (AI) Programming Partner: A Design Fiction about Gener ative AI for Software Engineering. HAIGEN Workshop at IUI 2022: 3rd Work shop on HumanAI CoCreation with Generative Models (2022). https://hai gen.github.io/2022/ ",
        "[53] Sandra R Murillo and J Alfredo S\u00e1nchez. 2014. Empowering interfaces for system administrators: Keeping the command line in mind when designing GUIs. In Proceedings of the XV International Conference on Human Computer Interaction. 14. ",
        "[54] Elizabeth D Mynatt and Gerhard Weber. 1994. Nonvisual presentation of graph ical user interfaces: contrasting two approaches. In Proceedings of the SIGCHI conference on Human factors in computing systems. 166172. ",
        "[55] Alok Mysore and Philip J Guo. 2017. Torta: Generating mixedmedia gui and commandline app tutorials using operatingsystemwide activity tracing. In Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology. 703714. ",
        "[56] C. Nass and Y. Moon. 2000. Machines and Mindlessness: Social Responses to Computers. Journal of Social Issues 56, 1 (2000), 81103. ",
        "[57] Nhan Nguyen and Sarah Nadi. 2022. An Empirical Evaluation of GitHub Copi lots Code Suggestions. In 2022 IEEE/ACM 19th International Conference on Mining Software Repositories (MSR). IEEE, 15. ",
        "[58] Martin Nordio, H Estler, Carlo A Furia, Bertrand Meyer, et al. 2011. Collaborative software development on the web. arXiv preprint arXiv:1105.0768 (2011). ",
        "[59] Maxwell Nye, Anders Andreassen, Guy GurAri, Henryk Witold Michalewski, Ja cob Austin, David Bieber, David Martin Dohan, Aitor Lewkowycz, Maarten Paul Bosma, David Luan, Charles Sutton, and Augustus Odena. 2021. Show Your Work: Scratchpads for Intermediate Computation with Language Models. https://arxiv.org/abs/2112.00114. ",
        "[60] OpenAI. 2022. ChatGPT: Optimizing Language Models for Dialogue. OpenAI Blog (30 11 2022). Retrieved 20Jan2023 from https://openai.com/blog/chatgpt/ ",
        "[61] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. https://arxiv. org/abs/2203.02155 ",
        "[62] Peter Pirolli and Stuart Card. 1999. Information foraging. Psychological review 106, 4 (1999), 643. ",
        "[63] Larry Press. 1990. Personal computing: Windows, DOS and the MAC. Commun. ACM 33, 11 (1990), 1926. ",
        "[64] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language Models are Unsupervised Multitask Learners. ",
        "[65] Alvin Rajkomar, Jeffrey Dean, and Isaac Kohane. 2019. Machine learning in medicine. New England Journal of Medicine 380, 14 (2019), 13471358. ",
        "[66] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125 (2022). ",
        "[67] B. Reeves and C.I. Nass. 1996. The Media Equation: How People Treat Computers, Television, and New Media Like Real People and Places. CSLI Publications. ",
        "[68] Mawarny Md Rejab, James Noble, and George Allan. 2014. Distributing Expertise in Agile Software Development Projects. In 2014 Agile Conference. 3336. ",
        "[69] Jeba Rezwana and Mary Lou Maher. 2021. COFI: A Framework for Modeling Interaction in HumanAI CoCreative Systems.. In ICCC. 444448. ",
        "[70] Charles H. Rich and Richard C. Waters. 1990. The Programmers Apprentice. AddisonWesley Publishing Company, Reading, MA. ",
        "[71] Peter Robe and Sandeep Kaur Kuttal. 2022. Designing PairBuddyA Conver sational Agent for Pair Programming. ACM Transactions on ComputerHuman Interaction (TOCHI) 29, 4 (2022), 144. ",
        "[72] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. 2022. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni tion. 1068410695. 509",
        "[73] Steven Ross, Elizabeth Brownholtz, and Robert Armes. 2004. A Multiple Application Conversational Agent. In Proceedings of the 9th International Con ference on Intelligent User Interfaces (Funchal, Madeira, Portugal) (IUI 04). Asso ciation for Computing Machinery, New York, NY, USA, 319321. ",
        "[74] Steven Ross, Elizabeth Brownholtz, and Robert Armes. 2004. Voice User Interface Principles for a Conversational Agent. In Proceedings of the 9th International Conference on Intelligent User Interfaces (Funchal, Madeira, Portugal) (IUI 04). Association for Computing Machinery, New York, NY, USA, 364365. ",
        "[75] Baptiste Roziere, MarieAnne Lachaux, Lowik Chanussot, and Guillaume Lample. 2020. Unsupervised Translation of Programming Languages. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 2060120611. ",
        "[76] Harvey Sacks. 1984. Notes on methodology. In Structures of Social Action: Studies in Conversation Analysis, John Heritage and J. Maxwell Atkinson (Eds.). Cambridge University Press, Cambridge, 227. ",
        "[77] Nithya Sambasivan and Rajesh Veeraraghavan. 2022. The Deskilling of Domain Expertise in AI Development. In CHI Conference on Human Factors in Computing Systems. 114. ",
        "[78] Harini Sampath, Alice Merrick, and Andrew Macvean. 2021. Accessibility of command line interfaces. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 110. ",
        "[79] Matthias Scheutz, Scott A DeLoach, and Julie A Adams. 2017. A framework for developing and using shared mental models in humanagent teams. Journal of Cognitive Engineering and Decision Making 11, 3 (2017), 203224. ",
        "[80] Isabella Seeber, Eva Bittner, Robert O Briggs, Triparna De Vreede, GertJan De Vreede, Aaron Elkins, Ronald Maier, Alexander B Merz, Sarah OesteRei\u00df, Nils Randrup, et al. 2020. Machines as teammates: A research agenda on AI in team collaboration. Information & management 57, 2 (2020), 103174. ",
        "[81] Shilad Sen, Werner Geyer, Michael Muller, Marty Moore, Beth Brownholtz, Eric Wilcox, and David R Millen. 2006. FeedMe: a collaborative alert filtering system. In Proceedings of the 2006 20th anniversary conference on Computer supported cooperative work. 8998. ",
        "[82] Ben Shneiderman. 2020. Humancentered artificial intelligence: Three fresh ideas. AIS Transactions on HumanComputer Interaction 12, 3 (2020), 109124. ",
        "[83] Ben Shneiderman. 2022. HumanCentered AI. Oxford University Press. ",
        "[84] Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, et al. 2022. BlenderBot 3: a deployed conversational agent that continually learns to responsibly engage. arXiv preprint arXiv:2208.03188 (2022). ",
        "[85] Michael Skirpan and Casey Fiesler. 2018. Ad empathy: A design fiction. In Proceedings of the 2018 ACM Conference on Supporting Groupwork. 267273. ",
        "[86] Diomidis Spinellis. 2012. Git. IEEE Software 29, 3 (2012), 100101. https: //doi.org/10.1109/MS.2012.61 ",
        "[87] Angie Spoto and Natalia Oleynik. 2017. Library of MixedInitiative Creative Interfaces. Retrieved 19Jun2021 from http://mici.codingconduct.cc/ ",
        "[88] Ayushi Srivastava, Shivani Kapania, Anupriya Tuli, and Pushpendra Singh. 2021. Actionable UI Design Guidelines for Smartphone Applications Inclusive of LowLiterate Users. Proceedings of the ACM on HumanComputer Interaction 5, CSCW1 (2021), 130. ",
        "[89] MargaretAnne Storey and Alexey Zagalsky. 2016. Disrupting developer produc tivity one bot at a time. In Proceedings of the 2016 24th ACM SIGSOFT international symposium on foundations of software engineering. 928931. ",
        "[90] Kartik Talamadupula. 2021. Applied AI matters: AI4Code: applying artificial intelligence to source code. AI Matters 7, 1 (2021), 1820. ",
        "[91] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kul shreshtha, HengTze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, and et al. 2022. LAMDA: Language models for dialog applications. https: //arxiv.org/abs/2201.08239 ",
        "[92] Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, Shao Kun Deng, and Neel Sundaresan. 2020. Unit Test Case Generation with Transformers and Focal Context. arXiv preprint arXiv:2009.05617 (2020). ",
        "[93] Severi Uusitalo, Anna Kantosalo, Antti Salovaara, Tapio Takala, and Christian Guckelsberger. 2022. Cocreative Product Design with Interactive Evolutionary Algorithms: A PracticeBased Reflection. In International Conference on Compu tational Intelligence in Music, Sound, Art and Design (Part of EvoStar). Springer, 292307. ",
        "[94] Priyan Vaithilingam and Philip J Guo. 2019. Bespoke: Interactively synthesizing custom GUIs from commandline applications by demonstration. In Proceedings of the 32nd annual ACM symposium on user interface software and technology. 563576. ",
        "[95] Priyan Vaithilingam, Tianyi Zhang, and Elena L. Glassman. 2022. Expectation vs. Experience: Evaluating the Usability of Code Generation Tools Powered by Large Language Models. In Extended Abstracts of the 2022 CHI Conference on Human Factors in Computing Systems (New Orleans, LA, USA) (CHI EA 22). Association for Computing Machinery, New York, NY, USA, Article 332, 7 pages. https://doi.org/10.1145/3491101.3519665 ",
        "[96] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In Advances in Neural Information Processing Systems, I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.), Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper/ 2017/file/3f5ee243547dee91fbd053c1c4a845aaPaper.pdf ",
        "[97] Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and Philip S Yu. 2018. Improving automatic source code summarization via deep rein forcement learning. In Proceedings of the 33rd ACM/IEEE international conference on automated software engineering. 397407. ",
        "[98] April Yi Wang, Dakuo Wang, Jaimie Drozdal, Michael Muller, Soya Park, Justin D Weisz, Xuye Liu, Lingfei Wu, and Casey Dugan. 2022. Documentation Matters: HumanCentered AI System to Assist Data Science Code Documentation in Computational Notebooks. ACM Transactions on ComputerHuman Interaction 29, 2 (2022), 133. ",
        "[99] Dakuo Wang, Justin D Weisz, Michael Muller, Parikshit Ram, Werner Geyer, Casey Dugan, Yla Tausczik, Horst Samulowitz, and Alexander Gray. 2019. HumanAI collaboration in data science: Exploring data scientists perceptions of automated AI. Proceedings of the ACM on HumanComputer Interaction 3, CSCW (2019), 124. ",
        "[100] Qiaosi Wang, Koustuv Saha, Eric Gregori, David Joyner, and Ashok Goel. 2021. Towards mutual theory of mind in humanai interaction: How language reflects what students perceive about a virtual teaching assistant. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems. 114. ",
        "[101] Jeremy Warner and Philip J Guo. 2017. Codepilot: Scaffolding endtoend collaborative software development for novice programmers. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. 11361141. ",
        "[102] Justin D Weisz, Michael Muller, Stephanie Houde, John Richards, Steven I Ross, Fernando Martinez, Mayank Agarwal, and Kartik Talamadupula. 2021. Per fection Not Required? HumanAI Partnerships in Code Translation. In 26th International Conference on Intelligent User Interfaces. 402412. ",
        "[103] Justin D Weisz, Michael Muller, Steven I Ross, Fernando Martinez, Stephanie Houde, Mayank Agarwal, Kartik Talamadupula, and John T Richards. 2022. Bet ter together? an evaluation of aisupported code translation. In 27th International Conference on Intelligent User Interfaces. 369391. ",
        "[104] Joseph Weizenbaum. 1966. ELIZA a computer program for the study of natural language communication between man and machine. Commun. ACM 9 (1966), 3645. ",
        "[105] Frank F Xu, Bogdan Vasilescu, and Graham Neubig. 2022. Inide code generation from natural language: Promise and challenges. ACM Transactions on Software Engineering and Methodology (TOSEM) 31, 2 (2022), 147. ",
        "[106] Aditya Ankur Yadav, Ishan Garg, and Dr. Pratistha Mathur. 2019. PACT Pro gramming Assistant ChaTbot. In 2019 2nd International Conference on Intelligent Communication and Computational Techniques (ICCT). 131136. ",
        "[107] Munazza Zaib, Quan Z. Sheng, and W. Zhang. 2020. A Short Survey of Pre trained Language Models for Conversational AIA New Age in NLP. Proceedings of the Australasian Computer Science Week Multiconference (2020). ",
        "[108] Elaine Zibrowski, Lisa Shepherd, Kamran Sedig, Richard Booth, Candace Gibson, et al. 2018. Easier and faster is not always better: grounded theory of the impact of largescale system transformation on the clinical work of emergency medicine nurses and physicians. JMIR Human Factors 5, 4 (2018), e11013. ",
        "[109] Albert Ziegler, Eirini Kalliamvakou, X. Alice Li, Andrew Rice, Devon Rifkin, Shawn Simister, Ganesh Sittampalam, and Edward Aftandilian. 2022. Produc tivity Assessment of Neural Code Completion. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming (San Diego, CA, USA) (MAPS 2022). Association for Computing Machinery, New York, NY, USA, 2129. https://doi.org/10.1145/3520312.3534864 510"
    ]
}