{
    "publication_date": "18-09-2023",
    "title": "Large Language Model Augmented Narrative Driven Recommendations",
    "authors": [
        "Sheshera Mysore",
        "Andrew McCallum",
        "Hamed Zamani"
    ],
    "institutes": [
        "University of Massachusetts Amherst USA",
        "University of Massachusetts Amherst USA",
        "University of Massachusetts Amherst USA"
    ],
    "keywords": [
        "Narrative-driven recommendation",
        "Information systems",
        "Recommender systems",
        "Users and interactive retrieval",
        "Computing methodologies",
        "Natural language generation"
    ],
    "abstract": "Narrative-driven recommendation (NDR) presents an information access problem where users solicit recommendations with verbose descriptions of their preferences and context, for example, travelers soliciting recommendations for points of interest while describing their likes/dislikes and travel circumstances. These requests are increasingly important with the rise of natural language-based conversational interfaces for search and recommendation systems. However, NDR lacks abundant training data for models, and current platforms commonly do not support these requests. Fortunately, classical user-item interaction datasets contain rich textual data, e.g., reviews, which often describe user preferences and context \u2013 this may be used to bootstrap training for NDR models. In this work, we explore using large language models (LLMs) for data augmentation to train NDR models. We use LLMs for authoring synthetic narrative queries from user-item interactions with few-shot prompting and train retrieval models for NDR on synthetic queries and user-item interaction data. Our experiments demonstrate that this is an effective strategy for training small-parameter retrieval models that outperform other retrieval and LLM baselines for narrative-driven recommendation.",
    "content": "1 INTRODUCTION\nRecommender systems personalized to users are an important component of several industry-scale platforms [16, 17, 46]. These systems function by inferring users interests from their prior interactions on the platform and making recommendations based on these inferred interests. While recommendations based on historical interactions are effective, users soliciting recommendations often start with a vague idea about their desired target items or may desire recommendations depending on the context of use, often missing in historical interaction data (Figure 1). In these scenarios, it is common for users to solicit recommendations through long-form narrative queries describing their broad interests and context. Information access tasks like these have been studied as narrative-driven recommendations (NDR) for items ranging from books [5] and movies [18], to points of interest [1]. Bogers and Koolen [5] note these narrative requests to be common on discussion forums and several subreddits1, but, there is a lack of support for these complex natural language queries in current recommenders. However, with the emergence of conversational interfaces for information access tasks, support for complex NDR tasks is likely to become necessary. In this context, recent work has noted an increase in complex and subjective natural language requests compared to more conventional search interfaces [13, 34]. Furthermore, the emergence of large language models (LLM) with strong language understanding capabilities presents the potential for fulfilling such complex requests [9, 33]. This work explores the potential for re-purposing historical user-item recommendation datasets, traditionally used for training collaborative filtering recommenders, with LLMs to support NDR. Specifically, given a users interactions, \ud835\udc37\ud835\udc62, with items and their accompanying text documents (e.g., reviews, descriptions) \ud835\udc37\ud835\udc62 = {\ud835\udc51\ud835\udc56}\ud835\udc41\ud835\udc62 \ud835\udc56=1, selected from a user-item interaction dataset I, we prompt InstructGPT, a 175B parameter LLM, to author a synthetic narrative query \ud835\udc5e\ud835\udc62 based on \ud835\udc37\ud835\udc62 (Figure 2). Since we expect the query \ud835\udc5e\ud835\udc62 to be noisy and not fully representative of all the user reviews, \ud835\udc37\ud835\udc62 is filtered to retain only a fraction of the reviews based on a language-model assigned likelihood of \ud835\udc5e\ud835\udc62 given a user document, \ud835\udc51\ud835\udc56. Then, a pre-trained LM based retrieval model (110M parameters) is fine-tuned for retrieval on the synthetic queries and filtered reviews. Our approach, which we refer to as Mint2, follows from the observation that while narrative queries and suggestions are often made in online discussion forums, and could serve as training data, the number of these posts and the diversity of domains for which they are available is significantly smaller than the size and diversity of passively gathered user-item interaction datasets. E.g. while Bogers and Koolen [5] note nearly 25,000 narrative requests for books on the LibraryThing discussion forum, a publicly available user-item interaction dataset for Goodreads contains interactions with nearly 2.2M books by 460k users [43] . We empirically evaluate Mint in a publicly available test collection for point of interest recommendation: pointrec [1]. To train 1r/MovieSuggestions, r/booksuggestions, r/Animesuggest 2Mint: Data augMentation with INteraction narraTives. 777"
}